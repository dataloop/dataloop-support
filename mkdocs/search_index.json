{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to our support docs!\n\u00b6\n\n\nHere you will find a body of knowledge to get your monitoring started with\nOutlyer.\n\n\nStart with the \nInstallation\n pages to get Outlyer.com agents\ninstalled on your servers.\n\n\nNeed help? Head on over to our \nZendesk ticketing system\n to raise a\nsupport ticket with the Outlyer team, or send email to\n\nsupport(at)outlyer.com\n, which will also open a ticket.\n\n\n\n\nNote\n\n\nWe recently launched our enhanced Docker support! Check out the\n\nnew docs\n to get started. If you're upgrading\nfrom an older version of our Docker Agent, check the \nupgrading\n\npage for instructions.\n\n\n\n\nSetup and Configuration\n\u00b6\n\n\nGetting Started\n\n\nOutlyer Agent\n\n\nGraphite, StatsD & InfluxDB\n\n\nNagios and Prometheus\n\n\nAlerting\n\n\nDashboards\n\n\nOrganizational Account Model\n  \n\n\nIntegrations\n\u00b6\n\n\nThird Party\n\n\nWebhook\n\n\nTroubleshooting\n\u00b6\n\n\nExpired gpg keys\n\n\nGenerating an HTTP Archive (HAR) file\n\n\nLatest Pages\n\u00b6\n\n\nExpired gpg keys",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-our-support-docs",
            "text": "Here you will find a body of knowledge to get your monitoring started with\nOutlyer.  Start with the  Installation  pages to get Outlyer.com agents\ninstalled on your servers.  Need help? Head on over to our  Zendesk ticketing system  to raise a\nsupport ticket with the Outlyer team, or send email to support(at)outlyer.com , which will also open a ticket.   Note  We recently launched our enhanced Docker support! Check out the new docs  to get started. If you're upgrading\nfrom an older version of our Docker Agent, check the  upgrading \npage for instructions.",
            "title": "Welcome to our support docs!"
        },
        {
            "location": "/#setup-and-configuration",
            "text": "Getting Started  Outlyer Agent  Graphite, StatsD & InfluxDB  Nagios and Prometheus  Alerting  Dashboards  Organizational Account Model",
            "title": "Setup and Configuration"
        },
        {
            "location": "/#integrations",
            "text": "Third Party  Webhook",
            "title": "Integrations"
        },
        {
            "location": "/#troubleshooting",
            "text": "Expired gpg keys  Generating an HTTP Archive (HAR) file",
            "title": "Troubleshooting"
        },
        {
            "location": "/#latest-pages",
            "text": "Expired gpg keys",
            "title": "Latest Pages"
        },
        {
            "location": "/getting_started/overview/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nOverview\n\u00b6\n\n\nAgent Installation and Tagging\n\u00b6\n\n\nYou should plan to install the dataloop-agent on every server. On Linux the Dataloop (Outlyer) Agent is packaged up as a deb and rpm to make this simple. We have configuration management repositories for:\n\n\nPuppet\n\n\nChef\n\n\nAnsible\n\n\nSalt\n  \n\n\nOn Windows it's a manual install via the installer. However, this does support silent install if you wish to automate with Powershell.\n\n\nWe advise that you use configuration management to tag your servers so that plugin deployment, dashboards and alerts can be setup in an aggregated way. Each configuration management repository has a README.md that describes how to do the tagging.\n\n\nOur recommended best practices for tags are:\n\n\nproduct_name,environment,role\n\n\n\n\n\nFor example, production web servers for the acme todo list app would get tagged:\n\n\ntodo,prod,web\n\n\n\n\n\nThen when a product manager comes along and says: \"we should do some kind of funky calendar app next\", you can spin up new product environments with:\n\n\ncalendar,prod,web\n\n\n\n\n\nor,\n\n\ncalendar,qa,database\n\n\n\n\n\nTagging can also be done in the UI although this is only recommended for static environments. You want your servers to spin up with the Dataloop (Outlyer) Agent installed and with a set of tags that can be used in Outlyer to setup the rest of the monitoring.\n\n\nYou can of course add any number of tags to your agents. The more you add the better actually. The whole premise of our future auto discovery is that we'll detect what is running on your server and automatically tag it. But feel free to apply tags for versions, colours (blue, green deploy?) or any thing you'd like to use in future to refine how plugins are deployed, or dashboards and alerts are created.\n\n\nIt is important that you have a Dataloop (Outlyer) Agent installed on every server, that they are in the right tags and that you keep vigilant for any discrepancies between what's in Outlyer and what is reality. If you get the building blocks correct then the plugins, dashboards and alerts you layer on top will almost guarantee you'll get told when something breaks. If you accidentally don't install agents or put agents into the correct tags then the monitoring is invalid.\n\n\nMetrics Collection\n\u00b6\n\n\nOnce you have the agents installed and the tags setup as per section 1. you can start to layer on monitoring coverage.\n\n\nBy default every Dataloop (Outlyer) Agent gets put into a tag called \nall\n. This is useful because you can drag plugins onto this tag and they will run on every single server. We use this tag to deploy a script called 'base' which sends back all of the basic operating system metrics you would expect. Things like CPU, Memory, Disk, Network etc. We'd advise you leave 'base' running on the all tag otherwise you won't get alerted when disks fill up or servers max out on CPU.\n\n\nEvery server role that you have probably has something installed on it, like apache, or nginx or mysql or elasticsearch etc. For every role you should ensure that each of the components you depend on has at least one script deployed to monitor it. We provide scripts for most commonly used services. If your service is not listed then you may need to create a plugin to cover it, or request one via email to \nsupport[at]outlyer.com\n.com.\n\n\nIn our example before we had todo,prod and web tags. These boxes only run Nginx. In the Outlyer UI we would therefore drag the Nginx plugin on top of the 'web' tag so that it will automatically deploy and start sending back metrics.\n\n\nIt is worth noting at this stage that some plugins require some configuration. You should always click \nedit\n on a plugin, select a server to test it on and hit run. The console will show either an error, which needs fixing by either configuration or setting some variables in the script. In the case of variables we tend to put good comments into the script so it's easy to see what variables to update. Commonly altered variables will always be set at the top of the script in uppercase.\n\n\nIn a ideal world you'd have base running on every system via the all tag. Then a plugin deployed for every service you have. Ultimately this will all be auto-discovered, but for now you need to assign the correct plugins.\n\n\nYou will also undoubtedly have some things you want to check that we don't have out-of-the-box. For these things write some quick check scripts in the browser, test them via the run button, and drag them onto the tag you wish to run them on to deploy.\n\n\nMonitoring is an ongoing effort. You should aim for good coverage out of the gate, however, you will need to continually add check scripts as services are added, or as gaps in your monitoring are found (usually as a result of incident reviews and postmortems). The more Boolean checks and meaningful metrics you have, the better the dashboards and alerts get when you create them.\n\n\nOnce you have good Nagios check script coverage it might be time to think about some other ways to improve your coverage.\n\n\nReal-time metrics: Not everything needs to be measured in real time. You'll end up with a lot of coverage from the Nagios check scripts and you probably won't use all of those. We know this because we've got a great deal of experience collecting metrics in real time that have never been used, ever, and likely never will. The argument for 'collect everything as you can never go back in time and enable it' is valid and for every case I've wanted to look at trends back in time, 30 second resolution check scripts have been good enough.\n\n\nYou can install 3rd party agents like CollectD or BrightCover Diamond, configured with the Dataloop (Outlyer) Agent fingerprint as the metrics prefix to send metrics back to Outlyer in realtime. Doing this provides real-time metrics at the expense of additional complexity that needs to be managed outside of Outlyer. Use these tools if you want to spend the time setting them up and would like to create dashboards that update faster and alert on highly time sensitive items. We would however caution that this adds complexity back into your configuration management and typically puts the setup work back onto a select number of people.\n\n\nYou can fire metrics at graphite.dataloop.io at one second resolution. These metrics 'append' the Dataloop (Outlyer) Agent metrics, in so far as they bind to a Dataloop (Outlyer) Agent fingerprint so they look like they come from the same source as your Nagios plugins from that server. We have a support page dedicated to how Graphite metrics work.\n\n\nExamples of things you may want realtime metrics on are API response times or message queue statistics. These power the graphs that people stare at when there is an issue, and they expect to see immediate changes when something is fixed. We recommend you run scripts that send these real-time metrics to us from the boxes themselves, hitting localhost. This way they append the Nagios metrics and you can use the same tagging system in Outlyer to aggregate all response times across multiple hosts in a tag in graphs and alerts. You'll then see instantly which box has the problem.\n\n\nYou may want to check the entire service response time from outside. In this case it is valid to run the scripts on external agents, hitting an endpoint like a load balancer. If you do this be careful to identify the Dataloop (Outlyer) Agent these metrics are bound to as something easily recognisable as an external check box. Otherwise, you may end up with metrics bound to agents that are not in any way associated with the environment you are monitoring. It is not uncommon for people to setup Amazon micro instances in various regions to run these. Why would you do this in preference to something like Pingdom? Well, you get to run scripts and have an entire programming language at your disposal. You can get these agents to constantly run smoke tests against production. Obviously, it's a bit overkill if you do just want to do simple curls.\n\n\nThe Dataloop (Outlyer) Agent and plugins are our primary source of monitoring, and the Graphite metrics sit on top. The deployment architecture is extremely flexible as you can install a Dataloop (Outlyer) Agent anywhere, either inside your network, or outside and monitor either localhost or other servers.\n\n\nApplication Metrics: Nagios metrics poll the operating system and services from the outside. Some services expose a lot of useful metrics that we write scripts to collect. But what about the application that your team wrote? For that there are a couple of options.\n\n\n\n\n\n\nThe developers can expose metrics via a REST endpoint which could be scraped via a plugin. You could either make up your own spec, or use one like this: \n\n\nhttps://github.com/beamly/SE4\n  \n\n\nIf you're in a Java shop the Coda Hale metrics library is awesome. A lot of other languages have similar libraries for exposing internal metrics over an api so that monitoring tools like Dataloop can extract them.  \n\n\n\n\n\n\nStatsD - This is a piece of software that you can install on a central server somewhere (or we can host one for you). You then add a StatsD client library in your code and start to push realtime application level metrics into Outlyer. We use StatsD internally a lot to debug backend processes. Every time you send us a metric we measure how that flows through our system by monitoring various worker stats like inbound, outbound, held metrics etc. On top of counters like that you can also use StatsD to collect business and performance metrics. Timing the duration of functions and metrics is quite fun, especially when you can see what's happening in realtime in production.\n\n\n\n\n\n\nFor large installations we recommend setting up a StatsD server per environment and tagging the Dataloop (Outlyer) Agent on the StatsD server with the environment tag it is part of. This then makes metrics browsing easier inside Outlyer.\n\n\nDashboards\n\u00b6\n\n\nAlways create dashboards widgets based on tags, or combinations of tags. We let you create them for individual hosts but that's mostly for new people playing. If you create a dashboard of widgets based off a host and you delete that host, you will need to edit all of your widgets. Whereas if you created the dashboard from a tag, the hosts are expendable. You just need to put a Dataloop (Outlyer) Agent back into the tag again to resume showing metrics.\n\n\nWe recommend creating a dashboard per environment / role combo. So in our example above we had production web servers for the todo list product. For this I'd create a dashboard called \nTodo Prod Webservers\n and start to add a variety of tag level widgets.\n\n\nThe most important widgets to get onto a dashboard are the status widgets. Add an agent.status so you can visibly see all servers are alive. Beyond this, each plugin has a .status metric you should add. For our example this would be nginx.status. Then for each service there may be various metrics you want to see. For Nginx these might be:\n\n\n\n\n\n\nRequests per second\n\n\n\n\n\n\nConnections\n\n\n\n\n\n\n4xx and 5xx errors\n\n\n\n\n\n\nResponse times\n\n\n\n\n\n\nYou may want to add a few OS metrics too. Then you'll be able to tell if your disks are getting filled, or if you need to buy bigger boxes with more cpu or memory.\n\n\nIt's also a good idea to create some business dashboards. Write scripts that poll systems that have those metrics in them, or use StatsD as mentioned above. As with scripts, the dashboards are an ongoing process. Update them every time you want to keep an eye on something new.\n\n\nRules and Alerts\n\u00b6\n\n\nBy default you get an \nall systems\n ruleset with criterial in it that will alert you on the \nbase\n plugin on the all tag. Things like servers going down, cpu, disk, load etc. You should probably leave these as they are and possibly extend them if there are things you want to check across every box.\n\n\nFor each of your services it's probably worth having a catch all ruleset. For instance you might create a ruleset for \nNginx\n and put some criteria in there for the service status being alive and perhaps requests per second being below a certain threshold.\n\n\nYou should create different rulesets depending on who gets alerted. For instance you might create a ruleset for todo,stag,web that simply emails a group email address when alert criteria are triggered. Whereas your ruleset for todo,prod,web might send emails to pagerduty so that people get an SMS and woken up.",
            "title": "Overview"
        },
        {
            "location": "/getting_started/overview/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/getting_started/overview/#agent-installation-and-tagging",
            "text": "You should plan to install the dataloop-agent on every server. On Linux the Dataloop (Outlyer) Agent is packaged up as a deb and rpm to make this simple. We have configuration management repositories for:  Puppet  Chef  Ansible  Salt     On Windows it's a manual install via the installer. However, this does support silent install if you wish to automate with Powershell.  We advise that you use configuration management to tag your servers so that plugin deployment, dashboards and alerts can be setup in an aggregated way. Each configuration management repository has a README.md that describes how to do the tagging.  Our recommended best practices for tags are:  product_name,environment,role  For example, production web servers for the acme todo list app would get tagged:  todo,prod,web  Then when a product manager comes along and says: \"we should do some kind of funky calendar app next\", you can spin up new product environments with:  calendar,prod,web  or,  calendar,qa,database  Tagging can also be done in the UI although this is only recommended for static environments. You want your servers to spin up with the Dataloop (Outlyer) Agent installed and with a set of tags that can be used in Outlyer to setup the rest of the monitoring.  You can of course add any number of tags to your agents. The more you add the better actually. The whole premise of our future auto discovery is that we'll detect what is running on your server and automatically tag it. But feel free to apply tags for versions, colours (blue, green deploy?) or any thing you'd like to use in future to refine how plugins are deployed, or dashboards and alerts are created.  It is important that you have a Dataloop (Outlyer) Agent installed on every server, that they are in the right tags and that you keep vigilant for any discrepancies between what's in Outlyer and what is reality. If you get the building blocks correct then the plugins, dashboards and alerts you layer on top will almost guarantee you'll get told when something breaks. If you accidentally don't install agents or put agents into the correct tags then the monitoring is invalid.",
            "title": "Agent Installation and Tagging"
        },
        {
            "location": "/getting_started/overview/#metrics-collection",
            "text": "Once you have the agents installed and the tags setup as per section 1. you can start to layer on monitoring coverage.  By default every Dataloop (Outlyer) Agent gets put into a tag called  all . This is useful because you can drag plugins onto this tag and they will run on every single server. We use this tag to deploy a script called 'base' which sends back all of the basic operating system metrics you would expect. Things like CPU, Memory, Disk, Network etc. We'd advise you leave 'base' running on the all tag otherwise you won't get alerted when disks fill up or servers max out on CPU.  Every server role that you have probably has something installed on it, like apache, or nginx or mysql or elasticsearch etc. For every role you should ensure that each of the components you depend on has at least one script deployed to monitor it. We provide scripts for most commonly used services. If your service is not listed then you may need to create a plugin to cover it, or request one via email to  support[at]outlyer.com .com.  In our example before we had todo,prod and web tags. These boxes only run Nginx. In the Outlyer UI we would therefore drag the Nginx plugin on top of the 'web' tag so that it will automatically deploy and start sending back metrics.  It is worth noting at this stage that some plugins require some configuration. You should always click  edit  on a plugin, select a server to test it on and hit run. The console will show either an error, which needs fixing by either configuration or setting some variables in the script. In the case of variables we tend to put good comments into the script so it's easy to see what variables to update. Commonly altered variables will always be set at the top of the script in uppercase.  In a ideal world you'd have base running on every system via the all tag. Then a plugin deployed for every service you have. Ultimately this will all be auto-discovered, but for now you need to assign the correct plugins.  You will also undoubtedly have some things you want to check that we don't have out-of-the-box. For these things write some quick check scripts in the browser, test them via the run button, and drag them onto the tag you wish to run them on to deploy.  Monitoring is an ongoing effort. You should aim for good coverage out of the gate, however, you will need to continually add check scripts as services are added, or as gaps in your monitoring are found (usually as a result of incident reviews and postmortems). The more Boolean checks and meaningful metrics you have, the better the dashboards and alerts get when you create them.  Once you have good Nagios check script coverage it might be time to think about some other ways to improve your coverage.  Real-time metrics: Not everything needs to be measured in real time. You'll end up with a lot of coverage from the Nagios check scripts and you probably won't use all of those. We know this because we've got a great deal of experience collecting metrics in real time that have never been used, ever, and likely never will. The argument for 'collect everything as you can never go back in time and enable it' is valid and for every case I've wanted to look at trends back in time, 30 second resolution check scripts have been good enough.  You can install 3rd party agents like CollectD or BrightCover Diamond, configured with the Dataloop (Outlyer) Agent fingerprint as the metrics prefix to send metrics back to Outlyer in realtime. Doing this provides real-time metrics at the expense of additional complexity that needs to be managed outside of Outlyer. Use these tools if you want to spend the time setting them up and would like to create dashboards that update faster and alert on highly time sensitive items. We would however caution that this adds complexity back into your configuration management and typically puts the setup work back onto a select number of people.  You can fire metrics at graphite.dataloop.io at one second resolution. These metrics 'append' the Dataloop (Outlyer) Agent metrics, in so far as they bind to a Dataloop (Outlyer) Agent fingerprint so they look like they come from the same source as your Nagios plugins from that server. We have a support page dedicated to how Graphite metrics work.  Examples of things you may want realtime metrics on are API response times or message queue statistics. These power the graphs that people stare at when there is an issue, and they expect to see immediate changes when something is fixed. We recommend you run scripts that send these real-time metrics to us from the boxes themselves, hitting localhost. This way they append the Nagios metrics and you can use the same tagging system in Outlyer to aggregate all response times across multiple hosts in a tag in graphs and alerts. You'll then see instantly which box has the problem.  You may want to check the entire service response time from outside. In this case it is valid to run the scripts on external agents, hitting an endpoint like a load balancer. If you do this be careful to identify the Dataloop (Outlyer) Agent these metrics are bound to as something easily recognisable as an external check box. Otherwise, you may end up with metrics bound to agents that are not in any way associated with the environment you are monitoring. It is not uncommon for people to setup Amazon micro instances in various regions to run these. Why would you do this in preference to something like Pingdom? Well, you get to run scripts and have an entire programming language at your disposal. You can get these agents to constantly run smoke tests against production. Obviously, it's a bit overkill if you do just want to do simple curls.  The Dataloop (Outlyer) Agent and plugins are our primary source of monitoring, and the Graphite metrics sit on top. The deployment architecture is extremely flexible as you can install a Dataloop (Outlyer) Agent anywhere, either inside your network, or outside and monitor either localhost or other servers.  Application Metrics: Nagios metrics poll the operating system and services from the outside. Some services expose a lot of useful metrics that we write scripts to collect. But what about the application that your team wrote? For that there are a couple of options.    The developers can expose metrics via a REST endpoint which could be scraped via a plugin. You could either make up your own spec, or use one like this:   https://github.com/beamly/SE4     If you're in a Java shop the Coda Hale metrics library is awesome. A lot of other languages have similar libraries for exposing internal metrics over an api so that monitoring tools like Dataloop can extract them.      StatsD - This is a piece of software that you can install on a central server somewhere (or we can host one for you). You then add a StatsD client library in your code and start to push realtime application level metrics into Outlyer. We use StatsD internally a lot to debug backend processes. Every time you send us a metric we measure how that flows through our system by monitoring various worker stats like inbound, outbound, held metrics etc. On top of counters like that you can also use StatsD to collect business and performance metrics. Timing the duration of functions and metrics is quite fun, especially when you can see what's happening in realtime in production.    For large installations we recommend setting up a StatsD server per environment and tagging the Dataloop (Outlyer) Agent on the StatsD server with the environment tag it is part of. This then makes metrics browsing easier inside Outlyer.",
            "title": "Metrics Collection"
        },
        {
            "location": "/getting_started/overview/#dashboards",
            "text": "Always create dashboards widgets based on tags, or combinations of tags. We let you create them for individual hosts but that's mostly for new people playing. If you create a dashboard of widgets based off a host and you delete that host, you will need to edit all of your widgets. Whereas if you created the dashboard from a tag, the hosts are expendable. You just need to put a Dataloop (Outlyer) Agent back into the tag again to resume showing metrics.  We recommend creating a dashboard per environment / role combo. So in our example above we had production web servers for the todo list product. For this I'd create a dashboard called  Todo Prod Webservers  and start to add a variety of tag level widgets.  The most important widgets to get onto a dashboard are the status widgets. Add an agent.status so you can visibly see all servers are alive. Beyond this, each plugin has a .status metric you should add. For our example this would be nginx.status. Then for each service there may be various metrics you want to see. For Nginx these might be:    Requests per second    Connections    4xx and 5xx errors    Response times    You may want to add a few OS metrics too. Then you'll be able to tell if your disks are getting filled, or if you need to buy bigger boxes with more cpu or memory.  It's also a good idea to create some business dashboards. Write scripts that poll systems that have those metrics in them, or use StatsD as mentioned above. As with scripts, the dashboards are an ongoing process. Update them every time you want to keep an eye on something new.",
            "title": "Dashboards"
        },
        {
            "location": "/getting_started/overview/#rules-and-alerts",
            "text": "By default you get an  all systems  ruleset with criterial in it that will alert you on the  base  plugin on the all tag. Things like servers going down, cpu, disk, load etc. You should probably leave these as they are and possibly extend them if there are things you want to check across every box.  For each of your services it's probably worth having a catch all ruleset. For instance you might create a ruleset for  Nginx  and put some criteria in there for the service status being alive and perhaps requests per second being below a certain threshold.  You should create different rulesets depending on who gets alerted. For instance you might create a ruleset for todo,stag,web that simply emails a group email address when alert criteria are triggered. Whereas your ruleset for todo,prod,web might send emails to pagerduty so that people get an SMS and woken up.",
            "title": "Rules and Alerts"
        },
        {
            "location": "/getting_started/use_cases/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nUse Cases\n\u00b6\n\n\nOutlyer supports several ways to collect data. We are typically used as a central aggregation point for consolidating many tools into a single platform. This document describes best practices for which collection method we recommend depending upon use case.\n\n\nServer Monitoring\n\u00b6\n\n\nInstall a Dataloop Agent onto each server. This lets Outlyer know if your server is up or down and collects operating system metrics like CPU, Disk, Memory and Network.\n\n\nService Monitoring\n\u00b6\n\n\nFor common services Outyer provides \npacks\n which include a plugin, dashboard and alert rules. \n\n\nCustom Service Monitoring\n\u00b6\n\n\nCreate custom Nagios check scripts and apply them to agents using tags. Build dashboards and alert rules based on tags. To automate sharing configuration between teams, create private packs.\n\n\nDocker Monitoring\n\u00b6\n\n\nInstall a Dataloop Agent onto each Docker host to collect host level metrics and run Nagios plugins for service monitoring. Also run an outlyer/agent container on each Docker host to collect container resource metrics.\n\n\nApplication Metrics\n\u00b6\n\n\nInstrument your application with a Prometheus client library. Install a Dataloop Agent onto each server and scrape the Prometheus /metrics endpoint using a Prometheus format plugin.\n\n\nhttps://prometheus.io/docs/instrumenting/clientlibs/\n\n\nApplications Instrumented with StatsD\n\u00b6\n\n\nRun the Prometheus StatsD exporter on each StatsD server.\n\n\nhttps://github.com/prometheus/statsd_exporter\n\n\nConfigure StatsD to send metrics into the StatsD exporter. Scrape the Prometheus metrics endpoint on the StatsD exporter using a Dataloop Agent and Prometheus format plugin: \n\n\nStatsD, Tags and Outlyer\n\n\nLong Running Jobs\n\u00b6\n\n\nFor tasks that take a long time to complete write the output to a local file and monitor it with a Dataloop Agent and custom Nagios check script.\n\n\nEvent Driven Jobs\n\u00b6\n\n\nFor tasks that are initiated as a result of an external event like CI builds or cron jobs push Graphite line protocol metrics to graphite.dataloop.io on TCP port 2003 using netcat.\n\n\nhttps://docs.outlyer.com/endpoints/graphite/\n\n\nRemote Monitoring\n\u00b6\n\n\nUse the Dataloop Agent and custom Nagios plugins to remotely monitor over the network. For remote website monitoring designate a single Dataloop (Outlyer) Agent to run the plugin. For monitoring external services like AWS RDS run the plugin from the application servers that access it. Where configuration varies between agents deploy a YAML file with settings from configuration management and read the file on each plugin run.\n\n\nAggregation\n\u00b6\n\n\nMany open source collection tools like CollectD support Graphite output. These collectors can be configured to send metrics into Outlyer via our Graphite port.",
            "title": "Typical use cases"
        },
        {
            "location": "/getting_started/use_cases/#use-cases",
            "text": "Outlyer supports several ways to collect data. We are typically used as a central aggregation point for consolidating many tools into a single platform. This document describes best practices for which collection method we recommend depending upon use case.",
            "title": "Use Cases"
        },
        {
            "location": "/getting_started/use_cases/#server-monitoring",
            "text": "Install a Dataloop Agent onto each server. This lets Outlyer know if your server is up or down and collects operating system metrics like CPU, Disk, Memory and Network.",
            "title": "Server Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#service-monitoring",
            "text": "For common services Outyer provides  packs  which include a plugin, dashboard and alert rules.",
            "title": "Service Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#custom-service-monitoring",
            "text": "Create custom Nagios check scripts and apply them to agents using tags. Build dashboards and alert rules based on tags. To automate sharing configuration between teams, create private packs.",
            "title": "Custom Service Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#docker-monitoring",
            "text": "Install a Dataloop Agent onto each Docker host to collect host level metrics and run Nagios plugins for service monitoring. Also run an outlyer/agent container on each Docker host to collect container resource metrics.",
            "title": "Docker Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#application-metrics",
            "text": "Instrument your application with a Prometheus client library. Install a Dataloop Agent onto each server and scrape the Prometheus /metrics endpoint using a Prometheus format plugin.  https://prometheus.io/docs/instrumenting/clientlibs/",
            "title": "Application Metrics"
        },
        {
            "location": "/getting_started/use_cases/#applications-instrumented-with-statsd",
            "text": "Run the Prometheus StatsD exporter on each StatsD server.  https://github.com/prometheus/statsd_exporter  Configure StatsD to send metrics into the StatsD exporter. Scrape the Prometheus metrics endpoint on the StatsD exporter using a Dataloop Agent and Prometheus format plugin:   StatsD, Tags and Outlyer",
            "title": "Applications Instrumented with StatsD"
        },
        {
            "location": "/getting_started/use_cases/#long-running-jobs",
            "text": "For tasks that take a long time to complete write the output to a local file and monitor it with a Dataloop Agent and custom Nagios check script.",
            "title": "Long Running Jobs"
        },
        {
            "location": "/getting_started/use_cases/#event-driven-jobs",
            "text": "For tasks that are initiated as a result of an external event like CI builds or cron jobs push Graphite line protocol metrics to graphite.dataloop.io on TCP port 2003 using netcat.  https://docs.outlyer.com/endpoints/graphite/",
            "title": "Event Driven Jobs"
        },
        {
            "location": "/getting_started/use_cases/#remote-monitoring",
            "text": "Use the Dataloop Agent and custom Nagios plugins to remotely monitor over the network. For remote website monitoring designate a single Dataloop (Outlyer) Agent to run the plugin. For monitoring external services like AWS RDS run the plugin from the application servers that access it. Where configuration varies between agents deploy a YAML file with settings from configuration management and read the file on each plugin run.",
            "title": "Remote Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#aggregation",
            "text": "Many open source collection tools like CollectD support Graphite output. These collectors can be configured to send metrics into Outlyer via our Graphite port.",
            "title": "Aggregation"
        },
        {
            "location": "/getting_started/where_we_fit/",
            "text": "Where We Fit in the Monitoring Landscape\n\u00b6\n\n\nWe are a platform for time-series data visualisation and alerting. You can send us any volume of data and we will aim to process and display it in real time with seamless historical querying. Our platform is agnostic in terms of what time-series data you send to us. However, we spend most of our time helping SaaS companies scale their online services. This generally includes operating system, application and business level metrics; hence a focus on plugins, interfaces and integrations that make sense in that context.\n\n\nWe believe in using a few best of breed tools rather than a single mediocre unified tool. With open standards protocols and APIs, you can easily join tools together to prevent vendor lock-in and future-proof your monitoring stack.\n\n\nLogs\n\u00b6\n\n\nWe don't accept log data. Other tools such as \nELK\n, \nSplunk\n and \nGraylog\n do a great job at processing, indexing and querying logs. However, we do integrate with these systems. In the case of the ELK stack, you can use the StatsD forwarder to send time-series metrics from your logging pipeline to us via our Graphite port. For Splunk we have plugins that can scrape the API.\n\n\nAPM\n\u00b6\n\n\nWe don't provide tools that profile or introspect your production applications. Tools like \nNew Relic\n and \nAppDynamics\n do a good job here. We integrate with these tools via scripts that poll their API.\n\n\nStatsD\n\u00b6\n\n\nWhere it gets slightly confusing is with \nStatsD\n. We do support StatsD metrics which a lot of companies use to send back data from their live production apps. These are custom metrics that have to be specifically configured within the application code.\n\n\nAPM and StatsD are best used in combination. Use an APM tool to profile your application and provide top 10s and drill-downs into slow transactions. Use StatsD custom metrics when you want to track a particular thing, like the performance of a section of code, business metrics like signups or even feature usage.\n\n\nOn-Call Management\n\u00b6\n\n\nManaging on-call scheduling, nagging, escalation and policies isn't our specialty at this time. We integrate with other tools like \nPagerDuty\n and \nOpsGenie\n for those features.\n\n\nException Tracking\n\u00b6\n\n\nTools like Bugsnag will email your developers directly with stack traces caused by users in production. We're not that type of tool.\n\n\nIncident Management Tools\n\u00b6\n\n\nWe focus on gathering data, visualizing it and alerting. We don't help manage incidents in the way that tools like \nBigPanda\n do. If you have a wide variety of monitoring tools then our alerts could be configured to send to incident management tools for correlation.",
            "title": "Where we fit"
        },
        {
            "location": "/getting_started/where_we_fit/#where-we-fit-in-the-monitoring-landscape",
            "text": "We are a platform for time-series data visualisation and alerting. You can send us any volume of data and we will aim to process and display it in real time with seamless historical querying. Our platform is agnostic in terms of what time-series data you send to us. However, we spend most of our time helping SaaS companies scale their online services. This generally includes operating system, application and business level metrics; hence a focus on plugins, interfaces and integrations that make sense in that context.  We believe in using a few best of breed tools rather than a single mediocre unified tool. With open standards protocols and APIs, you can easily join tools together to prevent vendor lock-in and future-proof your monitoring stack.",
            "title": "Where We Fit in the Monitoring Landscape"
        },
        {
            "location": "/getting_started/where_we_fit/#logs",
            "text": "We don't accept log data. Other tools such as  ELK ,  Splunk  and  Graylog  do a great job at processing, indexing and querying logs. However, we do integrate with these systems. In the case of the ELK stack, you can use the StatsD forwarder to send time-series metrics from your logging pipeline to us via our Graphite port. For Splunk we have plugins that can scrape the API.",
            "title": "Logs"
        },
        {
            "location": "/getting_started/where_we_fit/#apm",
            "text": "We don't provide tools that profile or introspect your production applications. Tools like  New Relic  and  AppDynamics  do a good job here. We integrate with these tools via scripts that poll their API.",
            "title": "APM"
        },
        {
            "location": "/getting_started/where_we_fit/#statsd",
            "text": "Where it gets slightly confusing is with  StatsD . We do support StatsD metrics which a lot of companies use to send back data from their live production apps. These are custom metrics that have to be specifically configured within the application code.  APM and StatsD are best used in combination. Use an APM tool to profile your application and provide top 10s and drill-downs into slow transactions. Use StatsD custom metrics when you want to track a particular thing, like the performance of a section of code, business metrics like signups or even feature usage.",
            "title": "StatsD"
        },
        {
            "location": "/getting_started/where_we_fit/#on-call-management",
            "text": "Managing on-call scheduling, nagging, escalation and policies isn't our specialty at this time. We integrate with other tools like  PagerDuty  and  OpsGenie  for those features.",
            "title": "On-Call Management"
        },
        {
            "location": "/getting_started/where_we_fit/#exception-tracking",
            "text": "Tools like Bugsnag will email your developers directly with stack traces caused by users in production. We're not that type of tool.",
            "title": "Exception Tracking"
        },
        {
            "location": "/getting_started/where_we_fit/#incident-management-tools",
            "text": "We focus on gathering data, visualizing it and alerting. We don't help manage incidents in the way that tools like  BigPanda  do. If you have a wide variety of monitoring tools then our alerts could be configured to send to incident management tools for correlation.",
            "title": "Incident Management Tools"
        },
        {
            "location": "/getting_started/data_retention_policy/",
            "text": "Data Retention Policy\n\u00b6\n\n\nOutlyer supports Nagios check scripts which send back metrics at 30 second intervals (by default) and Graphite / StatsD metrics which are 1 second resolution.\n\n\nWe store:\n\n\n* 10 second resolution for 13 months for statsd metrics\n* 30 second resolution for 13 months for plugins metrics\n\n\n\n\n\n10 second resolution is really as low as you need go to debug most issues as they happen. Anything more granular will require you to be on servers debugging live.",
            "title": "Data retention policy"
        },
        {
            "location": "/getting_started/data_retention_policy/#data-retention-policy",
            "text": "Outlyer supports Nagios check scripts which send back metrics at 30 second intervals (by default) and Graphite / StatsD metrics which are 1 second resolution.  We store:  * 10 second resolution for 13 months for statsd metrics\n* 30 second resolution for 13 months for plugins metrics  10 second resolution is really as low as you need go to debug most issues as they happen. Anything more granular will require you to be on servers debugging live.",
            "title": "Data Retention Policy"
        },
        {
            "location": "/getting_started/faq/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nFAQ\n\u00b6\n\n\nOutlyer was built to be the fastest and simplest way to set up monitoring for your rapidly changing infrastructure. We\u2019re constantly looking for ways to help users get up to speed in minutes and this FAQ is our first step.\n\n\n\n\nQ. My question isn\u2019t covered here, where can I get help?\n\n\n\n\nA. Email us at \nsupport[at]outlyer.com\n or \njoin our Slack room\n\n\n\n\n\n\nQ. I signed up and logged in for the first time but I\u2019m confused. How  do I start monitoring?\n\n\n\n\nA. The best way to start is to install an Dataloop (Outlyer) Agent via curl onto a test virtual machine and have a play with installing packs and writing your own plugins. If you get stuck contact support and we'll happily guide you through everything.\n\n\n\n\n\n\nQ. I'm sending metrics into Outlyer. How do I browse them?\n\n\n\n\nA. You can browse your metrics on the dashboards and rules pages.\n\n\n\n\nCreate a new dashboard and then click on the + widget to bring up the metric browser. Click on one tag, or a combination of tags, or a single Dataloop (Outlyer) Agent and click 'select'. You will now be shown a tree of metrics available across the agents selected.\n\n\nThe metrics you see will be determined by the Nagios plugins running and the Graphite or StatsD metrics you are sending in. You can now tick one or many metrics to display on graph and number widgets.\n\n\nOn the rules page add a new rule and then add a new criteria. You will be presented with several drop downs that allow metric browsing and selection.\n\n\n\n\nQ. I can't see my StatsD metrics. Where should they appear?\n\n\n\n\nA. If you are using the Outlyer-hosted StatsD, then metrics can be browsed under the 'statsd' tag, or by browsing the 'dataloop' Dataloop (Outlyer) Agent directly. If you cannot see your StatsD metrics in the tree then check your client settings or contact us for support by email or Slack.\n\n\n\n\nIf you have setup your own StatsD server then metrics will appear under whatever Dataloop (Outlyer) Agent fingerprint they are being sent from.\n\n\n\n\nQ. How do I troubleshoot what the Dataloop (Outlyer) Agent is doing?\n\n\n\n\nA. You\u2019ll hopefully never have to troubleshoot, but just in case..\n\n\n\n\nThe Dataloop (Outlyer) Agent installer creates a \u2018dataloop\u2019 directory in /var/log. In this directory you\u2019ll find an agent.log file that shows you exactly what the Dataloop (Outlyer) Agent is doing.\n\n\nYou can increase the logging information by turning on debugging. In the agent.yaml configuration file, set debug to yes:\n\n\ndebug: yes\n\n\n\n\n\nTo restart the Dataloop (Outlyer) Agent just run:\n\n\nservice dataloop-agent restart\n\n\n\n\n\nOn Windows the installer creates a service called \u2018Dataloop\u2019. To restart the Dataloop (Outlyer) Agent just restart the Windows service from computer management. Windows puts the log file in C:\\Dataloop.\n\n\n\n\nQ. How does the Agent connect to the web interface? Do I need to open any ports?\n\n\n\n\nA. The Dataloop (Outlyer) Agent connects outbound on tcp port 443 to agent.dataloop.io.\n\n\n\n\n\n\nQ. Can I connect the Agent to the internet via a proxy?\n\n\n\n\nA. Yes, export the HTTPS_PROXY environment variable in /etc/default/dataloop-agent and restart the dataloop-agent service\n\n\n\n\n\n\nQ. What data are you collecting from me?\n\n\n\n\nA. The Dataloop (Outlyer) Agent sends the hostname, mac address, fingerprint (a random unique string that we create to identify your server) and the data from your scripts. We also send back a list of processes (process.name from the psutils library) and some metadata.\n\n\n\n\nWe use Nagios format scripts so the output is limited to 0,1,2 and 3 for alert status and something=value for the performance metrics. What gets sent back will largely be decided by you when you assign plugins.\n\n\n\n\nQ. How exactly do you keep up with the rate of change in my dynamic environments?\n\n\n\n\nA. All configuration is driven by tags. Simply run dataloop-agent --add-tags on the command line (or automate it via config management). Plugin deployment, dashboards and alerts are all bound to tags so purely by manipulating what tags your Dataloop (Outlyer) Agent belongs to we keep up with change.\n\n\n\n\nWe provide secure Dataloop (Outlyer) Agent registration, de-registration and presence detection. Simply run dataloop-agent --deregister to remove your Dataloop (Outlyer) Agent from the system.\n\n\nWe also support Dataloop (Outlyer) Agent finger printing so that you can recreate nodes and reattach to old metrics. This is especially useful for phoenix deployments.\n\n\nBy default agents will deregister on graceful shutdown which works well on AWS with auto scaling groups. You can spin up and down instances and your dashboards and alert rules will dynamically update.\n\n\n\n\nQ. Where is your product roadmap?\n\n\n\n\nA. We are very customer focused and are constantly iterating the product based on feedback. You can see and vote on which features you would like to see in Outlyer (formerly Dataloop.IO - thanks for your patience while we are rebranding) \nhere\n\n\n\n\n\n\nQ. What are shells?\n\n\n\n\nA. Something we added mostly for Windows.\n\n\n\n\nLinux and OSX run scripts with a shebang at the top of the file. Windows treats everything as if it\u2019s a .cmd or a .exe. For those wanting to run Powershell or WSF / vbs / js files you can setup shell paths under the settings page. You could for instance setup a WSF shell pointing to c:\\windows\\system32\\cscript.exe and apply it to your WSF script using the shell dropdown in the editor.\n\n\nShells can also be used on Linux / OSX and we\u2019ve found them very helpful for running PhantomJS scripts.\n\n\n\n\nQ. How do I remove the agent?\n\n\n\n\nA. On Linux remove the dataloop-agent package\n\n\n\n\nOn Windows remove the Dataloop (Outlyer) Agent via the add/remove programs interface.\n\n\n\n\nQ. What\u2019s the quickest way to write Nagios plugins?\n\n\n\n\nA. If you\u2019re using Linux then you\u2019re spoiled for choice as you have Bash, Ruby and Python installed by default. Pick your favourite scripting language and start writing.\n\n\n\n\nOn Windows you can use batch, or setup a shell for Powershell. Or, alternately use our built in Python interpreter by selecting 'Built-In Python' from the drop down at the top of the plugin editor page.\n\n\nYou can find the Nagios Plugin Development Guide here: link\n\n\nIf you need any help with writing scripts let us know.\n\n\n\n\nQ. Will you support other operating systems and architectures?\n\n\n\n\nA. We\u2019ll probably only ever create installers for Linux and Windows. However, for those that want to install our Dataloop (Outlyer) Agent on more exotic platforms (like Raspberry Pi or AIX / HP-UX / Solaris etc) we may be open to providing special builds. The Dataloop (Outlyer) Agent is written in Python so in theory should run everywhere. If you\u2019d like another operating system package please contact us.\n\n\n\n\n\n\nQ. How can I secure the Agent?\n\n\n\n\nA. We have a support page for security.\n\n\n\n\n\n\nQ. The share dashboard button has gone missing. How can I get it back?\n\n\n\n\nA. This can happen when you have browser extensions like Adblock installed\n\n\n\n\n\n\nQ. Why are my alerts all grey and not triggering?\n\n\n\n\nA. Alerts are grey when any agent in the alert criteria is not returning data for the defined metric path.  \n\n\n\n\nCorrect this by removing the offending agents, or ensure the agent returns the metric for the criteria.\n\n\nPlease see the \nrules\n page fo rmore details about alerts.",
            "title": "FAQ"
        },
        {
            "location": "/getting_started/faq/#faq",
            "text": "Outlyer was built to be the fastest and simplest way to set up monitoring for your rapidly changing infrastructure. We\u2019re constantly looking for ways to help users get up to speed in minutes and this FAQ is our first step.   Q. My question isn\u2019t covered here, where can I get help?   A. Email us at  support[at]outlyer.com  or  join our Slack room    Q. I signed up and logged in for the first time but I\u2019m confused. How  do I start monitoring?   A. The best way to start is to install an Dataloop (Outlyer) Agent via curl onto a test virtual machine and have a play with installing packs and writing your own plugins. If you get stuck contact support and we'll happily guide you through everything.    Q. I'm sending metrics into Outlyer. How do I browse them?   A. You can browse your metrics on the dashboards and rules pages.   Create a new dashboard and then click on the + widget to bring up the metric browser. Click on one tag, or a combination of tags, or a single Dataloop (Outlyer) Agent and click 'select'. You will now be shown a tree of metrics available across the agents selected.  The metrics you see will be determined by the Nagios plugins running and the Graphite or StatsD metrics you are sending in. You can now tick one or many metrics to display on graph and number widgets.  On the rules page add a new rule and then add a new criteria. You will be presented with several drop downs that allow metric browsing and selection.   Q. I can't see my StatsD metrics. Where should they appear?   A. If you are using the Outlyer-hosted StatsD, then metrics can be browsed under the 'statsd' tag, or by browsing the 'dataloop' Dataloop (Outlyer) Agent directly. If you cannot see your StatsD metrics in the tree then check your client settings or contact us for support by email or Slack.   If you have setup your own StatsD server then metrics will appear under whatever Dataloop (Outlyer) Agent fingerprint they are being sent from.   Q. How do I troubleshoot what the Dataloop (Outlyer) Agent is doing?   A. You\u2019ll hopefully never have to troubleshoot, but just in case..   The Dataloop (Outlyer) Agent installer creates a \u2018dataloop\u2019 directory in /var/log. In this directory you\u2019ll find an agent.log file that shows you exactly what the Dataloop (Outlyer) Agent is doing.  You can increase the logging information by turning on debugging. In the agent.yaml configuration file, set debug to yes:  debug: yes  To restart the Dataloop (Outlyer) Agent just run:  service dataloop-agent restart  On Windows the installer creates a service called \u2018Dataloop\u2019. To restart the Dataloop (Outlyer) Agent just restart the Windows service from computer management. Windows puts the log file in C:\\Dataloop.   Q. How does the Agent connect to the web interface? Do I need to open any ports?   A. The Dataloop (Outlyer) Agent connects outbound on tcp port 443 to agent.dataloop.io.    Q. Can I connect the Agent to the internet via a proxy?   A. Yes, export the HTTPS_PROXY environment variable in /etc/default/dataloop-agent and restart the dataloop-agent service    Q. What data are you collecting from me?   A. The Dataloop (Outlyer) Agent sends the hostname, mac address, fingerprint (a random unique string that we create to identify your server) and the data from your scripts. We also send back a list of processes (process.name from the psutils library) and some metadata.   We use Nagios format scripts so the output is limited to 0,1,2 and 3 for alert status and something=value for the performance metrics. What gets sent back will largely be decided by you when you assign plugins.   Q. How exactly do you keep up with the rate of change in my dynamic environments?   A. All configuration is driven by tags. Simply run dataloop-agent --add-tags on the command line (or automate it via config management). Plugin deployment, dashboards and alerts are all bound to tags so purely by manipulating what tags your Dataloop (Outlyer) Agent belongs to we keep up with change.   We provide secure Dataloop (Outlyer) Agent registration, de-registration and presence detection. Simply run dataloop-agent --deregister to remove your Dataloop (Outlyer) Agent from the system.  We also support Dataloop (Outlyer) Agent finger printing so that you can recreate nodes and reattach to old metrics. This is especially useful for phoenix deployments.  By default agents will deregister on graceful shutdown which works well on AWS with auto scaling groups. You can spin up and down instances and your dashboards and alert rules will dynamically update.   Q. Where is your product roadmap?   A. We are very customer focused and are constantly iterating the product based on feedback. You can see and vote on which features you would like to see in Outlyer (formerly Dataloop.IO - thanks for your patience while we are rebranding)  here    Q. What are shells?   A. Something we added mostly for Windows.   Linux and OSX run scripts with a shebang at the top of the file. Windows treats everything as if it\u2019s a .cmd or a .exe. For those wanting to run Powershell or WSF / vbs / js files you can setup shell paths under the settings page. You could for instance setup a WSF shell pointing to c:\\windows\\system32\\cscript.exe and apply it to your WSF script using the shell dropdown in the editor.  Shells can also be used on Linux / OSX and we\u2019ve found them very helpful for running PhantomJS scripts.   Q. How do I remove the agent?   A. On Linux remove the dataloop-agent package   On Windows remove the Dataloop (Outlyer) Agent via the add/remove programs interface.   Q. What\u2019s the quickest way to write Nagios plugins?   A. If you\u2019re using Linux then you\u2019re spoiled for choice as you have Bash, Ruby and Python installed by default. Pick your favourite scripting language and start writing.   On Windows you can use batch, or setup a shell for Powershell. Or, alternately use our built in Python interpreter by selecting 'Built-In Python' from the drop down at the top of the plugin editor page.  You can find the Nagios Plugin Development Guide here: link  If you need any help with writing scripts let us know.   Q. Will you support other operating systems and architectures?   A. We\u2019ll probably only ever create installers for Linux and Windows. However, for those that want to install our Dataloop (Outlyer) Agent on more exotic platforms (like Raspberry Pi or AIX / HP-UX / Solaris etc) we may be open to providing special builds. The Dataloop (Outlyer) Agent is written in Python so in theory should run everywhere. If you\u2019d like another operating system package please contact us.    Q. How can I secure the Agent?   A. We have a support page for security.    Q. The share dashboard button has gone missing. How can I get it back?   A. This can happen when you have browser extensions like Adblock installed    Q. Why are my alerts all grey and not triggering?   A. Alerts are grey when any agent in the alert criteria is not returning data for the defined metric path.     Correct this by removing the offending agents, or ensure the agent returns the metric for the criteria.  Please see the  rules  page fo rmore details about alerts.",
            "title": "FAQ"
        },
        {
            "location": "/agent/",
            "text": "Installation\n\u00b6\n\n\n\n\n\n\nSupported operating systems\n and\n   prerequisites for installation.\n\n\n\n\n\n\nInstalling agents on \nLinux\n,\n   \nWindows\n,\n   \nDocker\n, or\n   \nRaspberry Pi\n platforms.\n\n\n\n\n\n\nDeploying agents with \nconfiguration management\n\n   tools like Chef, Puppet, Ansible, or SaltStack.\n\n\n\n\n\n\nUninstalling the agent\n.\n\n\n\n\n\n\nConfiguration file reference\n.\n\n\n\n\n\n\nSolo Mode\n.\n\n\n\n\n\n\nCommand Line Interface\n.\n\n\n\n\n\n\nAgent fingerprints\n.",
            "title": "Introduction"
        },
        {
            "location": "/agent/#installation",
            "text": "Supported operating systems  and\n   prerequisites for installation.    Installing agents on  Linux ,\n    Windows ,\n    Docker , or\n    Raspberry Pi  platforms.    Deploying agents with  configuration management \n   tools like Chef, Puppet, Ansible, or SaltStack.    Uninstalling the agent .    Configuration file reference .    Solo Mode .    Command Line Interface .    Agent fingerprints .",
            "title": "Installation"
        },
        {
            "location": "/agent/supported_operating_systems/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our Dataloop (Outlyer) Agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nSupported Operating Systems\n\u00b6\n\n\nWe build the Dataloop Dataloop (Outlyer) Agent on Centos 6 and Debian 7 which means that anything released in May 2010 or later is supported.\n\n\n\n\nSometimes we get requests for older servers. This page is the resting place for hacks to get the agents working in extreme cases. They are officially unsupported, but we understand the pressures that sysadmins are put under to support oddities, so we will help out on a best effort basis.\n\n\nIf you have a unique snowflake running an operating system released over 5 years ago then hit us up on \nSlack\n.",
            "title": "Supported platforms"
        },
        {
            "location": "/agent/supported_operating_systems/#supported-operating-systems",
            "text": "We build the Dataloop Dataloop (Outlyer) Agent on Centos 6 and Debian 7 which means that anything released in May 2010 or later is supported.   Sometimes we get requests for older servers. This page is the resting place for hacks to get the agents working in extreme cases. They are officially unsupported, but we understand the pressures that sysadmins are put under to support oddities, so we will help out on a best effort basis.  If you have a unique snowflake running an operating system released over 5 years ago then hit us up on  Slack .",
            "title": "Supported Operating Systems"
        },
        {
            "location": "/agent/installation_linux/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nInstallation on Linux\n\u00b6\n\n\nDebian based Linux (Debian, Ubuntu etc)\n\u00b6\n\n\n\n\nImport the apt repository gpg key\n\n\n\n\ncurl -s https://download.dataloop.io/pubkey.gpg | apt-key add -\n\n\n\n\n\n\n\nAdd the Dataloop apt repository\n\n\n\n\necho 'deb https://download.dataloop.io/deb/ stable main' > /etc/apt/sources.list.d/dataloop.list\n\n\n\n\n\n\n\nInstall the dataloop agent\n\n\n\n\nsudo apt-get update && sudo apt-get install dataloop-agent\n\n\n\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the agent\n\n\n\n\n\n\nsudo service dataloop-agent start\n\n\n\n\n\n\n\nSet the Dataloop (Outlyer) Agent to run on reboot\n\n\n\n\nsudo update-rc.d dataloop-agent defaults\n\n\n\n\n\n\n\nRedhat based Linux (Redhat, Centos etc)\n\u00b6\n\n\n\n\nAdd the Dataloop yum repository\n\n\n\n\nPaste the following into /etc/yum.repos.d/dataloop.repo\n\n\n[dataloop]\nname=Dataloop Agent\nbaseurl=https://download.dataloop.io/packages/stable/el$releasever/$basearch/\nenabled=1\ngpgcheck=0\n\n\n\n\n\n\n\nInstall the Dataloop Agent\n\n\n\n\nsudo yum install dataloop-agent\n\n\n\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the Dataloop Agent service\n\n\n\n\n\n\nsudo service dataloop-agent start\n\n\n\n\n\n\n\nSet the Dataloop (Outlyer) Agent to run on reboot\n\n\n\n\nsudo chkconfig --level 345 dataloop-agent on\n\n\n\n\n\n\n\nSuse based Linux\n\u00b6\n\n\n\n\nDownload the RPM\n\n\n\n\nwget https://download.dataloop.io/suse/x86_64/dataloop-agent_latest-suse-x86_64.rpm\n\n\n\n\n\n\n\nInstall it\n\n\n\n\nsudo rpm -i dataloop-agent-1.1.18-1.x86_64.rpm\n\n\n\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the Dataloop Agent service\n\n\n\n\n\n\nsudo systemctl start dataloop-agent\n\n\n\n\n\n\n\nSet the Dataloop (Outlyer) Agent to run on reboot\n\n\n\n\nsudo systemctl enable dataloop-agent\n\n\n\n\n\n\n\nArchlinux based Linux (x86_64)\n\u00b6\n\n\n\n\nDownload the PKG\n\n\n\n\nwget https://download.dataloop.io/archlinux/dataloop-agent_latest-archlinux-x86_64.pkg.tar.xz\n\n\n\n\n\n\n\nInstall it\n\n\n\n\npacman -U dataloop-agent-1.1.20-1-x86_64.pkg.tar.xz\n\n\n\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the Dataloop Agent service\n\n\n\n\n\n\nsudo systemctl start dataloop-agent\n\n\n\n\n\n\n\nSet the Dataloop (Outlyer) Agent to run on reboot\n\n\n\n\nsudo systemctl enable dataloop-agent\n\n\n\n\n\nAlternatively you can make the package yourself by following the instructions at https://github.com/dataloop/dataloop-archlinux\n\n\n\n\nCurl sudo bash installer (only recommended on test machines)\n\u00b6\n\n\nThis essentially automates the above.\n\n\n\n\nRun the following command \n\n\n\n\ncurl -s https://download.dataloop.io/setup.sh | sudo bash -s xxx\n\n\n\n\n\nWhere xxx is your API key that you would have been emailed on signup. Or you can log into the web interface and click 'Install New Agent' on the 'Setup Monitoring' page.",
            "title": "Installing on Linux"
        },
        {
            "location": "/agent/installation_linux/#installation-on-linux",
            "text": "",
            "title": "Installation on Linux"
        },
        {
            "location": "/agent/installation_linux/#debian-based-linux-debian-ubuntu-etc",
            "text": "Import the apt repository gpg key   curl -s https://download.dataloop.io/pubkey.gpg | apt-key add -   Add the Dataloop apt repository   echo 'deb https://download.dataloop.io/deb/ stable main' > /etc/apt/sources.list.d/dataloop.list   Install the dataloop agent   sudo apt-get update && sudo apt-get install dataloop-agent    Update the /etc/dataloop/agent.yaml file with your API key    Start the agent    sudo service dataloop-agent start   Set the Dataloop (Outlyer) Agent to run on reboot   sudo update-rc.d dataloop-agent defaults",
            "title": "Debian based Linux (Debian, Ubuntu etc)"
        },
        {
            "location": "/agent/installation_linux/#redhat-based-linux-redhat-centos-etc",
            "text": "Add the Dataloop yum repository   Paste the following into /etc/yum.repos.d/dataloop.repo  [dataloop]\nname=Dataloop Agent\nbaseurl=https://download.dataloop.io/packages/stable/el$releasever/$basearch/\nenabled=1\ngpgcheck=0   Install the Dataloop Agent   sudo yum install dataloop-agent    Update the /etc/dataloop/agent.yaml file with your API key    Start the Dataloop Agent service    sudo service dataloop-agent start   Set the Dataloop (Outlyer) Agent to run on reboot   sudo chkconfig --level 345 dataloop-agent on",
            "title": "Redhat based Linux (Redhat, Centos etc)"
        },
        {
            "location": "/agent/installation_linux/#suse-based-linux",
            "text": "Download the RPM   wget https://download.dataloop.io/suse/x86_64/dataloop-agent_latest-suse-x86_64.rpm   Install it   sudo rpm -i dataloop-agent-1.1.18-1.x86_64.rpm    Update the /etc/dataloop/agent.yaml file with your API key    Start the Dataloop Agent service    sudo systemctl start dataloop-agent   Set the Dataloop (Outlyer) Agent to run on reboot   sudo systemctl enable dataloop-agent",
            "title": "Suse based Linux"
        },
        {
            "location": "/agent/installation_linux/#archlinux-based-linux-x86_64",
            "text": "Download the PKG   wget https://download.dataloop.io/archlinux/dataloop-agent_latest-archlinux-x86_64.pkg.tar.xz   Install it   pacman -U dataloop-agent-1.1.20-1-x86_64.pkg.tar.xz    Update the /etc/dataloop/agent.yaml file with your API key    Start the Dataloop Agent service    sudo systemctl start dataloop-agent   Set the Dataloop (Outlyer) Agent to run on reboot   sudo systemctl enable dataloop-agent  Alternatively you can make the package yourself by following the instructions at https://github.com/dataloop/dataloop-archlinux",
            "title": "Archlinux based Linux (x86_64)"
        },
        {
            "location": "/agent/installation_linux/#curl-sudo-bash-installer-only-recommended-on-test-machines",
            "text": "This essentially automates the above.   Run the following command    curl -s https://download.dataloop.io/setup.sh | sudo bash -s xxx  Where xxx is your API key that you would have been emailed on signup. Or you can log into the web interface and click 'Install New Agent' on the 'Setup Monitoring' page.",
            "title": "Curl sudo bash installer (only recommended on test machines)"
        },
        {
            "location": "/agent/installation_windows/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nInstallation on Windows\n\u00b6\n\n\nWindows Systems\n\u00b6\n\n\n\n\nInstall the latest Dataloop Agent package from\n\n\n\n\nhttps://download.dataloop.io/windows/latest/dataloop-1.3-windows-installer.exe\n\n\nFor some systems you may need to install the Microsoft Visual C++ DLLs, they can be found here:\n\n\nhttp://www.microsoft.com/en-us/download/details.aspx?id=29\n\n\n\n\nSilent Installation\n\u00b6\n\n\nStarting with version 1.1.27 it is possible to install Dataloop-agent silently, passing the API Key to be put in the agent.yaml configuration file.\n\n\nThe command line usage for this is:\n\n\ndataloop-agent-1.1.27-1_x86.exe /S /apikey 1111-2222-3333-4444-5555",
            "title": "Installing on Windows"
        },
        {
            "location": "/agent/installation_windows/#installation-on-windows",
            "text": "",
            "title": "Installation on Windows"
        },
        {
            "location": "/agent/installation_windows/#windows-systems",
            "text": "Install the latest Dataloop Agent package from   https://download.dataloop.io/windows/latest/dataloop-1.3-windows-installer.exe  For some systems you may need to install the Microsoft Visual C++ DLLs, they can be found here:  http://www.microsoft.com/en-us/download/details.aspx?id=29",
            "title": "Windows Systems"
        },
        {
            "location": "/agent/installation_windows/#silent-installation",
            "text": "Starting with version 1.1.27 it is possible to install Dataloop-agent silently, passing the API Key to be put in the agent.yaml configuration file.  The command line usage for this is:  dataloop-agent-1.1.27-1_x86.exe /S /apikey 1111-2222-3333-4444-5555",
            "title": "Silent Installation"
        },
        {
            "location": "/agent/installation_raspberry_pi/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nInstallation on Raspberry Pi\n\u00b6\n\n\nCurrently, only Debian based Linux (Raspbian Jessie) running on Raspberry PI 2's (armv7) is supported.\n\n\nRaspbian Jessie:\n\u00b6\n\n\nFor the next 2 steps you must be logged in as root. You can do this via \nsu root\n. If you don't know your password for root you can easily update it using the command \nsudo passwd root\n to set it.\n\n\n\n\nImport the apt repository gpg key\n\n\n\n\ncurl -s https://download.dataloop.io/pubkey.gpg | apt-key add -\n\n\n\n\n\n\n\nAdd the Dataloop apt repository\n\n\n\n\necho 'deb https://download.dataloop.io/deb/ unstable main' > /etc/apt/sources.list.d/dataloop.list\n\n\n\n\n\n\n\nInstall the dataloop agent\n\n\n\n\nsudo apt-get update && sudo apt-get install dataloop-agent\n\n\n\n\n\nNote:\n If you get the error 'E: The method driver /usr/lib/apt/methods/https could not be found' then you can easily install it using the command 'sudo apt-get install apt-transport-https'.\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the agent\n\n\n\n\n\n\nsudo systemctl start dataloop-agent\n\n\n\n\n\n\n\nSet the Dataloop (Outlyer) Agent to run on reboot\n\n\n\n\nsudo systemctl enable dataloop-agent\n\n\n\n\n\n\n\nAccess GPIO Pins (Optional)\n\n\n\n\nFor security, the Dataloop (Outlyer) Agent runs as the non-privileged 'dataloop' user. By default this user will not have access to the GPIO pins on your PI required for reading and writing to sensors attached to your PI. In order to allow your Dataloop (Outlyer) Agent plugins to work with GPIO you will need to add the dataloop user to the 'gpio' group on your PI. Please note this will allow anyone running scripts on your Dataloop (Outlyer) Agent to read and write to your GPIO memory bus! To add the 'dataloop' user to the 'gpio' group type the following command:\n\n\nsudo adduser dataloop gpio\n\n\n\n\n\nMake sure you restart the Dataloop Dataloop (Outlyer) Agent afterwards to ensure the Dataloop (Outlyer) Agent permissions are correct:\n\n\nsudo systemctl restart dataloop-agent",
            "title": "Installing on Raspberry Pi"
        },
        {
            "location": "/agent/installation_raspberry_pi/#installation-on-raspberry-pi",
            "text": "Currently, only Debian based Linux (Raspbian Jessie) running on Raspberry PI 2's (armv7) is supported.",
            "title": "Installation on Raspberry Pi"
        },
        {
            "location": "/agent/installation_raspberry_pi/#raspbian-jessie",
            "text": "For the next 2 steps you must be logged in as root. You can do this via  su root . If you don't know your password for root you can easily update it using the command  sudo passwd root  to set it.   Import the apt repository gpg key   curl -s https://download.dataloop.io/pubkey.gpg | apt-key add -   Add the Dataloop apt repository   echo 'deb https://download.dataloop.io/deb/ unstable main' > /etc/apt/sources.list.d/dataloop.list   Install the dataloop agent   sudo apt-get update && sudo apt-get install dataloop-agent  Note:  If you get the error 'E: The method driver /usr/lib/apt/methods/https could not be found' then you can easily install it using the command 'sudo apt-get install apt-transport-https'.    Update the /etc/dataloop/agent.yaml file with your API key    Start the agent    sudo systemctl start dataloop-agent   Set the Dataloop (Outlyer) Agent to run on reboot   sudo systemctl enable dataloop-agent   Access GPIO Pins (Optional)   For security, the Dataloop (Outlyer) Agent runs as the non-privileged 'dataloop' user. By default this user will not have access to the GPIO pins on your PI required for reading and writing to sensors attached to your PI. In order to allow your Dataloop (Outlyer) Agent plugins to work with GPIO you will need to add the dataloop user to the 'gpio' group on your PI. Please note this will allow anyone running scripts on your Dataloop (Outlyer) Agent to read and write to your GPIO memory bus! To add the 'dataloop' user to the 'gpio' group type the following command:  sudo adduser dataloop gpio  Make sure you restart the Dataloop Dataloop (Outlyer) Agent afterwards to ensure the Dataloop (Outlyer) Agent permissions are correct:  sudo systemctl restart dataloop-agent",
            "title": "Raspbian Jessie:"
        },
        {
            "location": "/agent/installation_docker/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nOutlyer Agent Autodiscovery Container\n\u00b6\n\n\nThis container contains a Outlyer (Dataloop) agent. It will create virtual agents in Outlyer for each running container. Depending on which OS you are running on your Docker hosts you may need to add different run options.\n\n\nThe list of metrics returned for each running containers can be found \nhere\n.\n\n\nMost Linuxes\n\u00b6\n\n\nDATALOOP_AGENT_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nDATALOOP_NAME=docker_container_name\n\ndocker run -d \\\n-e \"DATALOOP_AGENT_KEY=${DATALOOP_AGENT_KEY}\" \\\n-e \"DATALOOP_NAME=${DATALOOP_NAME}\" \\\n-v /var/run/docker.sock:/var/run/docker.sock:ro \\\n-v /proc:/rootfs/proc:ro \\\n-v /sys/fs/cgroup:/rootfs/sys/fs/cgroup:ro \\\noutlyer/agent:latest\n\n\n\n\n\nAmazon Linux (ECS)\n\u00b6\n\n\nIf using an Amazon Linux AMI you will need to change the /cgroup volume mount location\n\n\nDATALOOP_AGENT_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nDATALOOP_NAME=docker_container_name\n\ndocker run -d \\\n-e \"DATALOOP_AGENT_KEY=${DATALOOP_AGENT_KEY}\" \\\n-e \"DATALOOP_NAME=${DATALOOP_NAME}\" \\\n-v /var/run/docker.sock:/var/run/docker.sock:ro \\\n-v /proc:/rootfs/proc:ro \\\n-v /cgroup:/rootfs/sys/fs/cgroup:ro \\\noutlyer/agent:latest\n\n\n\n\n\nCentOS 6\n\u00b6\n\n\nCentOS 6 supports Docker to version 1.7, whereat the platform appears to have been deprecated.\n\n\nYou can still run this container, if you install cgroups and adjust the volume mapping for the cgroups directory.\n\n\nsudo yum install -y libcgroup\n\n\n\n\n\nCentOS 6 has cgroups under \n/cgroups\n. Run the container accordingly\n\n\nDATALOOP_AGENT_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nDATALOOP_NAME=docker_container_name\n\ndocker run -d \\\n-e \"DATALOOP_AGENT_KEY=${DATALOOP_AGENT_KEY}\" \\\n-e \"DATALOOP_NAME=${DATALOOP_NAME}\" \\\n-v /var/run/docker.sock:/var/run/docker.sock:ro \\\n-v /proc:/rootfs/proc:ro \\\n-v /cgroup:/rootfs/sys/fs/cgroup:ro \\\noutlyer/agent:latest\n\n\n\n\n\nSupport Versions\n\u00b6\n\n\nThis container has been tested to run on Docker >= 1.7\n\n\nTroubleshooting\n\u00b6\n\n\nPlease contact us on \nhttps://slack.outlyer.com\n or \nsupport[at]outlyer.com\n for help.\n\n\nIf you dont see any memory metrics in your containers you will need to enable memory accounting in cgroups. To do that just add some kernel command-line parameters: cgroup_enable=memory swapaccount=1. More info from the \ndocker documentation\n.\n\n\n\n\nTip\n\n\nWhen running the agent with the above \ndocker run\n options, some features will be missing. Please run the container with \n--net host\n if you would like the following features:\n\n\n- CPU load average for your containers  \n- Container network connections  \n- Host network metrics sent under your agent container\n\n\n\n\n\nIf you'd like to have host network metrics sent under your agent container (rather than your actual container's network metrics. and you cannot run your container with \n--net host\n, please follow these steps:\n\n\n\n\nMount your host's sys folder on your agent container (e.g. pass \n-v /sys:/rootfs/sys\n to \ndocker run\n);\n\n\nOn your account's main overview page \nhttps://app.outlyer.com/YOUR_ORG/YOUR_ACCOUNT/#/setup/overview\n, click on \"Setup Monitoring\";\n\n\nClick on \"Plugins List\", then on \"base.py\".\n\n\nOn the code editor, there should be a function called \ncheck_netio\n. Replace it with the one here: \nhttps://gist.github.com/alexdias/d51ef12d02d9e8f56721c48d52517f5c\n\n\nClick \nSave changes\n to have your base plugin updated.\n\n\n\n\n\n\nProxy\n\u00b6\n\n\nIf you are behind a proxy server, you can pass the standard proxy environment variables to the docker container to have data passed through:\n\n\ndocker run -e HTTP_PROXY=http://proxy:port....\n\n\n\n\n\nEnvironment Variables\n\u00b6\n\n\nThis docker image will accept various other environment variables to allow you to customize the configuration for the dataloop-agent\n\n\nDATALOOP_AGENT_KEY\n \nRequired\n The API Key for your Outlyer account. \nDefault\n: None\n\n\nDATALOOP_NAME\n \nOptional\n A name for your agent to appear as in Outlyer. \nDefault\n: dataloop\n\n\nDATALOOP_TAGS\n \nOptional\n A comma separated list of tags to apply to the agent. \nDefault\n: docker\n\n\nDATALOOP_FINGERPRINT\n \nOptional\n You can pass an existing fingerprint if you want to keep your data association in Outlyer. \nDefault\n None\n\n\nDATALOOP_DEBUG\n \nOptional\n You can pass in the debug flag \nyes/no\n or \ntrue/false\n to add extra logging to the agent. \nDefault\n no\n\n\nExposed Ports\n\u00b6\n\n\nNone\n\n\nContributing Changes\n\u00b6\n\n\nIf you want to modify the container then feel free to submit a pull request.\n\n\nThis container is also provided as a base container running an Outlyer (Dataloop) agent. You can extend it with embedded plugins, or with Prometheus exporters, for example.\n\n\nBased upon \nAlpine Linux\n and using \nTini\n for process managment.\n\n\nPlease see \nhere\n for more information about Scott Mebberson's base images and design.",
            "title": "Installing on Docker"
        },
        {
            "location": "/agent/installation_docker/#outlyer-agent-autodiscovery-container",
            "text": "This container contains a Outlyer (Dataloop) agent. It will create virtual agents in Outlyer for each running container. Depending on which OS you are running on your Docker hosts you may need to add different run options.  The list of metrics returned for each running containers can be found  here .",
            "title": "Outlyer Agent Autodiscovery Container"
        },
        {
            "location": "/agent/installation_docker/#most-linuxes",
            "text": "DATALOOP_AGENT_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nDATALOOP_NAME=docker_container_name\n\ndocker run -d \\\n-e \"DATALOOP_AGENT_KEY=${DATALOOP_AGENT_KEY}\" \\\n-e \"DATALOOP_NAME=${DATALOOP_NAME}\" \\\n-v /var/run/docker.sock:/var/run/docker.sock:ro \\\n-v /proc:/rootfs/proc:ro \\\n-v /sys/fs/cgroup:/rootfs/sys/fs/cgroup:ro \\\noutlyer/agent:latest",
            "title": "Most Linuxes"
        },
        {
            "location": "/agent/installation_docker/#amazon-linux-ecs",
            "text": "If using an Amazon Linux AMI you will need to change the /cgroup volume mount location  DATALOOP_AGENT_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nDATALOOP_NAME=docker_container_name\n\ndocker run -d \\\n-e \"DATALOOP_AGENT_KEY=${DATALOOP_AGENT_KEY}\" \\\n-e \"DATALOOP_NAME=${DATALOOP_NAME}\" \\\n-v /var/run/docker.sock:/var/run/docker.sock:ro \\\n-v /proc:/rootfs/proc:ro \\\n-v /cgroup:/rootfs/sys/fs/cgroup:ro \\\noutlyer/agent:latest",
            "title": "Amazon Linux (ECS)"
        },
        {
            "location": "/agent/installation_docker/#centos-6",
            "text": "CentOS 6 supports Docker to version 1.7, whereat the platform appears to have been deprecated.  You can still run this container, if you install cgroups and adjust the volume mapping for the cgroups directory.  sudo yum install -y libcgroup  CentOS 6 has cgroups under  /cgroups . Run the container accordingly  DATALOOP_AGENT_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nDATALOOP_NAME=docker_container_name\n\ndocker run -d \\\n-e \"DATALOOP_AGENT_KEY=${DATALOOP_AGENT_KEY}\" \\\n-e \"DATALOOP_NAME=${DATALOOP_NAME}\" \\\n-v /var/run/docker.sock:/var/run/docker.sock:ro \\\n-v /proc:/rootfs/proc:ro \\\n-v /cgroup:/rootfs/sys/fs/cgroup:ro \\\noutlyer/agent:latest",
            "title": "CentOS 6"
        },
        {
            "location": "/agent/installation_docker/#support-versions",
            "text": "This container has been tested to run on Docker >= 1.7",
            "title": "Support Versions"
        },
        {
            "location": "/agent/installation_docker/#troubleshooting",
            "text": "Please contact us on  https://slack.outlyer.com  or  support[at]outlyer.com  for help.  If you dont see any memory metrics in your containers you will need to enable memory accounting in cgroups. To do that just add some kernel command-line parameters: cgroup_enable=memory swapaccount=1. More info from the  docker documentation .   Tip  When running the agent with the above  docker run  options, some features will be missing. Please run the container with  --net host  if you would like the following features:  - CPU load average for your containers  \n- Container network connections  \n- Host network metrics sent under your agent container  If you'd like to have host network metrics sent under your agent container (rather than your actual container's network metrics. and you cannot run your container with  --net host , please follow these steps:   Mount your host's sys folder on your agent container (e.g. pass  -v /sys:/rootfs/sys  to  docker run );  On your account's main overview page  https://app.outlyer.com/YOUR_ORG/YOUR_ACCOUNT/#/setup/overview , click on \"Setup Monitoring\";  Click on \"Plugins List\", then on \"base.py\".  On the code editor, there should be a function called  check_netio . Replace it with the one here:  https://gist.github.com/alexdias/d51ef12d02d9e8f56721c48d52517f5c  Click  Save changes  to have your base plugin updated.",
            "title": "Troubleshooting"
        },
        {
            "location": "/agent/installation_docker/#proxy",
            "text": "If you are behind a proxy server, you can pass the standard proxy environment variables to the docker container to have data passed through:  docker run -e HTTP_PROXY=http://proxy:port....",
            "title": "Proxy"
        },
        {
            "location": "/agent/installation_docker/#environment-variables",
            "text": "This docker image will accept various other environment variables to allow you to customize the configuration for the dataloop-agent  DATALOOP_AGENT_KEY   Required  The API Key for your Outlyer account.  Default : None  DATALOOP_NAME   Optional  A name for your agent to appear as in Outlyer.  Default : dataloop  DATALOOP_TAGS   Optional  A comma separated list of tags to apply to the agent.  Default : docker  DATALOOP_FINGERPRINT   Optional  You can pass an existing fingerprint if you want to keep your data association in Outlyer.  Default  None  DATALOOP_DEBUG   Optional  You can pass in the debug flag  yes/no  or  true/false  to add extra logging to the agent.  Default  no",
            "title": "Environment Variables"
        },
        {
            "location": "/agent/installation_docker/#exposed-ports",
            "text": "None",
            "title": "Exposed Ports"
        },
        {
            "location": "/agent/installation_docker/#contributing-changes",
            "text": "If you want to modify the container then feel free to submit a pull request.  This container is also provided as a base container running an Outlyer (Dataloop) agent. You can extend it with embedded plugins, or with Prometheus exporters, for example.  Based upon  Alpine Linux  and using  Tini  for process managment.  Please see  here  for more information about Scott Mebberson's base images and design.",
            "title": "Contributing Changes"
        },
        {
            "location": "/agent/config_management/",
            "text": "Chef, Ansible, Puppet and Salt\n\u00b6\n\n\nOutlyer currently has modules for Chef, Ansible, Puppet and Salt to help with rolling out the agent.\n\n\nYou can find these in our public GitHub account:\n\n\nhttps://github.com/outlyerapp",
            "title": "Deploying with a configuration management tool"
        },
        {
            "location": "/agent/config_management/#chef-ansible-puppet-and-salt",
            "text": "Outlyer currently has modules for Chef, Ansible, Puppet and Salt to help with rolling out the agent.  You can find these in our public GitHub account:  https://github.com/outlyerapp",
            "title": "Chef, Ansible, Puppet and Salt"
        },
        {
            "location": "/agent/fingerprints/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nFingerprints\n\u00b6\n\n\nWhen an Agent starts for the first time on a new server it creates an agent.finger file (in \n/etc/dataloop\n on Linux boxes and \nC:\\Dataloop\n on Windows). This is used to uniquely identify the Agent.\n\n\nDeleting the \nagent.finger\n file and restarting the service will cause the Dataloop (Outlyer) Agent to generate a new identity and it will lose its old metrics.\n\n\nConversely, starting servers with known fingerprint files allows you to connect new boxes to existing metrics which is extremely useful for dynamically changing environments where configuration management tools are used.\n\n\nExample\n\u00b6\n\n\nYou may have an environment made up of 6 different server roles.\n\nEach role is scaled up to 10 servers.\n\nYour deployment mechanism may mean that every server gets recycled every few days as features are released.\n\nYou may decide to store two fingerprints for each role in your configuration management system so that 2 / 10 of the nodes rejoin their old metrics.\n\nThe remaining 8 servers per role get a new identify each time.\n\nIn this way you can look back across many releases to determine what effects the changes have had on your service.",
            "title": "About agent fingerprints"
        },
        {
            "location": "/agent/fingerprints/#fingerprints",
            "text": "When an Agent starts for the first time on a new server it creates an agent.finger file (in  /etc/dataloop  on Linux boxes and  C:\\Dataloop  on Windows). This is used to uniquely identify the Agent.  Deleting the  agent.finger  file and restarting the service will cause the Dataloop (Outlyer) Agent to generate a new identity and it will lose its old metrics.  Conversely, starting servers with known fingerprint files allows you to connect new boxes to existing metrics which is extremely useful for dynamically changing environments where configuration management tools are used.",
            "title": "Fingerprints"
        },
        {
            "location": "/agent/fingerprints/#example",
            "text": "You may have an environment made up of 6 different server roles. \nEach role is scaled up to 10 servers. \nYour deployment mechanism may mean that every server gets recycled every few days as features are released. \nYou may decide to store two fingerprints for each role in your configuration management system so that 2 / 10 of the nodes rejoin their old metrics. \nThe remaining 8 servers per role get a new identify each time. \nIn this way you can look back across many releases to determine what effects the changes have had on your service.",
            "title": "Example"
        },
        {
            "location": "/agent/uninstall/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nUninstall\n\u00b6\n\n\nOn Linux you can simply remove the \ndataloop-agent\n package.\n\n\nRedhat based:\n\u00b6\n\n\nyum remove dataloop-agent\n\n\n\n\n\n\n\nDebian based:\n\u00b6\n\n\napt-get remove dataloop-agent\n\n\n\n\n\n\n\nWindows\n\u00b6\n\n\nOn Windows you can remove via \nControl Panel > Add/Remove Programs\n.",
            "title": "Uninstalling"
        },
        {
            "location": "/agent/uninstall/#uninstall",
            "text": "On Linux you can simply remove the  dataloop-agent  package.",
            "title": "Uninstall"
        },
        {
            "location": "/agent/uninstall/#redhat-based",
            "text": "yum remove dataloop-agent",
            "title": "Redhat based:"
        },
        {
            "location": "/agent/uninstall/#debian-based",
            "text": "apt-get remove dataloop-agent",
            "title": "Debian based:"
        },
        {
            "location": "/agent/uninstall/#windows",
            "text": "On Windows you can remove via  Control Panel > Add/Remove Programs .",
            "title": "Windows"
        },
        {
            "location": "/agent/command_line_interface/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nCommand Line Interface\n\u00b6\n\n\nOn Linux you can simply type \ndataloop-agent\n or call directly from \n/usr/bin/dataloop-agent\n\n\nOn Windows you can run:\n\n\nC:\\Dataloop\\embedded\\bin\\python.exe C:\\Dataloop\\agent\\agent.py\n\n\nRun the \ndataloop-agent\n command with the \n-h\n argument to show the full help\n\n\n# dataloop-agent -h\nusage: dataloop-agent [-h] [--config PATH] -a KEY [-t START_TAGS] [-s]\n                      [--name NAME] [-v]\n                      {tags,agent} ...\n\n## DataLoop.io Agent\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config PATH         optional: path to config yaml file\n  -a KEY, --api-key KEY\n                        required: your agent api-key. This can be found in the\n                        app\n  -t START_TAGS, --tags START_TAGS\n                        optional: start agent with tags\n  -s, --solo            optional: run from a local plugin source\n  --name NAME           optional: set the agent name\n  -v, --version         show program's version number and exit\n\ncli:\n  {tags,agent}          CLI commands\n    tags                update agent tags\n    agent               update agent details\n\n\n\n\n\nYou can also pass -h into the options:\n\n\n# dataloop-agent tags -h\nusage: dataloop-agent tags [-h] {list,add,remove,clear} ...\n\noptional arguments:\n  -h, --help            show this help message and exit\n\ntags:\n  {list,add,remove,clear}\n                        tags commands\n    list                list this agents tags\n    add                 add tags to this agent\n    remove              remove tags from this agent\n    clear               clear all tags from this agent\n\n\n\n\n\nThe most common use for the CLI is for tagging either in automated scripts or in config management.\n\n\nAs an example, while the Dataloop (Outlyer) Agent is running, you can list what tags it is a member of and then modify them:\n\n\n#> dataloop-agent tags list\nall\nriak\nprod\ntag1\n\n#> dataloop-agent tags remove tag1\n\n#> dataloop-agent tags list\nall\nriak\nprod\n\n#> dataloop-agent tags add tag2\n\n#> dataloop-agent tags list\nall\nriak\nprod\ntag2",
            "title": "Command-line options"
        },
        {
            "location": "/agent/command_line_interface/#command-line-interface",
            "text": "On Linux you can simply type  dataloop-agent  or call directly from  /usr/bin/dataloop-agent  On Windows you can run:  C:\\Dataloop\\embedded\\bin\\python.exe C:\\Dataloop\\agent\\agent.py  Run the  dataloop-agent  command with the  -h  argument to show the full help  # dataloop-agent -h\nusage: dataloop-agent [-h] [--config PATH] -a KEY [-t START_TAGS] [-s]\n                      [--name NAME] [-v]\n                      {tags,agent} ...\n\n## DataLoop.io Agent\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config PATH         optional: path to config yaml file\n  -a KEY, --api-key KEY\n                        required: your agent api-key. This can be found in the\n                        app\n  -t START_TAGS, --tags START_TAGS\n                        optional: start agent with tags\n  -s, --solo            optional: run from a local plugin source\n  --name NAME           optional: set the agent name\n  -v, --version         show program's version number and exit\n\ncli:\n  {tags,agent}          CLI commands\n    tags                update agent tags\n    agent               update agent details  You can also pass -h into the options:  # dataloop-agent tags -h\nusage: dataloop-agent tags [-h] {list,add,remove,clear} ...\n\noptional arguments:\n  -h, --help            show this help message and exit\n\ntags:\n  {list,add,remove,clear}\n                        tags commands\n    list                list this agents tags\n    add                 add tags to this agent\n    remove              remove tags from this agent\n    clear               clear all tags from this agent  The most common use for the CLI is for tagging either in automated scripts or in config management.  As an example, while the Dataloop (Outlyer) Agent is running, you can list what tags it is a member of and then modify them:  #> dataloop-agent tags list\nall\nriak\nprod\ntag1\n\n#> dataloop-agent tags remove tag1\n\n#> dataloop-agent tags list\nall\nriak\nprod\n\n#> dataloop-agent tags add tag2\n\n#> dataloop-agent tags list\nall\nriak\nprod\ntag2",
            "title": "Command Line Interface"
        },
        {
            "location": "/agent/configuration/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nConfiguration\n\u00b6\n\n\nBy default the dataloop-agent will read \nagent.yaml\n for configuration from \n/etc/dataloop\n on Linux and \nc:\\dataloop\n on Windows.\n\n\nThe only mandatory configuration required in this file is the api-key.\n\n\nChanges will be picked up on dataloop-agent service restart.\n\n\n\n\nExample:\n\n\n\n\n---\n##\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo_mode: no\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n\n##  set tags to a comma separated list of tags applied to this agent\n#tags:\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:\n\n\n\n\n\nOn Linux there are also a couple of tweaks that can be made in \n/etc/default/dataloop-agent\n for Debian based distros and \n/etc/sysconfig/dataloop-agent\n for Redhat.\n\n\n\n\nExample:\n\n\n\n\n# Deregister node with dataloop when the agent stops?\n# Only 'no' will make this stop.\nDEREGISTER_ONSTOP=yes\nexport HTTP_PROXY=http://squid.box\n\n\n\n\n\nBy default the Dataloop Dataloop (Outlyer) Agent will deregister on server and service shutdown. This is extremely useful when you have dynamically scaled environments e.g. Amazon ASG's.\n\n\nIf you have a more static infrastructure or want to perform the deregister at your orchestration later then set to no.\n\n\nYou can also specify \nHTTP_PROXY\n or \nHTTPS_PROXY\n if the Dataloop (Outlyer) Agent needs to proxy out to the internet.\n\n\nOn Windows you can configure these as environment variables and the Dataloop (Outlyer) Agent will connect out via a proxy.",
            "title": "Configuration"
        },
        {
            "location": "/agent/configuration/#configuration",
            "text": "By default the dataloop-agent will read  agent.yaml  for configuration from  /etc/dataloop  on Linux and  c:\\dataloop  on Windows.  The only mandatory configuration required in this file is the api-key.  Changes will be picked up on dataloop-agent service restart.   Example:   ---\n##\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo_mode: no\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n\n##  set tags to a comma separated list of tags applied to this agent\n#tags:\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:  On Linux there are also a couple of tweaks that can be made in  /etc/default/dataloop-agent  for Debian based distros and  /etc/sysconfig/dataloop-agent  for Redhat.   Example:   # Deregister node with dataloop when the agent stops?\n# Only 'no' will make this stop.\nDEREGISTER_ONSTOP=yes\nexport HTTP_PROXY=http://squid.box  By default the Dataloop Dataloop (Outlyer) Agent will deregister on server and service shutdown. This is extremely useful when you have dynamically scaled environments e.g. Amazon ASG's.  If you have a more static infrastructure or want to perform the deregister at your orchestration later then set to no.  You can also specify  HTTP_PROXY  or  HTTPS_PROXY  if the Dataloop (Outlyer) Agent needs to proxy out to the internet.  On Windows you can configure these as environment variables and the Dataloop (Outlyer) Agent will connect out via a proxy.",
            "title": "Configuration"
        },
        {
            "location": "/agent/solo_mode/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nSolo Mode\n\u00b6\n\n\nYou can configure the dataloop Dataloop (Outlyer) Agent to run in solo mode by updating the agent.yaml and restarting the dataloop-agent service.\n\n\nWhen running in solo mode the Dataloop (Outlyer) Agent won't run remote commands from the web interface.\n\n\nIt will also not automatically download plugins.\n\n\nHowever, you should still tag solo mode agents in the usual way so that the metrics coming from these agents appear in dashboards and alerts.\n\n\nSolo mode agents show up in the UI with a little padlock next to their icon. Plugins deployed into the folder will also show up in the agent details page so you can verify they are being run.\n\n\nWhen running agents in solo mode you will also need to deploy a copy of \nbase.py\n locally.\n\n\nAgent Config\n\u00b6\n\n\nTo enable solo mode edit the \nagent.yaml\n file and restart the dataloop-agent service.\n\n\n---\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo:  yes\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxx\n\n### The dataloop server endpoint\nserver: wss://agent.dataloop.io\n\n##  set tags to a comma separated list of tags applied to this agent\ntags: tag1,tag2,tag3\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:\n\n\n\n\n\nLocal Plugin Deployment\n\u00b6\n\n\nYou can deploy plugins to agents running in Solo mode or normal mode. Local plugins take precedence over plugins stored in Dataloop.\n\n\nCopy a plugin into /opt/dataloop/plugins (linux) or c:\\dataloop\\plugins (windows).\nEnsure the file has a valid file extension\nEnsure the file is owned by the dataloop user\nEnsure the file is executable\n\n\n\n\n\nWithin 10 seconds the plugin will be automatically loaded into a running Dataloop agent. The agent.log file will display whether plugins have been successfully loaded or not.\nLocal Plugin Config\n\n\nYou can optionally change plugin settings by creating a file in the plugins directory alongside the plugins.\n\n\nCreate a file called plugin_name.yaml\nThe file must exactly match the name of the plugin except with a yaml extension\nThe file must be owned by the dataloop user\n\n\n\n\n\nAn example yaml file to change the settings for a plugin called node_exporter.sh\n\n\n# cat /opt/dataloop/plugins/node_exporter.yaml\n---\ninterval: 10\nparams: ''\nshell: ['/bin/bash']\ntype: INPROCESS\nformat: PROMETHEUS\n\n\n\n\n\n\n\n\n\ninterval: any number in seconds and defaults to 30 seconds if not supplied\n\n\n\n\n\n\nparams: a string to pass into the plugin as an argument. defaults to '' if not supplied\n\n\n\n\n\n\nshell: a list of commands used to execute the plugin. defaults to [''] if not supplied\n\n\n\n\n\n\ntype: either \nINPROCESS\n or \nSCRIPT\n. defaults to SCRIPT which executes plugins as if they were run on the command line with the shebang. \nINPROCESS\n runs Python 2.7 code in the context of the running Dataloop (Outlyer) Agent code.\n\n\n\n\n\n\nformat: \nNAGIOS\n (default) or \nPROMETHEUS\n depending on the output format of the plugin",
            "title": "Solo mode"
        },
        {
            "location": "/agent/solo_mode/#solo-mode",
            "text": "You can configure the dataloop Dataloop (Outlyer) Agent to run in solo mode by updating the agent.yaml and restarting the dataloop-agent service.  When running in solo mode the Dataloop (Outlyer) Agent won't run remote commands from the web interface.  It will also not automatically download plugins.  However, you should still tag solo mode agents in the usual way so that the metrics coming from these agents appear in dashboards and alerts.  Solo mode agents show up in the UI with a little padlock next to their icon. Plugins deployed into the folder will also show up in the agent details page so you can verify they are being run.  When running agents in solo mode you will also need to deploy a copy of  base.py  locally.",
            "title": "Solo Mode"
        },
        {
            "location": "/agent/solo_mode/#agent-config",
            "text": "To enable solo mode edit the  agent.yaml  file and restart the dataloop-agent service.  ---\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo:  yes\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxx\n\n### The dataloop server endpoint\nserver: wss://agent.dataloop.io\n\n##  set tags to a comma separated list of tags applied to this agent\ntags: tag1,tag2,tag3\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:",
            "title": "Agent Config"
        },
        {
            "location": "/agent/solo_mode/#local-plugin-deployment",
            "text": "You can deploy plugins to agents running in Solo mode or normal mode. Local plugins take precedence over plugins stored in Dataloop.  Copy a plugin into /opt/dataloop/plugins (linux) or c:\\dataloop\\plugins (windows).\nEnsure the file has a valid file extension\nEnsure the file is owned by the dataloop user\nEnsure the file is executable  Within 10 seconds the plugin will be automatically loaded into a running Dataloop agent. The agent.log file will display whether plugins have been successfully loaded or not.\nLocal Plugin Config  You can optionally change plugin settings by creating a file in the plugins directory alongside the plugins.  Create a file called plugin_name.yaml\nThe file must exactly match the name of the plugin except with a yaml extension\nThe file must be owned by the dataloop user  An example yaml file to change the settings for a plugin called node_exporter.sh  # cat /opt/dataloop/plugins/node_exporter.yaml\n---\ninterval: 10\nparams: ''\nshell: ['/bin/bash']\ntype: INPROCESS\nformat: PROMETHEUS    interval: any number in seconds and defaults to 30 seconds if not supplied    params: a string to pass into the plugin as an argument. defaults to '' if not supplied    shell: a list of commands used to execute the plugin. defaults to [''] if not supplied    type: either  INPROCESS  or  SCRIPT . defaults to SCRIPT which executes plugins as if they were run on the command line with the shebang.  INPROCESS  runs Python 2.7 code in the context of the running Dataloop (Outlyer) Agent code.    format:  NAGIOS  (default) or  PROMETHEUS  depending on the output format of the plugin",
            "title": "Local Plugin Deployment"
        },
        {
            "location": "/docker/introduction/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nIntroduction to Using Outlyer with Docker\n\u00b6\n\n\nWhat is Docker?\n\u00b6\n\n\nDocker is software that allows you to run your applications inside\ncontainers on a host. Similar to virtual machines but much more lightweight,\ncontainers package up an application with a complete filesystem.\n\n\nRead more at \nWhat is Docker?\n\n\nMonitoring Docker Environments\n\u00b6\n\n\nDocker brings a new level of complexity to monitoring. Each container consumes\nCPU, memory, network, and other resources, but all containers must share the\nresources available to their host.\n\n\nOutlyer gives you the ability to monitor the resources used by each of the\ncontainers as well as the host.\n\n\nOur packs and plugins work with containers as well. The MySQL plugin, for\nexample, will report the same metrics whether MySQL is running on a traditional\nhost, or inside a container.\n\n\nMonitoring with Outlyer\n\u00b6\n\n\nWe provide a special agent for Docker environments. Naturally, this agent is\nitself packaged as a Docker container. Our container provides the following\nfeatures:\n\n\n\n\n\n\nContainer inventory and autodiscovery.\n The agent reports metrics for\n   every container. When you create new containers, the agent will automatically\n   begin reporting metrics for those new containers without any work from you.\n   When you delete containers, they will be removed from the Outlyer console.\n\n\n\n\n\n\nPer-container and per-host metrics.\n The agent will report base system\n   metrics (CPU, memory, I/O, and network usage) consumed by each container,\n   as well as for the host machine the containers are running on.\n\n\n\n\n\n\nRunning packs and plugins.\n The agent can run our standard packs and\n   monitoring plugins against services running inside containers. There are\n   usually no changes required to the plugin scripts.\n\n\n\n\n\n\n\n\nNote\n\n\nIt is not necessary to install the regular Outlyer (Dataloop) Agent on a host if\nthe Outlyer (Dataloop) Docker Agent is deployed there. The Docker Agent will report\nhost metrics as well as container metrics.\n\n\n\n\nContainer Orchestration\n\u00b6\n\n\nManaging individual Docker hosts directly has given way to \ncontainer\norchestration\n software like Kubernetes, Swarm, and Mesos. These systems\nlet you manage your Docker hosts as a single cluster, also adding a layer\nof additional services like fault-tolerance and DNS.\n\n\nOn our \ninstallation page\n, we show you how to deploy\nOutlyer to all of the hosts in your Kubernetes or Swarm cluster with a\nsingle command. (Support for Mesos is coming soon.)\n\n\nGetting Started\n\u00b6\n\n\n\n\nInstall the agent\n on your Docker hosts.\n\n\nInstall the Docker pack\n in your account.\n\n\nUse \"host view\"\n to visualize your hosts, containers, and the\n    connections between them.",
            "title": "Introduction to Docker monitoring"
        },
        {
            "location": "/docker/introduction/#introduction-to-using-outlyer-with-docker",
            "text": "",
            "title": "Introduction to Using Outlyer with Docker"
        },
        {
            "location": "/docker/introduction/#what-is-docker",
            "text": "Docker is software that allows you to run your applications inside\ncontainers on a host. Similar to virtual machines but much more lightweight,\ncontainers package up an application with a complete filesystem.  Read more at  What is Docker?",
            "title": "What is Docker?"
        },
        {
            "location": "/docker/introduction/#monitoring-docker-environments",
            "text": "Docker brings a new level of complexity to monitoring. Each container consumes\nCPU, memory, network, and other resources, but all containers must share the\nresources available to their host.  Outlyer gives you the ability to monitor the resources used by each of the\ncontainers as well as the host.  Our packs and plugins work with containers as well. The MySQL plugin, for\nexample, will report the same metrics whether MySQL is running on a traditional\nhost, or inside a container.",
            "title": "Monitoring Docker Environments"
        },
        {
            "location": "/docker/introduction/#monitoring-with-outlyer",
            "text": "We provide a special agent for Docker environments. Naturally, this agent is\nitself packaged as a Docker container. Our container provides the following\nfeatures:    Container inventory and autodiscovery.  The agent reports metrics for\n   every container. When you create new containers, the agent will automatically\n   begin reporting metrics for those new containers without any work from you.\n   When you delete containers, they will be removed from the Outlyer console.    Per-container and per-host metrics.  The agent will report base system\n   metrics (CPU, memory, I/O, and network usage) consumed by each container,\n   as well as for the host machine the containers are running on.    Running packs and plugins.  The agent can run our standard packs and\n   monitoring plugins against services running inside containers. There are\n   usually no changes required to the plugin scripts.     Note  It is not necessary to install the regular Outlyer (Dataloop) Agent on a host if\nthe Outlyer (Dataloop) Docker Agent is deployed there. The Docker Agent will report\nhost metrics as well as container metrics.",
            "title": "Monitoring with Outlyer"
        },
        {
            "location": "/docker/introduction/#container-orchestration",
            "text": "Managing individual Docker hosts directly has given way to  container\norchestration  software like Kubernetes, Swarm, and Mesos. These systems\nlet you manage your Docker hosts as a single cluster, also adding a layer\nof additional services like fault-tolerance and DNS.  On our  installation page , we show you how to deploy\nOutlyer to all of the hosts in your Kubernetes or Swarm cluster with a\nsingle command. (Support for Mesos is coming soon.)",
            "title": "Container Orchestration"
        },
        {
            "location": "/docker/introduction/#getting-started",
            "text": "Install the agent  on your Docker hosts.  Install the Docker pack  in your account.  Use \"host view\"  to visualize your hosts, containers, and the\n    connections between them.",
            "title": "Getting Started"
        },
        {
            "location": "/docker/installation/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nDocker Agent and Pack installation\n\u00b6\n\n\nThere are two steps to getting our Docker support up and running:\n\n\n\n\nInstalling the Docker Agent.\n\n\nInstalling the Docker Monitoring Pack in your account.\n\n\n\n\nInstalling the Agent container\n\u00b6\n\n\nThe Docker Agent is itself just a Docker container. It runs alongside the other\ncontainers on your Docker host. How you deploy the container depends on your\nenvironment. Choose the option that matches your needs:\n\n\n\n\nDeploying on a Kubernetes cluster\n\n\nDeploying on a Swarm cluster\n\n\nDeploying manually\n\n\n\n\nDeploying on Kubernetes\n\u00b6\n\n\nDownload our \nsample daemonset file\n and customize as necessary for your\ninstallation. At minimum, you will want to set your DOCKER_AGENT_KEY:\n\n\nenv\n:\n\n  \n-\n \nname\n:\n \nDATALOOP_AGENT_KEY\n\n\n    \nvalue\n:\n \n<your dataloop agent key>\n\n\n\n\n\n\nThen deploy it to Kubernetes with this command:\n\n\nkubectl apply -f kubernetes_daemonset.yaml\n\n\n\n\n\n\n\nTip\n\n\nIf you used kubeadm to setup your cluster, or if you use \nRBAC\n, \nthe default service account will not have some permissions. We use your cluster's configuration to talk to\nthe API, which uses \nservice accounts\n.\nKubeadm sets up \nRBAC\n,\nwhich in turn by default grants \nno permissions\n\noutside of the \"kube-system\" namespace.\n\n\nThis problem will manifest itself as an HTTP 403 error when running plugins.\n\nTo give your default service account the necessary \"view\" permissions, run the following:\n\n\nkubectl create rolebinding default-view --clusterrole=view --serviceaccount=default:default --namespace=default\n\n\n\n\n\n\nDeploying on Swarm\n\u00b6\n\n\nPlease follow the instructions detailed in a standard \nDocker deployment\n,\nmaking sure that you pass \n--net=host\n to the \ndocker run\n command.\n\n\nWhy can't the Agent run as a swarm service?\n\u00b6\n\n\nSwarm does not \nyet\n support running\ncontainers with \n--net=host\n, which is needed for the Agent to communicate\nsimply with all containers on the host.\n\n\n\n\nStarting the container by hand\n\u00b6\n\n\nThe Dataloop container can be manually started with a simple \ndocker run\n\ncommand. Detailed instructions are \nhere\n.\n\n\nConfiguration\n\u00b6\n\n\nNo matter which method you choose, you will need to specify your agent key,\nwhich associates the agent with your account. You can also pass in any of\nthese additional environment variables:\n\n\n\n\nDATALOOP_AGENT_KEY\n\n\nYour agent key.\n\n\nDATALOOP_NAME\n\n\nAssigns a name to the agent. If not provided, the host name will be used.\n\n\nDATALOOP_TAGS\n\n\nComma-separated list of extra tags to apply to the agent.\n\n\nDATALOOP_DEBUG\n\n\nIf set to \ntrue\n, the agent will enable extra verbose logging.\n\n\n\n\nInstalling the Docker monitoring pack\n\u00b6\n\n\nThe Docker pack provides some default dashboards and alerts for Docker\nenvironments. To install it:\n\n\n\n\nGo to \nSetup Monitoring > Pack Library\n.\n\n\nScroll down to the Docker pack.\n\n\nClick the green \"Install\" button on the right side of the screen.\n\n\n\n\n\n\nViewing the Docker dashboard\n\u00b6\n\n\nThe dashboard in the Docker pack provides a snapshot of your environment's\noverall health, including service status, number of containers running, and\ncontainer resource usage. To view it, go to \nDashboards > Docker\n.\n\n\nCustomizing the standard alerts\n\u00b6\n\n\nThe pack provides a simple alert rule that triggers whenever any of the\nmonitored containers goes down. To customize the rule, go to\n\nAlerts > Docker\n.",
            "title": "Installation"
        },
        {
            "location": "/docker/installation/#docker-agent-and-pack-installation",
            "text": "There are two steps to getting our Docker support up and running:   Installing the Docker Agent.  Installing the Docker Monitoring Pack in your account.",
            "title": "Docker Agent and Pack installation"
        },
        {
            "location": "/docker/installation/#installing-the-agent-container",
            "text": "The Docker Agent is itself just a Docker container. It runs alongside the other\ncontainers on your Docker host. How you deploy the container depends on your\nenvironment. Choose the option that matches your needs:   Deploying on a Kubernetes cluster  Deploying on a Swarm cluster  Deploying manually",
            "title": "Installing the Agent container"
        },
        {
            "location": "/docker/installation/#deploying-on-kubernetes",
            "text": "Download our  sample daemonset file  and customize as necessary for your\ninstallation. At minimum, you will want to set your DOCKER_AGENT_KEY:  env : \n   -   name :   DATALOOP_AGENT_KEY       value :   <your dataloop agent key>   Then deploy it to Kubernetes with this command:  kubectl apply -f kubernetes_daemonset.yaml   Tip  If you used kubeadm to setup your cluster, or if you use  RBAC , \nthe default service account will not have some permissions. We use your cluster's configuration to talk to\nthe API, which uses  service accounts .\nKubeadm sets up  RBAC ,\nwhich in turn by default grants  no permissions \noutside of the \"kube-system\" namespace.  This problem will manifest itself as an HTTP 403 error when running plugins. \nTo give your default service account the necessary \"view\" permissions, run the following:  kubectl create rolebinding default-view --clusterrole=view --serviceaccount=default:default --namespace=default",
            "title": "Deploying on Kubernetes"
        },
        {
            "location": "/docker/installation/#deploying-on-swarm",
            "text": "Please follow the instructions detailed in a standard  Docker deployment ,\nmaking sure that you pass  --net=host  to the  docker run  command.",
            "title": "Deploying on Swarm"
        },
        {
            "location": "/docker/installation/#why-cant-the-agent-run-as-a-swarm-service",
            "text": "Swarm does not  yet  support running\ncontainers with  --net=host , which is needed for the Agent to communicate\nsimply with all containers on the host.",
            "title": "Why can't the Agent run as a swarm service?"
        },
        {
            "location": "/docker/installation/#starting-the-container-by-hand",
            "text": "The Dataloop container can be manually started with a simple  docker run \ncommand. Detailed instructions are  here .",
            "title": "Starting the container by hand"
        },
        {
            "location": "/docker/installation/#configuration",
            "text": "No matter which method you choose, you will need to specify your agent key,\nwhich associates the agent with your account. You can also pass in any of\nthese additional environment variables:   DATALOOP_AGENT_KEY  Your agent key.  DATALOOP_NAME  Assigns a name to the agent. If not provided, the host name will be used.  DATALOOP_TAGS  Comma-separated list of extra tags to apply to the agent.  DATALOOP_DEBUG  If set to  true , the agent will enable extra verbose logging.",
            "title": "Configuration"
        },
        {
            "location": "/docker/installation/#installing-the-docker-monitoring-pack",
            "text": "The Docker pack provides some default dashboards and alerts for Docker\nenvironments. To install it:   Go to  Setup Monitoring > Pack Library .  Scroll down to the Docker pack.  Click the green \"Install\" button on the right side of the screen.",
            "title": "Installing the Docker monitoring pack"
        },
        {
            "location": "/docker/installation/#viewing-the-docker-dashboard",
            "text": "The dashboard in the Docker pack provides a snapshot of your environment's\noverall health, including service status, number of containers running, and\ncontainer resource usage. To view it, go to  Dashboards > Docker .",
            "title": "Viewing the Docker dashboard"
        },
        {
            "location": "/docker/installation/#customizing-the-standard-alerts",
            "text": "The pack provides a simple alert rule that triggers whenever any of the\nmonitored containers goes down. To customize the rule, go to Alerts > Docker .",
            "title": "Customizing the standard alerts"
        },
        {
            "location": "/docker/upgrading/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nUpgrading to the latest version\n\u00b6\n\n\nThe latest version of our agent is always available on \nDocker Hub\n with\nthe \nlatest\n tag, so upgrading should be as simple as:\n\n\n    docker pull outlyer/agent:latest\n    docker stop outlyer\n    docker start outlyer\n\n\n\n\n\nIf you are using Docker Compose to run the container, just execute this in the\ndirectory where you placed our \ndocker-compose.yml\n:\n\n\n    docker-compose up\n\n\n\n\n\nFor Kubernetes versions prior to 1.6, you must delete the Outlyer DaemonSet and\nrecreate it:\n\n\n    kubectl delete -f outlyer_daemonset.yaml\n    kubectl apply -f outlyer_daemonset.yaml\n\n\n\n\n\nKubernetes 1.6 and later support rolling updates to DaemonSets. See the\n\nKubernetes docs\n for guidance.\n\n\nFor Swarm, you should update the image on your manager nodes and worker\nnodes as follows:\n\n\n    docker service update --image outlyer/agent:latest agent-worker\n    docker service update --image outlyer/agent:latest agent-manager\n\n\n\n\n\nUpgrading from the previous generation agent\n\u00b6\n\n\nIf you are currently running a version of our Docker Agent prior to\n\n2017.3-1\n, you should remove it before installing the latest agent.\nThis only needs to be done once.\n\n\n\n\nFirst, remove any existing Dataloop Docker containers. Use \ndocker ps\n\n    to find the container:\n\n\n\n\n    docker ps | grep dataloop | cut -f1 -d\" \"\n\n\n\n\n\n\n\nStop and remove the container, using the container ID you obtained\n    in step 1.\n\n\n\n\n    docker rm -f 7a5c126f7297\n\n\n\n\n\n\n\n\n\nIf the Host Agent is installed, you should remove that also. The Host\n    Agent is no longer required on Docker machines. Follow our\n    \nuninstall instructions\n to remove it.\n\n\n\n\n\n\nNow you can \ninstall the latest version\n of the\n    Docker Agent.",
            "title": "Upgrading from an older release"
        },
        {
            "location": "/docker/upgrading/#upgrading-to-the-latest-version",
            "text": "The latest version of our agent is always available on  Docker Hub  with\nthe  latest  tag, so upgrading should be as simple as:      docker pull outlyer/agent:latest\n    docker stop outlyer\n    docker start outlyer  If you are using Docker Compose to run the container, just execute this in the\ndirectory where you placed our  docker-compose.yml :      docker-compose up  For Kubernetes versions prior to 1.6, you must delete the Outlyer DaemonSet and\nrecreate it:      kubectl delete -f outlyer_daemonset.yaml\n    kubectl apply -f outlyer_daemonset.yaml  Kubernetes 1.6 and later support rolling updates to DaemonSets. See the Kubernetes docs  for guidance.  For Swarm, you should update the image on your manager nodes and worker\nnodes as follows:      docker service update --image outlyer/agent:latest agent-worker\n    docker service update --image outlyer/agent:latest agent-manager",
            "title": "Upgrading to the latest version"
        },
        {
            "location": "/docker/upgrading/#upgrading-from-the-previous-generation-agent",
            "text": "If you are currently running a version of our Docker Agent prior to 2017.3-1 , you should remove it before installing the latest agent.\nThis only needs to be done once.   First, remove any existing Dataloop Docker containers. Use  docker ps \n    to find the container:       docker ps | grep dataloop | cut -f1 -d\" \"   Stop and remove the container, using the container ID you obtained\n    in step 1.       docker rm -f 7a5c126f7297    If the Host Agent is installed, you should remove that also. The Host\n    Agent is no longer required on Docker machines. Follow our\n     uninstall instructions  to remove it.    Now you can  install the latest version  of the\n    Docker Agent.",
            "title": "Upgrading from the previous generation agent"
        },
        {
            "location": "/docker/host_view/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nUsing Host View to Visualize Containers and Relationships\n\u00b6\n\n\nHost View displays an overview of all the containers and hosts in your\naccount. It has powerful features that let you filter, group, and\ncolorize objects based on criteria you select. Here are some of the things\nyou can do with Host View:\n\n\n\n\nShow which containers belong to which hosts.\n\n\nVisualize which containers are consuming the most CPU or memory.\n\n\nDisplay quick facts at a glance, including: CPU and memory usage,\n   tags and plugins assigned to an object, and running processes.\n\n\n\n\n\n\nGetting started with Host View\n\u00b6\n\n\nTo display the host view, navigate to \nStatus > Host View\n. The default\nhost view is displayed as a set of colored squares. Each square represents\na container or host.\n\n\nClicking on one of the colored squares will open a side panel with details\nabout the selected object, including:\n\n\n\n\nCurrent status (Connected/disconnected)\n\n\nCPU and memory metrics for the past hour\n\n\nHostname and agent fingerprint (for debugging)\n\n\nParent hostname (if the selected object is a container) or\n   containers running on this host (if the selected object is a host)\n\n\nAlerts currently being triggered\n\n\nTags assigned to this object\n\n\nPlugins configured to collect data on this object\n\n\nProcesses running on this object\n\n\n\n\nDisplaying connections between objects\n\u00b6\n\n\nWhen you select an object, you will see lines connecting this object to its\nparent or children.\n\n\n\n\n\n\nIf you select a container, Host View will draw a line connecting the\n   container to its host.\n\n\n\n\n\n\nIf you select a host, Host View will draw lines connecting the host to\n   each of its containers.\n\n\n\n\n\n\nIf you would prefer not to see the connections, change the view setting in the\nsidebar from \"Show container relationships\" to \"Don't show any connections\".\n\n\nThe \"highlight connected nodes\" checkbox will cause other objects to be\ngrayed out, making it easier to see the connections to the selected object.\n\n\nZooming in for more details\n\u00b6\n\n\nYou can zoom in on sections of the host view to display more details inside\neach object.\n\n\n\n\nTo zoom in on an object:\n\n\n\n\nClick the \n zoom in button in the bottom right corner of\n    the Host View.\n\n\nOr scroll your mouse wheel to zoom in.\n\n\n\n\nFiltering the view\n\u00b6\n\n\nIf you have many objects in the Host View, you might prefer to display only\na subset. If you select a tag in the \"Filter By\" view, you will see only the\nobjects that have that tag. For example:\n\n\n\n\nSelect the \ncontainer:host\n tag to display only hosts, not containers.\n\n\nSelect the \ncontainer:id\n tag to display only containers.\n\n\nSelect the \nec2\n tag to display only AWS EC2 instances.\n\n\nSelect the \nmysql\n tag to display only objects that have been tagged to\n   run the MySQL plugin.\n\n\n\n\nIf you select multiple tags, then only those objects that match ALL of the\nfilters will be displayed.\n\n\nTo remove the filters and display all objects, click the \"x\" next to the\nselected filter.\n\n\nGrouping objects in the view\n\u00b6\n\n\nThis is one of the most powerful features of Host View. You can use your tags\n(and additional metadata supplied by Outlyer) to group similar objects\ntogether.\n\n\nA very useful example would be to group your Docker containers with their\nhost, as in the following screenshot.\n\n\n\n\nTo group containers by host:\n\n\n\n\nIn the upper right corner of Host View, click \n.\n\n\nSelect the \ncontainer:parent_id\n meta-tag.\n\n\n\n\nYou will also see your hosts grouped in the lower-left corner, in a group\ncalled \"no container:parent_id\". Since hosts do not carry the \nparent_id\n tag,\nthis is where they are placed. If you wanted to remove the hosts from view,\nyou could add a filter on \ncontainer:id\n.\n\n\nAnother example, especially useful in clustered environments, would be to\ngroup by \ncontainer:image\n. This will show you how many instances of each\nDocker image are running in your environment.\n\n\nSorting objects in the view\n\u00b6\n\n\nYou can sort the objects in view by their name, CPU usage, or memory usage. To\nsort the view, click the \"Unsorted\" dropdown in the top right corner of the\ndisplay and choose one of the options.\n\n\nObjects will be sorted from left to right, then top to bottom. If you are also\ngrouping objects, then the objects within each group will be sorted.\n\n\nColorizing and resizing objects\n\u00b6\n\n\nAs an alternative to sorting, you can adjust the colors of objects based on\ntheir CPU or memory usage. This turns the display into a \"heat map\" where\nyou can quickly spot problems in your infrastructure.\n\n\n\n\nTo change how objects are colorized:\n\n\n\n\nClick the \"color by presence\" dropdown list in the upper right corner\n\n    of the Host View.\n\n\nSelect either \"color by CPU\" or \"color by memory\" as appropriate.\n\n\nChoose whether you'd prefer a red/amber/green display or a smooth gradient\n    of colors, and adjust the thresholds as needed.",
            "title": "Using Host View to visualize containers and relationships"
        },
        {
            "location": "/docker/host_view/#using-host-view-to-visualize-containers-and-relationships",
            "text": "Host View displays an overview of all the containers and hosts in your\naccount. It has powerful features that let you filter, group, and\ncolorize objects based on criteria you select. Here are some of the things\nyou can do with Host View:   Show which containers belong to which hosts.  Visualize which containers are consuming the most CPU or memory.  Display quick facts at a glance, including: CPU and memory usage,\n   tags and plugins assigned to an object, and running processes.",
            "title": "Using Host View to Visualize Containers and Relationships"
        },
        {
            "location": "/docker/host_view/#getting-started-with-host-view",
            "text": "To display the host view, navigate to  Status > Host View . The default\nhost view is displayed as a set of colored squares. Each square represents\na container or host.  Clicking on one of the colored squares will open a side panel with details\nabout the selected object, including:   Current status (Connected/disconnected)  CPU and memory metrics for the past hour  Hostname and agent fingerprint (for debugging)  Parent hostname (if the selected object is a container) or\n   containers running on this host (if the selected object is a host)  Alerts currently being triggered  Tags assigned to this object  Plugins configured to collect data on this object  Processes running on this object",
            "title": "Getting started with Host View"
        },
        {
            "location": "/docker/host_view/#displaying-connections-between-objects",
            "text": "When you select an object, you will see lines connecting this object to its\nparent or children.    If you select a container, Host View will draw a line connecting the\n   container to its host.    If you select a host, Host View will draw lines connecting the host to\n   each of its containers.    If you would prefer not to see the connections, change the view setting in the\nsidebar from \"Show container relationships\" to \"Don't show any connections\".  The \"highlight connected nodes\" checkbox will cause other objects to be\ngrayed out, making it easier to see the connections to the selected object.",
            "title": "Displaying connections between objects"
        },
        {
            "location": "/docker/host_view/#zooming-in-for-more-details",
            "text": "You can zoom in on sections of the host view to display more details inside\neach object.   To zoom in on an object:   Click the   zoom in button in the bottom right corner of\n    the Host View.  Or scroll your mouse wheel to zoom in.",
            "title": "Zooming in for more details"
        },
        {
            "location": "/docker/host_view/#filtering-the-view",
            "text": "If you have many objects in the Host View, you might prefer to display only\na subset. If you select a tag in the \"Filter By\" view, you will see only the\nobjects that have that tag. For example:   Select the  container:host  tag to display only hosts, not containers.  Select the  container:id  tag to display only containers.  Select the  ec2  tag to display only AWS EC2 instances.  Select the  mysql  tag to display only objects that have been tagged to\n   run the MySQL plugin.   If you select multiple tags, then only those objects that match ALL of the\nfilters will be displayed.  To remove the filters and display all objects, click the \"x\" next to the\nselected filter.",
            "title": "Filtering the view"
        },
        {
            "location": "/docker/host_view/#grouping-objects-in-the-view",
            "text": "This is one of the most powerful features of Host View. You can use your tags\n(and additional metadata supplied by Outlyer) to group similar objects\ntogether.  A very useful example would be to group your Docker containers with their\nhost, as in the following screenshot.   To group containers by host:   In the upper right corner of Host View, click  .  Select the  container:parent_id  meta-tag.   You will also see your hosts grouped in the lower-left corner, in a group\ncalled \"no container:parent_id\". Since hosts do not carry the  parent_id  tag,\nthis is where they are placed. If you wanted to remove the hosts from view,\nyou could add a filter on  container:id .  Another example, especially useful in clustered environments, would be to\ngroup by  container:image . This will show you how many instances of each\nDocker image are running in your environment.",
            "title": "Grouping objects in the view"
        },
        {
            "location": "/docker/host_view/#sorting-objects-in-the-view",
            "text": "You can sort the objects in view by their name, CPU usage, or memory usage. To\nsort the view, click the \"Unsorted\" dropdown in the top right corner of the\ndisplay and choose one of the options.  Objects will be sorted from left to right, then top to bottom. If you are also\ngrouping objects, then the objects within each group will be sorted.",
            "title": "Sorting objects in the view"
        },
        {
            "location": "/docker/host_view/#colorizing-and-resizing-objects",
            "text": "As an alternative to sorting, you can adjust the colors of objects based on\ntheir CPU or memory usage. This turns the display into a \"heat map\" where\nyou can quickly spot problems in your infrastructure.   To change how objects are colorized:   Click the \"color by presence\" dropdown list in the upper right corner \n    of the Host View.  Select either \"color by CPU\" or \"color by memory\" as appropriate.  Choose whether you'd prefer a red/amber/green display or a smooth gradient\n    of colors, and adjust the thresholds as needed.",
            "title": "Colorizing and resizing objects"
        },
        {
            "location": "/dashboards/",
            "text": "Dashboards\n\u00b6\n\n\nPublic Dashboards\n\n\nAnnotations\n\n\nGraph Widgets\n\n\nNumeric Widgets\n\n\nGrafana\n\n\nTroubleshooting",
            "title": "About dashboards"
        },
        {
            "location": "/dashboards/#dashboards",
            "text": "Public Dashboards  Annotations  Graph Widgets  Numeric Widgets  Grafana  Troubleshooting",
            "title": "Dashboards"
        },
        {
            "location": "/dashboards/graph_widgets/",
            "text": "Graph Widgets\n\u00b6\n\n\nChart\n\u00b6\n\n\nA simple widget displaying the area under all metrics.\n\n\nThis is useful for displaying a trend across multiple metric sources. Using the Chart widget will improve dashboard performance if you have hundreds of metric sources. They are also the most readable from a distance if you are creating dashboards for TV screens.\n\n\nDetailed\n\u00b6\n\n\nThis simply draws the data as received. You will get multiple series lines in different shades of the widget colour. There is a legend option available.\n\n\nThese widgets are most useful when troubleshooting problems. Easily spot outliers by clicking the graph widget to get a list of agents and values.\n\n\nStacked\n\u00b6\n\n\nDraws all series lines stacked on top of each other and ordered by highest average first. There is a legend option available.\n\n\nThese widgets are useful for visually seeing where the bottleneck is. Plot multiple metrics on a widget and see if there are any humps caused by something specific.",
            "title": "Graph widgets"
        },
        {
            "location": "/dashboards/graph_widgets/#graph-widgets",
            "text": "",
            "title": "Graph Widgets"
        },
        {
            "location": "/dashboards/graph_widgets/#chart",
            "text": "A simple widget displaying the area under all metrics.  This is useful for displaying a trend across multiple metric sources. Using the Chart widget will improve dashboard performance if you have hundreds of metric sources. They are also the most readable from a distance if you are creating dashboards for TV screens.",
            "title": "Chart"
        },
        {
            "location": "/dashboards/graph_widgets/#detailed",
            "text": "This simply draws the data as received. You will get multiple series lines in different shades of the widget colour. There is a legend option available.  These widgets are most useful when troubleshooting problems. Easily spot outliers by clicking the graph widget to get a list of agents and values.",
            "title": "Detailed"
        },
        {
            "location": "/dashboards/graph_widgets/#stacked",
            "text": "Draws all series lines stacked on top of each other and ordered by highest average first. There is a legend option available.  These widgets are useful for visually seeing where the bottleneck is. Plot multiple metrics on a widget and see if there are any humps caused by something specific.",
            "title": "Stacked"
        },
        {
            "location": "/dashboards/numeric_widgets/",
            "text": "Numeric Widgets\n\u00b6\n\n\nWidgets available when browsing a single agent\n\u00b6\n\n\nLast Value\n\u00b6\n\n\nThe last value received for the given metric. Basically a gauge.\n\n\nTotal Value\n\u00b6\n\n\nThe sum of all values for a given metric over the time period set the dashboard.\n\n\nA good use case for this is adding up counter values from StatsD. We'll add up all of those 10 second flushes and help you display things like 'signups in the last 24 hours'.\n\n\nWidgets available when browsing multiple agents (using tags)\n\u00b6\n\n\nAverage\n\u00b6\n\n\nAn average of the last values received by all agents in the tag(s).\n\n\nLowest\n\u00b6\n\n\nThe lowest of the last values received by all agents in the tag(s)\n\n\nThis widget is useful for displaying metrics like best current response time.\n\n\nHighest\n\u00b6\n\n\nThe highest of the last values received by all agents in the tag(s)\n\n\nThis widget is useful for displaying metrics like worst current response time.\n\n\nCombined\n\u00b6\n\n\nThe summation of the last values received by all agents in the tag(s)\n\n\nThis widget is useful for displaying metrics like current used disk space across a cluster",
            "title": "Numeric widgets"
        },
        {
            "location": "/dashboards/numeric_widgets/#numeric-widgets",
            "text": "",
            "title": "Numeric Widgets"
        },
        {
            "location": "/dashboards/numeric_widgets/#widgets-available-when-browsing-a-single-agent",
            "text": "",
            "title": "Widgets available when browsing a single agent"
        },
        {
            "location": "/dashboards/numeric_widgets/#last-value",
            "text": "The last value received for the given metric. Basically a gauge.",
            "title": "Last Value"
        },
        {
            "location": "/dashboards/numeric_widgets/#total-value",
            "text": "The sum of all values for a given metric over the time period set the dashboard.  A good use case for this is adding up counter values from StatsD. We'll add up all of those 10 second flushes and help you display things like 'signups in the last 24 hours'.",
            "title": "Total Value"
        },
        {
            "location": "/dashboards/numeric_widgets/#widgets-available-when-browsing-multiple-agents-using-tags",
            "text": "",
            "title": "Widgets available when browsing multiple agents (using tags)"
        },
        {
            "location": "/dashboards/numeric_widgets/#average",
            "text": "An average of the last values received by all agents in the tag(s).",
            "title": "Average"
        },
        {
            "location": "/dashboards/numeric_widgets/#lowest",
            "text": "The lowest of the last values received by all agents in the tag(s)  This widget is useful for displaying metrics like best current response time.",
            "title": "Lowest"
        },
        {
            "location": "/dashboards/numeric_widgets/#highest",
            "text": "The highest of the last values received by all agents in the tag(s)  This widget is useful for displaying metrics like worst current response time.",
            "title": "Highest"
        },
        {
            "location": "/dashboards/numeric_widgets/#combined",
            "text": "The summation of the last values received by all agents in the tag(s)  This widget is useful for displaying metrics like current used disk space across a cluster",
            "title": "Combined"
        },
        {
            "location": "/dashboards/annotations/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nAnnotations\n\u00b6\n\n\nYou can post annotations into Outlyer via either the API or command line utility.\n\n\nAPI Docs\n\n\nAnnotations are posted into \nstreams\n. These streams can then be switched on and off within the Outlyer dashboards page via the annotations drop down at the top.\n\n\nCurl\n\u00b6\n\n\ncurl -X POST -H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer {{your_api_key}}\" \\\n-d '{\"name\": \"name\", \"description\": \"description\"}' \\\n\"http://api.dataloop.io/v1/orgs/{{your_org}}/accounts/{{your_account}}/annotations/{{stream_name}}\"\n\n\n\n\n\nCommand Line\n\u00b6\n\n\nSetup the Dataloop command line utility (\ndlcli\n)\n\n\nPost a new annotation for a new deployment.\n\n\ndlcli push annotation deployments --name 'deployed micro-service-1' --description 'version 100'\n\n\n\n\n\nNow in Outlyer, browse to a dashboard and tick the \ndeployments\n stream from the annotations drop down.\n\n\nAnnotations will then be visible on graph widgets and the details can be seen by hovering over the dot on the horizontal axis.\n\n\nAnsible\n\u00b6\n\n\nCreate a handler to annotate Dataloop whenever you deploy some code to production. In our example we are posting the package name and version under the stream deployments which is configured in the url.\n\n\n  handlers:\n  - name: annotate_dataloop\n    uri:\n      url: https://app.dataloop.io/api/v1/orgs/{{ dataloop_org_name }}/accounts/{{ dataloop_account_name }}/annotations/deployments\n      method: POST\n      HEADER_Authorization: \"Bearer {{ dataloop_api_key }}\"\n      body_format: json\n      body: {\"name\": \"{{ package }}\", \"description\": \"{{ version }}\"}\n      run_once: true\n\n\n\n\n\nThen call \nannotate_dataloop\n with a notify on the task that does the package deployment.\nChef\n\n\nChef\n\u00b6\n\n\nWe have a Chef handler for posting annotations into Outlyer on every chef run.\n\n\nhttps://github.com/dataloop/dataloop-chef-handler",
            "title": "Annotations"
        },
        {
            "location": "/dashboards/annotations/#annotations",
            "text": "You can post annotations into Outlyer via either the API or command line utility.  API Docs  Annotations are posted into  streams . These streams can then be switched on and off within the Outlyer dashboards page via the annotations drop down at the top.",
            "title": "Annotations"
        },
        {
            "location": "/dashboards/annotations/#curl",
            "text": "curl -X POST -H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer {{your_api_key}}\" \\\n-d '{\"name\": \"name\", \"description\": \"description\"}' \\\n\"http://api.dataloop.io/v1/orgs/{{your_org}}/accounts/{{your_account}}/annotations/{{stream_name}}\"",
            "title": "Curl"
        },
        {
            "location": "/dashboards/annotations/#command-line",
            "text": "Setup the Dataloop command line utility ( dlcli )  Post a new annotation for a new deployment.  dlcli push annotation deployments --name 'deployed micro-service-1' --description 'version 100'  Now in Outlyer, browse to a dashboard and tick the  deployments  stream from the annotations drop down.  Annotations will then be visible on graph widgets and the details can be seen by hovering over the dot on the horizontal axis.",
            "title": "Command Line"
        },
        {
            "location": "/dashboards/annotations/#ansible",
            "text": "Create a handler to annotate Dataloop whenever you deploy some code to production. In our example we are posting the package name and version under the stream deployments which is configured in the url.    handlers:\n  - name: annotate_dataloop\n    uri:\n      url: https://app.dataloop.io/api/v1/orgs/{{ dataloop_org_name }}/accounts/{{ dataloop_account_name }}/annotations/deployments\n      method: POST\n      HEADER_Authorization: \"Bearer {{ dataloop_api_key }}\"\n      body_format: json\n      body: {\"name\": \"{{ package }}\", \"description\": \"{{ version }}\"}\n      run_once: true  Then call  annotate_dataloop  with a notify on the task that does the package deployment.\nChef",
            "title": "Ansible"
        },
        {
            "location": "/dashboards/annotations/#chef",
            "text": "We have a Chef handler for posting annotations into Outlyer on every chef run.  https://github.com/dataloop/dataloop-chef-handler",
            "title": "Chef"
        },
        {
            "location": "/dashboards/public_dashboards/",
            "text": "Public Dashboards\n\u00b6\n\n\nYou can generate a public dashboard URL by clicking the share icon at the top of the dashboards page.\n\n\nPublic dashboards provide a read only unauthenticated view that never times out. These are great for quickly sharing a dashboard with a colleague or for use on TV screens.\n\n\nUpon request we can set a global password on public dashboards so that you can restrict access to your company.\n\n\nWhen tiling Outlyer public dashboards with other dashboards on the same screen you may want to turn off the chrome around the edge of the widgets to save space. To do this simply append \n&chrome=false\n to the end of the public dashboard url.",
            "title": "Sharing your dashboards"
        },
        {
            "location": "/dashboards/public_dashboards/#public-dashboards",
            "text": "You can generate a public dashboard URL by clicking the share icon at the top of the dashboards page.  Public dashboards provide a read only unauthenticated view that never times out. These are great for quickly sharing a dashboard with a colleague or for use on TV screens.  Upon request we can set a global password on public dashboards so that you can restrict access to your company.  When tiling Outlyer public dashboards with other dashboards on the same screen you may want to turn off the chrome around the edge of the widgets to save space. To do this simply append  &chrome=false  to the end of the public dashboard url.",
            "title": "Public Dashboards"
        },
        {
            "location": "/dashboards/troubleshooting/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \u201cdataloop agent\u201d, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything._\n\n\n\n\nTroubleshooting\n\u00b6\n\n\nHovering over a widget will display some options in the top right corner.\n\n\nThe best way to troubleshoot a widget is to click the (i) button and confirm the metric, scope, sources and last update time.\n\n\nIf troubleshooting a plugin then try to run it on a Dataloop (Outlyer) Agent in the scope specified on the widget to check that the correct data is being returned.\n\n\nThe Dataloop Agent sends plugin data back using the wall clock time of the computer it is running on. Ensure you are running NTP or similar otherwise widgets can display odd looking data.",
            "title": "Troubleshooting"
        },
        {
            "location": "/dashboards/troubleshooting/#troubleshooting",
            "text": "Hovering over a widget will display some options in the top right corner.  The best way to troubleshoot a widget is to click the (i) button and confirm the metric, scope, sources and last update time.  If troubleshooting a plugin then try to run it on a Dataloop (Outlyer) Agent in the scope specified on the widget to check that the correct data is being returned.  The Dataloop Agent sends plugin data back using the wall clock time of the computer it is running on. Ensure you are running NTP or similar otherwise widgets can display odd looking data.",
            "title": "Troubleshooting"
        },
        {
            "location": "/alerting/rules/",
            "text": "Alert Rules\n\u00b6\n\n\nAlert rules are comprised of \ncriteria\n and \nactions\n. It is recommended that all rules be created based on tags and that criteria for a given service are grouped together based on severity.\n\n\n\n\nExample: The rule 'ElasticSearch Warning' may contain criteria that need attention but wouldn't necessarily wake anyone up. The actions for this rule would email a group and send a webhook to Slack. Whereas the 'ElasticSearch Critical' rule would contain a few critical checks, such as service down and would send a webhook to PagerDuty.\n\n\n\n\nCriteria are made up of a scope, the metric to alert on and options like comparator, duration and threshold.\n\n\nSupported actions include sending an email or a webhook. When a webhook is configured for a supported integration (listed under the Integrations section of the support docs) we detect the url and send additional fields.\n\n\nIf multiple criteria are created within a rule then \nANY\n need to be met before the rule is triggered and \nALL\n actions are run.\n\n\nOutlyer rules are very flexible, to the point where you can shoot yourself in the foot if you choose to do so.\n\n\n\n\nExample: You can create a plugin to monitor a service and apply it to one tag of agents. Then configure a criteria to alert on that same plugin using a different tag that only contains a subset of agents that the plugin is deployed to. For this reason we have defined a simple traffic light system to help uncover mistakes in setup.\n\n\n\n\nCriteria and Rule status\n\u00b6\n\n\nYou can view the overall health of rules in the alerts page in Outlyer. Each rule is colour coded as per the table below. To be confident that you have effective alerting setup the aim should be to keep all rules in a green state.\n\n\n\n\n\n\n\n\nColour\n\n\nState\n\n\n\n\n\n\n\n\n\n\nGreen\n\n\nClear\n\n\n\n\n\n\nOrange\n\n\nPending\n\n\n\n\n\n\nGrey\n\n\nUnknown\n\n\n\n\n\n\nRed\n\n\nTriggered\n\n\n\n\n\n\n\n\nWhen a criteria is defined in Outlyer, a query is added to a background set of workers that poll every 10 seconds and compare desired state to current state.\n\n\nIf a complete set of data is returned for \nall\n criteria in a rule and all checks pass then a criteria is considered clear. If all criteria within a rule are clear then the rule is clear.\n\n\nPending is the transitional state between Clear and Triggered. A criteria will stay pending for the duration specified.\n\n\n\n\nExample: A criteria duration is set to 5 minutes and a problem occurs the criteria will switch from green to orange within 10 seconds and then stay pending for 5 minutes before turning red. The same is true when a problem is fixed. The criteria will change from red to orange and stay pending for 5 minutes before turning green.\n\n\n\n\nA rule will appear red if \nany\n criteria within it are red.\n\n\nA rule will appear grey if \nany\n of the criteria are not receiving data, of if the rule contains no criteria.\n\n\nWhen evaluating the order of state both within criteria and for criteria within rules we use the following matrix.\n\n\n\n\n\n\n\n\n\n\nunknown\n\n\nclear\n\n\ntriggered\n\n\n\n\n\n\n\n\n\n\nunknown\n\n\nunknown\n\n\nunknown\n\n\ntriggered\n\n\n\n\n\n\nclear\n\n\nunknown\n\n\nclear\n\n\ntriggered\n\n\n\n\n\n\ntriggered\n\n\ntriggered\n\n\ntriggered\n\n\ntriggered\n\n\n\n\n\n\n\n\nThis means that a triggered criteria will always turn a rule red. Also, that unknown criteria will always turn a rule grey (in the absence of any triggered criteria).\n\n\nAlert Lifecycle\n\u00b6\n\n\nFor a rule that is currently \ngreen\n the only state change possible is to change to \ngrey\n if any of the criteria stop returning data for any defined metric path. Or, alternatively, it may turn \nred\n if a Dataloop (Outlyer) Agent or plugin changes status or a threshold is met.\n\n\nActions are run when state changes to \nred\n with a message beginning \nALERT:\n which signifies the start of an incident. Integration fields are configured in such a way that this will create a unique new incident.\n\n\nWhen a rule is red, but criteria within it change state, the actions are triggered again with a message beginning \nUPDATE:\n which signifies that something has changed (either got better or worse). These are correlated with supported integrations so that updates append additional information to the open incident, and don't create new incidents.\n\n\nWhen a rule is either grey or red it may change state to green. On state change to green the actions are triggered with a message beginning \nRESOLVED:\n. This signifies the end of the incident and supported integrations are configured to automatically close the incident.\n\n\nIt is worth noting that a rule that has criteria not receiving data will never send a resolved email or webhook. However, changes to criteria within a grey rule will still send update notifications. It is recommended that an effort is made to keep all rules in a green state.",
            "title": "Introduction"
        },
        {
            "location": "/alerting/rules/#alert-rules",
            "text": "Alert rules are comprised of  criteria  and  actions . It is recommended that all rules be created based on tags and that criteria for a given service are grouped together based on severity.   Example: The rule 'ElasticSearch Warning' may contain criteria that need attention but wouldn't necessarily wake anyone up. The actions for this rule would email a group and send a webhook to Slack. Whereas the 'ElasticSearch Critical' rule would contain a few critical checks, such as service down and would send a webhook to PagerDuty.   Criteria are made up of a scope, the metric to alert on and options like comparator, duration and threshold.  Supported actions include sending an email or a webhook. When a webhook is configured for a supported integration (listed under the Integrations section of the support docs) we detect the url and send additional fields.  If multiple criteria are created within a rule then  ANY  need to be met before the rule is triggered and  ALL  actions are run.  Outlyer rules are very flexible, to the point where you can shoot yourself in the foot if you choose to do so.   Example: You can create a plugin to monitor a service and apply it to one tag of agents. Then configure a criteria to alert on that same plugin using a different tag that only contains a subset of agents that the plugin is deployed to. For this reason we have defined a simple traffic light system to help uncover mistakes in setup.",
            "title": "Alert Rules"
        },
        {
            "location": "/alerting/rules/#criteria-and-rule-status",
            "text": "You can view the overall health of rules in the alerts page in Outlyer. Each rule is colour coded as per the table below. To be confident that you have effective alerting setup the aim should be to keep all rules in a green state.     Colour  State      Green  Clear    Orange  Pending    Grey  Unknown    Red  Triggered     When a criteria is defined in Outlyer, a query is added to a background set of workers that poll every 10 seconds and compare desired state to current state.  If a complete set of data is returned for  all  criteria in a rule and all checks pass then a criteria is considered clear. If all criteria within a rule are clear then the rule is clear.  Pending is the transitional state between Clear and Triggered. A criteria will stay pending for the duration specified.   Example: A criteria duration is set to 5 minutes and a problem occurs the criteria will switch from green to orange within 10 seconds and then stay pending for 5 minutes before turning red. The same is true when a problem is fixed. The criteria will change from red to orange and stay pending for 5 minutes before turning green.   A rule will appear red if  any  criteria within it are red.  A rule will appear grey if  any  of the criteria are not receiving data, of if the rule contains no criteria.  When evaluating the order of state both within criteria and for criteria within rules we use the following matrix.      unknown  clear  triggered      unknown  unknown  unknown  triggered    clear  unknown  clear  triggered    triggered  triggered  triggered  triggered     This means that a triggered criteria will always turn a rule red. Also, that unknown criteria will always turn a rule grey (in the absence of any triggered criteria).",
            "title": "Criteria and Rule status"
        },
        {
            "location": "/alerting/rules/#alert-lifecycle",
            "text": "For a rule that is currently  green  the only state change possible is to change to  grey  if any of the criteria stop returning data for any defined metric path. Or, alternatively, it may turn  red  if a Dataloop (Outlyer) Agent or plugin changes status or a threshold is met.  Actions are run when state changes to  red  with a message beginning  ALERT:  which signifies the start of an incident. Integration fields are configured in such a way that this will create a unique new incident.  When a rule is red, but criteria within it change state, the actions are triggered again with a message beginning  UPDATE:  which signifies that something has changed (either got better or worse). These are correlated with supported integrations so that updates append additional information to the open incident, and don't create new incidents.  When a rule is either grey or red it may change state to green. On state change to green the actions are triggered with a message beginning  RESOLVED: . This signifies the end of the incident and supported integrations are configured to automatically close the incident.  It is worth noting that a rule that has criteria not receiving data will never send a resolved email or webhook. However, changes to criteria within a grey rule will still send update notifications. It is recommended that an effort is made to keep all rules in a green state.",
            "title": "Alert Lifecycle"
        },
        {
            "location": "/alerting/metric_alerts/",
            "text": "Metric Alerts\n\u00b6\n\n\nOutlyer supports Nagios script performance metrics and Graphite metrics. We convert both of these metric sources to our own internal format so that you can graph and alert on both.\n\n\nMetric alerts can be configured via rules. Simply create a new rule, add a criteria and then select either an agent, or a tag of agents as the scope. The \nAlert On\n drop down will then show all of the available metrics that can be alerted on.\n\n\nCurrently you can set a threshold and a duration. We will be adding more advanced features soon.\n\n\nThe rules engine is real-time. You can stream 1 second granularity metrics into Outlyer and we'll alert you instantly when a rule is triggered. We can do this across multiple sources and with multiple criteria. Most importantly, we do this without any single points of failure so you can trust us to monitor production.",
            "title": "Metric alerts"
        },
        {
            "location": "/alerting/metric_alerts/#metric-alerts",
            "text": "Outlyer supports Nagios script performance metrics and Graphite metrics. We convert both of these metric sources to our own internal format so that you can graph and alert on both.  Metric alerts can be configured via rules. Simply create a new rule, add a criteria and then select either an agent, or a tag of agents as the scope. The  Alert On  drop down will then show all of the available metrics that can be alerted on.  Currently you can set a threshold and a duration. We will be adding more advanced features soon.  The rules engine is real-time. You can stream 1 second granularity metrics into Outlyer and we'll alert you instantly when a rule is triggered. We can do this across multiple sources and with multiple criteria. Most importantly, we do this without any single points of failure so you can trust us to monitor production.",
            "title": "Metric Alerts"
        },
        {
            "location": "/alerting/plugin_alerts/",
            "text": "Plugin Alerts\n\u00b6\n\n\nOutlyer uses Nagios format plugins which means that we adhere to these exit codes:\n\n\n0 OK\n1 WARNING\n2 CRITICAL\n3 UNKNOWN\n\n\n\n\n\nWe treat these exit codes like a metric stream in Outlyer. For every Nagios script that returns data you'll see \n<script name>.status\n in the \nAlert On\n drop down.\n\n\nIf you select this you'll be prompted with a slightly different set of options from metrics. Essentially you just need to set how long the script needs to have failed for before the criteria is triggered.\n\n\nWe actually treat exit code \n2\n and \n3\n as down. So if your script returns exit code \n2\n or \n3\n it will trigger a \nscript.status\n rule. This means we ignore OK and Warning for the purposes of alerting.",
            "title": "Plugin alerts"
        },
        {
            "location": "/alerting/plugin_alerts/#plugin-alerts",
            "text": "Outlyer uses Nagios format plugins which means that we adhere to these exit codes:  0 OK\n1 WARNING\n2 CRITICAL\n3 UNKNOWN  We treat these exit codes like a metric stream in Outlyer. For every Nagios script that returns data you'll see  <script name>.status  in the  Alert On  drop down.  If you select this you'll be prompted with a slightly different set of options from metrics. Essentially you just need to set how long the script needs to have failed for before the criteria is triggered.  We actually treat exit code  2  and  3  as down. So if your script returns exit code  2  or  3  it will trigger a  script.status  rule. This means we ignore OK and Warning for the purposes of alerting.",
            "title": "Plugin Alerts"
        },
        {
            "location": "/alerting/agent_alerts/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nAgent Alerts\n\u00b6\n\n\nAgents create a persistent websocket connection to \nhttps://agent.dataloop.io\n when they start.\n\n\nIf this connection drops you will see Dataloop (Outlyer) Agent names will change to either orange or red.\n\n\nThe Agent will do everything within its power to try to reconnect. So you may see some drops and reconnects depending on network issues.\n\n\nA large portion of the Outlyer web application depends on real time connectivity (remote command execution and live streaming graphs for instance) so we therefore make it obvious when a connection has dropped to prevent frustration when buttons are clicked but nothing happens.\n\n\nTo alert on Dataloop (Outlyer) Agent failure create a new rule and select your scope, be that either an individual Dataloop (Outlyer) Agent or a group of agents in a tag. In the \nAlert On\n drop down you can now select the \nagent.status\n metric. You'll then get the option to specify how long the agent(s) need to have been down for in order to trigger an alert.",
            "title": "Agent alerts"
        },
        {
            "location": "/alerting/agent_alerts/#agent-alerts",
            "text": "Agents create a persistent websocket connection to  https://agent.dataloop.io  when they start.  If this connection drops you will see Dataloop (Outlyer) Agent names will change to either orange or red.  The Agent will do everything within its power to try to reconnect. So you may see some drops and reconnects depending on network issues.  A large portion of the Outlyer web application depends on real time connectivity (remote command execution and live streaming graphs for instance) so we therefore make it obvious when a connection has dropped to prevent frustration when buttons are clicked but nothing happens.  To alert on Dataloop (Outlyer) Agent failure create a new rule and select your scope, be that either an individual Dataloop (Outlyer) Agent or a group of agents in a tag. In the  Alert On  drop down you can now select the  agent.status  metric. You'll then get the option to specify how long the agent(s) need to have been down for in order to trigger an alert.",
            "title": "Agent Alerts"
        },
        {
            "location": "/alerting/webhook/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nWebhook\n\u00b6\n\n\nWhen adding an action to a rule you can select the webhook option. This is a great way to connect your alerts into other systems like Slack or iFTTT (if this then that).\n\n\nTo configure a webhook simply change the URL field and hit test. By default we send the payload as JSON but you can change the drop down to send encoded in the URL if the service you are sending to prefers that.\n\n\nYou can add additional fields to the payload in the 'Extra Payload' box. This can also be used to override any of the current fields that we send.\n\n\nFinally, click the test button to verify your webhook is configured correctly.\n\n\nExample delivery\n\u00b6\n\n\nUser-Agent: Dataloop\nContent-Type: application/json\nX-Dataloop-Event: alert-webhook\n\n{\n  \"event\": \"alert\",\n  \"rule\": \"All Sytems\",\n  \"account\": \"noreply@outlyer.com\",\n  \"triggers\": [\n    {\n      \"criteria\": \"hosts are reporting down\",\n      \"sources\": [\n        {\n          \"name\": \"dataloop\",\n          \"tags\": [ \"all\" ],\n          \"timestamp\": \"Wed Dec 09 2015 15:21:20 GMT+0000 (GMT)\"\n        }\n    }\n  ],\n  \"text\": \"a formatted summary of the payload\"\n}\n\n\n\n\n\nDetails\n\u00b6\n\n\n\n\n\n\nevent: [\"alert\" | \"recovered\"]\n\n\n\n\n\n\ncriteria: [\"hosts are reporting down\" | \"check \n is failing\" | \"\n above threshold of \n\"]\n\n\n\n\n\n\ntext: will be picked up automatically by slack\n\n\n\n\n\n\nno triggers details when event is 'recovered'",
            "title": "About webhooks"
        },
        {
            "location": "/alerting/webhook/#webhook",
            "text": "When adding an action to a rule you can select the webhook option. This is a great way to connect your alerts into other systems like Slack or iFTTT (if this then that).  To configure a webhook simply change the URL field and hit test. By default we send the payload as JSON but you can change the drop down to send encoded in the URL if the service you are sending to prefers that.  You can add additional fields to the payload in the 'Extra Payload' box. This can also be used to override any of the current fields that we send.  Finally, click the test button to verify your webhook is configured correctly.",
            "title": "Webhook"
        },
        {
            "location": "/alerting/webhook/#example-delivery",
            "text": "User-Agent: Dataloop\nContent-Type: application/json\nX-Dataloop-Event: alert-webhook\n\n{\n  \"event\": \"alert\",\n  \"rule\": \"All Sytems\",\n  \"account\": \"noreply@outlyer.com\",\n  \"triggers\": [\n    {\n      \"criteria\": \"hosts are reporting down\",\n      \"sources\": [\n        {\n          \"name\": \"dataloop\",\n          \"tags\": [ \"all\" ],\n          \"timestamp\": \"Wed Dec 09 2015 15:21:20 GMT+0000 (GMT)\"\n        }\n    }\n  ],\n  \"text\": \"a formatted summary of the payload\"\n}",
            "title": "Example delivery"
        },
        {
            "location": "/alerting/webhook/#details",
            "text": "event: [\"alert\" | \"recovered\"]    criteria: [\"hosts are reporting down\" | \"check   is failing\" | \"  above threshold of  \"]    text: will be picked up automatically by slack    no triggers details when event is 'recovered'",
            "title": "Details"
        },
        {
            "location": "/integrations/webhook/slack/",
            "text": "Slack\n\u00b6\n\n\n\n\nSetting up Slack\n\u00b6\n\n\nWe integrate to allow the sending of alert triggers, updates and resolutions to a Slack channel.\n\n\nCreate a new incoming webhook in Slack. \nhttps://my.slack.com/services/new/incoming-webhook/\n. This allows you to set the Slack channel for Outlyer notifications.\n\n\nThis will look in the form: \nhttps://hooks.slack.com/services/T02<RANDOM>/<UUID>\n\n\nFor more details see their documentation: \nhttps://api.slack.com/incoming-webhooks\n\n\nFrom an alert, choose to add a new webhook action\n\n\n\n\nPaste the URL into an alert action in Outlyer.\n\n\n\n\nClick the test button and you should receive a message in your slack channel.\n\n\nYou can add further text to the payload if you wish, such as links to your runbooks for alert resolution.",
            "title": "Slack"
        },
        {
            "location": "/integrations/webhook/slack/#slack",
            "text": "",
            "title": "Slack"
        },
        {
            "location": "/integrations/webhook/slack/#setting-up-slack",
            "text": "We integrate to allow the sending of alert triggers, updates and resolutions to a Slack channel.  Create a new incoming webhook in Slack.  https://my.slack.com/services/new/incoming-webhook/ . This allows you to set the Slack channel for Outlyer notifications.  This will look in the form:  https://hooks.slack.com/services/T02<RANDOM>/<UUID>  For more details see their documentation:  https://api.slack.com/incoming-webhooks  From an alert, choose to add a new webhook action   Paste the URL into an alert action in Outlyer.   Click the test button and you should receive a message in your slack channel.  You can add further text to the payload if you wish, such as links to your runbooks for alert resolution.",
            "title": "Setting up Slack"
        },
        {
            "location": "/integrations/webhook/hipchat/",
            "text": "Hipchat\n\u00b6\n\n\nSetting up HipChat\n\u00b6\n\n\nSet the webhook URL in Outlyer to:\n\n\nhttps://api.hipchat.com/v2/room/{room_id_or_name}/notification?auth_token=<YOUR_TOKEN>\n\n\nThen in the Extra Payload box add the following:\n\n\n{\n  \"from\": \"Outlyer\",\n  \"message_format\": \"text\"\n}\n\n\n\n\n\nHit the test button and you should get a message in your HipChat room.\n\n\nOptional Settings\n\u00b6\n\n\nYou can pass in some additional details to HipChat via the optional fields as specified in the Developer Docs\n\n\nhttps://www.hipchat.com/docs/apiv2/method/send_room_notification",
            "title": "HipChat"
        },
        {
            "location": "/integrations/webhook/hipchat/#hipchat",
            "text": "",
            "title": "Hipchat"
        },
        {
            "location": "/integrations/webhook/hipchat/#setting-up-hipchat",
            "text": "Set the webhook URL in Outlyer to:  https://api.hipchat.com/v2/room/{room_id_or_name}/notification?auth_token=<YOUR_TOKEN>  Then in the Extra Payload box add the following:  {\n  \"from\": \"Outlyer\",\n  \"message_format\": \"text\"\n}  Hit the test button and you should get a message in your HipChat room.",
            "title": "Setting up HipChat"
        },
        {
            "location": "/integrations/webhook/hipchat/#optional-settings",
            "text": "You can pass in some additional details to HipChat via the optional fields as specified in the Developer Docs  https://www.hipchat.com/docs/apiv2/method/send_room_notification",
            "title": "Optional Settings"
        },
        {
            "location": "/integrations/webhook/pagerduty/",
            "text": "PagerDuty\n\u00b6\n\n\nSetting up PagerDuty\n\u00b6\n\n\nSetup a new Generic API Service in PagerDuty and take a copy of the Integration Key.\n\n\nSet the webhook URL in Outlyer to:\n\n\nhttps://events.pagerduty.com/generic/2010-04-15/create_event.json\n\n\nThen in the Extra Payload box add the following:\n\n\n{\n  \"service_key\": \"your_pagerduty_integration_key\"\n}\n\n\n\n\n\nHit the test button and you will get an incident triggered in PagerDuty.\n\n\nWhen the rule is resolved in Outlyer the webhook will fire again to resolve the incident in PagerDuty automatically.\n\n\nOptional Settings\n\u00b6\n\n\nYou can pass in some additional details to PagerDuty via the optional fields as specified in the Developer Docs\n\n\nhttps://developer.pagerduty.com/documentation/integration/events/trigger",
            "title": "PagerDuty"
        },
        {
            "location": "/integrations/webhook/pagerduty/#pagerduty",
            "text": "",
            "title": "PagerDuty"
        },
        {
            "location": "/integrations/webhook/pagerduty/#setting-up-pagerduty",
            "text": "Setup a new Generic API Service in PagerDuty and take a copy of the Integration Key.  Set the webhook URL in Outlyer to:  https://events.pagerduty.com/generic/2010-04-15/create_event.json  Then in the Extra Payload box add the following:  {\n  \"service_key\": \"your_pagerduty_integration_key\"\n}  Hit the test button and you will get an incident triggered in PagerDuty.  When the rule is resolved in Outlyer the webhook will fire again to resolve the incident in PagerDuty automatically.",
            "title": "Setting up PagerDuty"
        },
        {
            "location": "/integrations/webhook/pagerduty/#optional-settings",
            "text": "You can pass in some additional details to PagerDuty via the optional fields as specified in the Developer Docs  https://developer.pagerduty.com/documentation/integration/events/trigger",
            "title": "Optional Settings"
        },
        {
            "location": "/integrations/webhook/opsgenie/",
            "text": "OpsGenie\n\u00b6\n\n\nSetting up OpsGenie\n\u00b6\n\n\nWe have an official integration with OpsGenie. Instructions for how to set this up can be found here:\n\n\nhttps://www.opsgenie.com/docs/integrations/dataloop-io-integration\n\n\nOptional Settings\n\u00b6\n\n\nYou can pass in some additional details to OpsGenie via the optional fields as specified in the Developer Docs\n\n\nhttps://www.opsgenie.com/docs/web-api/alert-api\n\n\nOne of the more common options may be to specify which teams to alert.\n\n\n{\n  \"apiKey: \"your_opsgenie_api_key\",\n  \"teams\" : [\"operations\", \"developers\"]\n}",
            "title": "OpsGenie"
        },
        {
            "location": "/integrations/webhook/opsgenie/#opsgenie",
            "text": "",
            "title": "OpsGenie"
        },
        {
            "location": "/integrations/webhook/opsgenie/#setting-up-opsgenie",
            "text": "We have an official integration with OpsGenie. Instructions for how to set this up can be found here:  https://www.opsgenie.com/docs/integrations/dataloop-io-integration",
            "title": "Setting up OpsGenie"
        },
        {
            "location": "/integrations/webhook/opsgenie/#optional-settings",
            "text": "You can pass in some additional details to OpsGenie via the optional fields as specified in the Developer Docs  https://www.opsgenie.com/docs/web-api/alert-api  One of the more common options may be to specify which teams to alert.  {\n  \"apiKey: \"your_opsgenie_api_key\",\n  \"teams\" : [\"operations\", \"developers\"]\n}",
            "title": "Optional Settings"
        },
        {
            "location": "/integrations/webhook/bigpanda/",
            "text": "BigPanda\n\u00b6\n\n\nSetting up BigPanda\n\u00b6\n\n\nSet the webhook URL in Outlyer to:\n\n\nhttps://api.bigpanda.io/data/v2/alerts?access_token=<YOUR TOKEN>\n\n\nThen in the Extra Payload box add the following:\n\n\n{\n  \"app_key\": \"your_bigpanda_app_key\"\n}\n\n\n\n\n\nHit the test button and you should get an incident triggered in BigPanda.\n\n\nOptional Settings\n\u00b6\n\n\nYou can pass in some additional details to BigPanda via the optional fields as specified in the developer docs:\n\n\nhttps://www.bigpanda.io/docs/display/BD/POST+alerts",
            "title": "BigPanda"
        },
        {
            "location": "/integrations/webhook/bigpanda/#bigpanda",
            "text": "",
            "title": "BigPanda"
        },
        {
            "location": "/integrations/webhook/bigpanda/#setting-up-bigpanda",
            "text": "Set the webhook URL in Outlyer to:  https://api.bigpanda.io/data/v2/alerts?access_token=<YOUR TOKEN>  Then in the Extra Payload box add the following:  {\n  \"app_key\": \"your_bigpanda_app_key\"\n}  Hit the test button and you should get an incident triggered in BigPanda.",
            "title": "Setting up BigPanda"
        },
        {
            "location": "/integrations/webhook/bigpanda/#optional-settings",
            "text": "You can pass in some additional details to BigPanda via the optional fields as specified in the developer docs:  https://www.bigpanda.io/docs/display/BD/POST+alerts",
            "title": "Optional Settings"
        },
        {
            "location": "/integrations/webhook/victorops/",
            "text": "VictorOps\n\u00b6\n\n\nSetting up VictorOps\n\u00b6\n\n\nSet the webhook URL in Outlyer to:\n\n\nhttps://alert.victorops.com/integrations/generic/20131114/alert/<YOUR_API_KEY_HERE>/<ROUTING_KEY_HERE>\n\n\nHit the test button and you should get an incident triggered in VictorOps.\n\n\nWhen the rule is resolved in Outlyer the webhook will fire again to resolve the incident in VictorOps automatically.\n\n\nOptional Settings\n\u00b6\n\n\nYou can pass in some additional details to VictorOps via the optional fields as specified in the Developer Docs\n\n\nhttp://victorops.force.com/knowledgebase/articles/Integration/Alert-Ingestion-API-Documentation",
            "title": "VictorOps"
        },
        {
            "location": "/integrations/webhook/victorops/#victorops",
            "text": "",
            "title": "VictorOps"
        },
        {
            "location": "/integrations/webhook/victorops/#setting-up-victorops",
            "text": "Set the webhook URL in Outlyer to:  https://alert.victorops.com/integrations/generic/20131114/alert/<YOUR_API_KEY_HERE>/<ROUTING_KEY_HERE>  Hit the test button and you should get an incident triggered in VictorOps.  When the rule is resolved in Outlyer the webhook will fire again to resolve the incident in VictorOps automatically.",
            "title": "Setting up VictorOps"
        },
        {
            "location": "/integrations/webhook/victorops/#optional-settings",
            "text": "You can pass in some additional details to VictorOps via the optional fields as specified in the Developer Docs  http://victorops.force.com/knowledgebase/articles/Integration/Alert-Ingestion-API-Documentation",
            "title": "Optional Settings"
        },
        {
            "location": "/account_model/overview/",
            "text": "Overview\n\u00b6\n\n\nThe Outlyer account model is designed around the concept of organizations and accounts. Users can exist in multiple organizations and accounts and can switch between them by using the dropdowns in the global header. \n\n\nOrganizations\n\u00b6\n\n\nAn organization typically maps directly to your company name. On a new sign-up, users are prompted to provide an organization name.\n\n\nFor example, your organization might be called \nAcme-Inc\n.\n\n\nOnce an organization is created, additional members can be invited in.\n\n\nAccounts\n\u00b6\n\n\nAccounts can be created within an organization. For smaller companies these typically map to environments. At scale they can map to services or product service groupings. Accounts allow you to segment your monitoring setup. For MSPs these can even map to customer company names.\n\n\nFor example your accounts under the Acme-Inc organization might be called \nTest\n, \nStaging\n and \nProduction\n. \n\n\nOrganization Admins\n\u00b6\n\n\nThe first user to sign up and create an organization automatically becomes an \norganization admin\n. Additional organization admins can be invited via the user icon with the cog on the organization overview page sidebar.\n\n\nOrganization admins have full access to every account. They can create, rename and delete accounts in addition to managing the members and roles within each account.\n\n\nWhen creating new accounts an organization admin should also enter the account name and invite initial account members. Account member setup can be delegated by setting the role to admin.\n\n\nAccount Members\n\u00b6\n\n\nAccount members can only see accounts that they are a member of. Additional account members can be invited into an account by either organization admins or account admins via the user icon within each account.\n\n\nMembers can be given the following roles:\n\n\n\n\nAdmin - Full account access, invite additional members, assign the admin role\n\n\nMembers - Full account access\n\n\nView Only - View only account access (coming soon)\n\n\n\n\nInvites\n\u00b6\n\n\nInvitations are valid for 7 days and can be resent from the member management area. Invitations can also be cancelled from this area.",
            "title": "Overview"
        },
        {
            "location": "/account_model/overview/#overview",
            "text": "The Outlyer account model is designed around the concept of organizations and accounts. Users can exist in multiple organizations and accounts and can switch between them by using the dropdowns in the global header.",
            "title": "Overview"
        },
        {
            "location": "/account_model/overview/#organizations",
            "text": "An organization typically maps directly to your company name. On a new sign-up, users are prompted to provide an organization name.  For example, your organization might be called  Acme-Inc .  Once an organization is created, additional members can be invited in.",
            "title": "Organizations"
        },
        {
            "location": "/account_model/overview/#accounts",
            "text": "Accounts can be created within an organization. For smaller companies these typically map to environments. At scale they can map to services or product service groupings. Accounts allow you to segment your monitoring setup. For MSPs these can even map to customer company names.  For example your accounts under the Acme-Inc organization might be called  Test ,  Staging  and  Production .",
            "title": "Accounts"
        },
        {
            "location": "/account_model/overview/#organization-admins",
            "text": "The first user to sign up and create an organization automatically becomes an  organization admin . Additional organization admins can be invited via the user icon with the cog on the organization overview page sidebar.  Organization admins have full access to every account. They can create, rename and delete accounts in addition to managing the members and roles within each account.  When creating new accounts an organization admin should also enter the account name and invite initial account members. Account member setup can be delegated by setting the role to admin.",
            "title": "Organization Admins"
        },
        {
            "location": "/account_model/overview/#account-members",
            "text": "Account members can only see accounts that they are a member of. Additional account members can be invited into an account by either organization admins or account admins via the user icon within each account.  Members can be given the following roles:   Admin - Full account access, invite additional members, assign the admin role  Members - Full account access  View Only - View only account access (coming soon)",
            "title": "Account Members"
        },
        {
            "location": "/account_model/overview/#invites",
            "text": "Invitations are valid for 7 days and can be resent from the member management area. Invitations can also be cancelled from this area.",
            "title": "Invites"
        },
        {
            "location": "/getting_started/security/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nSecurity\n\u00b6\n\n\nDataloop(Outlyer) Agent\n\u00b6\n\n\nThe Dataloop Agent connects outbound on port \n443\n only.\n\n\nInstallation of the Dataloop (Outlyer) Agent can be done in a number of ways. Most of our customers use configuration management tools and the repos provided in the public \nOutlyer Github account\n. These are extremely simple; they add a repo file, install the dataloop-agent package, modify a config file and start a service.\n\n\nWe provide a curl installer for people who want to give Outlyer a quick try on a test machine. Curl installers by their very nature are not secure, however they are extremely quick (just a simple copy and paste). We strongly advise people stop using the curl installer after they have moved past the test phase.\n\n\nBy default the Dataloop (Outlyer) Agent runs as a non privileged user \ndataloop\n which can be locked down further by the operating system if required.\n\n\nThe Dataloop (Outlyer) Agent uses an non privileged account key to join Outlyer. On initial start up a key exchange happens and a unique Dataloop (Outlyer) Agent identifier is stored in a fingerprint file in the Dataloop (Outlyer) Agent config directory. This securely registers your server to Outlyer.\n\n\nData Privacy\n\u00b6\n\n\nBy default we only send back basic operating system metrics like CPU, disk, memory, network, and a process list and some metadata about your servers. This metadata includes things like network addresses, environment variables, and metadata from services like AWS and Facter / Ohai. We send all of this data back for the sole purpose of helping you to troubleshoot issues and for auto discovery of services, so that we can automate the setup of your monitoring. We will never share this data with any 3rd party.\n\n\nSolo Mode\n\u00b6\n\n\nOutlyer has a unique technology that allows teams outside of operations to quickly write plugins and deploy them to groups of servers. We recommend that this feature is enabled and used in development and test so that you get the full value out of our self service capability.\n\n\nIn some cases, usually on production or in more tightly controlled environments you will need to turn off these capabilities. For this scenario we provide \nSolo\n mode which completely disables remote script execution and deployment. This is as simple as updating the agent.yaml with solo: yes and restarting the service.\n\n\nIn Solo Mode the Dataloop (Outlyer) Agent no longer polls the Outlyer API for plugin downloads and instead polls the local disk. Drop all of your plugins into the plugins directory and the Dataloop (Outlyer) Agent will still only load those configured in the app. This means you keep the benefit of rapid script creation and deployment, while retaining control of what runs on your servers via your normal config management workflow.",
            "title": "Security and privacy"
        },
        {
            "location": "/getting_started/security/#security",
            "text": "",
            "title": "Security"
        },
        {
            "location": "/getting_started/security/#dataloopoutlyer-agent",
            "text": "The Dataloop Agent connects outbound on port  443  only.  Installation of the Dataloop (Outlyer) Agent can be done in a number of ways. Most of our customers use configuration management tools and the repos provided in the public  Outlyer Github account . These are extremely simple; they add a repo file, install the dataloop-agent package, modify a config file and start a service.  We provide a curl installer for people who want to give Outlyer a quick try on a test machine. Curl installers by their very nature are not secure, however they are extremely quick (just a simple copy and paste). We strongly advise people stop using the curl installer after they have moved past the test phase.  By default the Dataloop (Outlyer) Agent runs as a non privileged user  dataloop  which can be locked down further by the operating system if required.  The Dataloop (Outlyer) Agent uses an non privileged account key to join Outlyer. On initial start up a key exchange happens and a unique Dataloop (Outlyer) Agent identifier is stored in a fingerprint file in the Dataloop (Outlyer) Agent config directory. This securely registers your server to Outlyer.",
            "title": "Dataloop(Outlyer) Agent"
        },
        {
            "location": "/getting_started/security/#data-privacy",
            "text": "By default we only send back basic operating system metrics like CPU, disk, memory, network, and a process list and some metadata about your servers. This metadata includes things like network addresses, environment variables, and metadata from services like AWS and Facter / Ohai. We send all of this data back for the sole purpose of helping you to troubleshoot issues and for auto discovery of services, so that we can automate the setup of your monitoring. We will never share this data with any 3rd party.",
            "title": "Data Privacy"
        },
        {
            "location": "/getting_started/security/#solo-mode",
            "text": "Outlyer has a unique technology that allows teams outside of operations to quickly write plugins and deploy them to groups of servers. We recommend that this feature is enabled and used in development and test so that you get the full value out of our self service capability.  In some cases, usually on production or in more tightly controlled environments you will need to turn off these capabilities. For this scenario we provide  Solo  mode which completely disables remote script execution and deployment. This is as simple as updating the agent.yaml with solo: yes and restarting the service.  In Solo Mode the Dataloop (Outlyer) Agent no longer polls the Outlyer API for plugin downloads and instead polls the local disk. Drop all of your plugins into the plugins directory and the Dataloop (Outlyer) Agent will still only load those configured in the app. This means you keep the benefit of rapid script creation and deployment, while retaining control of what runs on your servers via your normal config management workflow.",
            "title": "Solo Mode"
        },
        {
            "location": "/account_model/private_packs/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nPrivate Packs\n\u00b6\n\n\nPrivate packs can be used to keep your custom plugins, dashboards and alert rules locked together as a single versioned entity within Outlyer.\n\n\nThey are designed to be small containers of monitoring configuration that can be shared around between accounts.\n\n\nCurrently private packs can only be managed with the Outlyer command line utility (dlcli). We will be adding private pack management to the UI soon\n\n\nTerminology:\n\u00b6\n\n\n\n\nPack: The installed entity\n\n\nTemplate: The entity that can be installed from, that creates a pack in the account\n\n\n\n\nCreate a new pack\n\u00b6\n\n\nIn our example we'll create a pack called microservice1. Names must be unique and friendly, ideally something that matches the service the pack will monitor. When used with autodiscovery the pack name is also used to tag agents.\n\n\ndlcli create pack microservice1\n\n\n\n\n\nThis should create a new directory on disk with the following structure:\n\n\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 dashboards\n\u2502   \u2514\u2500\u2500 microservice1.yaml\n\u251c\u2500\u2500 package.yaml\n\u251c\u2500\u2500 plugins\n\u2502   \u2514\u2500\u2500 microservice1.py\n\u2514\u2500\u2500 rules\n    \u2514\u2500\u2500 microservice1.yaml\n\n\n\n\n\nYou can now start to populate this structure with the content created in the Outlyer UI by copying and pasting the plugin content and pressing export to yaml on the dashboards and rules pages. Or alternatively by using the dlcli pull commands. You may also want to update the pack metadata file package.yaml with additional information.\n\n\nOnce you have finished editing your pack locally it's time to upload it into Outlyer as a Template that can be installed.\n\n\nTo upload the pack as a Template in Outlyer, run the following command from inside the pack directory:\n\n\ndlcli push template microservice1 .\n\n\n\n\n\nOnce the pack is pushed you can check it is there by running:\n\n\ndlcli get templates\n\n\n\n\n\nAssuming you are happy to install the pack you can now run\n\n\ndlcli install template microservice1\n\n\n\n\n\nTo uninstall the pack at a later date simply run\n\n\ndlcli rm pack microservice1\n\n\n\n\n\nTo delete the template run:\n\n\ndlcli rm template microservice1\n\n\n\n\n\nNote\n: Installing a template results in a tag being created that matches the pack name. You can therefore tag your agents with the same name as the pack and on install a link will automatically be created.\n\n\nTroubleshooting\n\u00b6\n\n\nYou can show the contents of a template by running:\n\n\ndlcli get template microservice1\n\n\n\n\n\nWhere \nmicroservice1\n is the name of the template you wish to inspect",
            "title": "Creating private packs"
        },
        {
            "location": "/account_model/private_packs/#private-packs",
            "text": "Private packs can be used to keep your custom plugins, dashboards and alert rules locked together as a single versioned entity within Outlyer.  They are designed to be small containers of monitoring configuration that can be shared around between accounts.  Currently private packs can only be managed with the Outlyer command line utility (dlcli). We will be adding private pack management to the UI soon",
            "title": "Private Packs"
        },
        {
            "location": "/account_model/private_packs/#terminology",
            "text": "Pack: The installed entity  Template: The entity that can be installed from, that creates a pack in the account",
            "title": "Terminology:"
        },
        {
            "location": "/account_model/private_packs/#create-a-new-pack",
            "text": "In our example we'll create a pack called microservice1. Names must be unique and friendly, ideally something that matches the service the pack will monitor. When used with autodiscovery the pack name is also used to tag agents.  dlcli create pack microservice1  This should create a new directory on disk with the following structure:  .\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 dashboards\n\u2502   \u2514\u2500\u2500 microservice1.yaml\n\u251c\u2500\u2500 package.yaml\n\u251c\u2500\u2500 plugins\n\u2502   \u2514\u2500\u2500 microservice1.py\n\u2514\u2500\u2500 rules\n    \u2514\u2500\u2500 microservice1.yaml  You can now start to populate this structure with the content created in the Outlyer UI by copying and pasting the plugin content and pressing export to yaml on the dashboards and rules pages. Or alternatively by using the dlcli pull commands. You may also want to update the pack metadata file package.yaml with additional information.  Once you have finished editing your pack locally it's time to upload it into Outlyer as a Template that can be installed.  To upload the pack as a Template in Outlyer, run the following command from inside the pack directory:  dlcli push template microservice1 .  Once the pack is pushed you can check it is there by running:  dlcli get templates  Assuming you are happy to install the pack you can now run  dlcli install template microservice1  To uninstall the pack at a later date simply run  dlcli rm pack microservice1  To delete the template run:  dlcli rm template microservice1  Note : Installing a template results in a tag being created that matches the pack name. You can therefore tag your agents with the same name as the pack and on install a link will automatically be created.",
            "title": "Create a new pack"
        },
        {
            "location": "/account_model/private_packs/#troubleshooting",
            "text": "You can show the contents of a template by running:  dlcli get template microservice1  Where  microservice1  is the name of the template you wish to inspect",
            "title": "Troubleshooting"
        },
        {
            "location": "/account_model/backup_restore/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nBackup and Restore\n\u00b6\n\n\nYou can backup and restore entire organizations, accounts or objects with the Dataloop Command Line Tool and a Linux backup server.\n\n\n\n\nInstall the Dataloop Command Line Utility\n\n\n\n\npip install dlcli\n\n\n\n\n\n\n\nCreate the file /etc/dataloop/dlcli.yaml and enter your details\n\n\n\n\n---\naccount: default\nkey: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\norg: acme-ltd\nurl: https://app.outlyer.com/api/v1\n\n\n\n\n\n\n\nCreate a directory for your backups\n\n\n\n\nmkdir /backup\n\n\n\n\n\n\n\n\n\nCreate a private Git repo called 'dataloop', add deploy keys for the root user with push access and optionally add a webhook to notify a Slack channel. Then clone into /backup so you have /backup/dataloop as your local copy.\n\n\n\n\n\n\nCreate a file called \ndataloop-backup.sh\n in \n/usr/local/bin\n with the following content\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n#!/usr/bin/env bash\n\n\ncd\n /backup/dataloop\nrm -fr /backup/dataloop/*\n/usr/local/bin/dlcli --backupdir /backup/dataloop --settingsfile /etc/dataloop/dlcli.yaml backup org acme-ltd\ngit add .\ngit commit -m \n'backup of acme-ltd org'\n\ngit push\n\n\n\n\n\n\nChange the \nacme-ltd\n to whatever your \norg\n is called.\n\n\n\n\nSetup a cron to run the backup every 10 mins\n\n\n\n\n*/10 * * * * /usr/local/bin/dataloop-backup.sh > /var/log/backup.log 2>&1",
            "title": "Backing up and restoring"
        },
        {
            "location": "/account_model/backup_restore/#backup-and-restore",
            "text": "You can backup and restore entire organizations, accounts or objects with the Dataloop Command Line Tool and a Linux backup server.   Install the Dataloop Command Line Utility   pip install dlcli   Create the file /etc/dataloop/dlcli.yaml and enter your details   ---\naccount: default\nkey: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\norg: acme-ltd\nurl: https://app.outlyer.com/api/v1   Create a directory for your backups   mkdir /backup    Create a private Git repo called 'dataloop', add deploy keys for the root user with push access and optionally add a webhook to notify a Slack channel. Then clone into /backup so you have /backup/dataloop as your local copy.    Create a file called  dataloop-backup.sh  in  /usr/local/bin  with the following content    1\n2\n3\n4\n5\n6\n7 #!/usr/bin/env bash  cd  /backup/dataloop\nrm -fr /backup/dataloop/*\n/usr/local/bin/dlcli --backupdir /backup/dataloop --settingsfile /etc/dataloop/dlcli.yaml backup org acme-ltd\ngit add .\ngit commit -m  'backup of acme-ltd org' \ngit push   Change the  acme-ltd  to whatever your  org  is called.   Setup a cron to run the backup every 10 mins   */10 * * * * /usr/local/bin/dataloop-backup.sh > /var/log/backup.log 2>&1",
            "title": "Backup and Restore"
        },
        {
            "location": "/agent/solo_mode/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nSolo Mode\n\u00b6\n\n\nYou can configure the dataloop Dataloop (Outlyer) Agent to run in solo mode by updating the agent.yaml and restarting the dataloop-agent service.\n\n\nWhen running in solo mode the Dataloop (Outlyer) Agent won't run remote commands from the web interface.\n\n\nIt will also not automatically download plugins.\n\n\nHowever, you should still tag solo mode agents in the usual way so that the metrics coming from these agents appear in dashboards and alerts.\n\n\nSolo mode agents show up in the UI with a little padlock next to their icon. Plugins deployed into the folder will also show up in the agent details page so you can verify they are being run.\n\n\nWhen running agents in solo mode you will also need to deploy a copy of \nbase.py\n locally.\n\n\nAgent Config\n\u00b6\n\n\nTo enable solo mode edit the \nagent.yaml\n file and restart the dataloop-agent service.\n\n\n---\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo:  yes\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxx\n\n### The dataloop server endpoint\nserver: wss://agent.dataloop.io\n\n##  set tags to a comma separated list of tags applied to this agent\ntags: tag1,tag2,tag3\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:\n\n\n\n\n\nLocal Plugin Deployment\n\u00b6\n\n\nYou can deploy plugins to agents running in Solo mode or normal mode. Local plugins take precedence over plugins stored in Dataloop.\n\n\nCopy a plugin into /opt/dataloop/plugins (linux) or c:\\dataloop\\plugins (windows).\nEnsure the file has a valid file extension\nEnsure the file is owned by the dataloop user\nEnsure the file is executable\n\n\n\n\n\nWithin 10 seconds the plugin will be automatically loaded into a running Dataloop agent. The agent.log file will display whether plugins have been successfully loaded or not.\nLocal Plugin Config\n\n\nYou can optionally change plugin settings by creating a file in the plugins directory alongside the plugins.\n\n\nCreate a file called plugin_name.yaml\nThe file must exactly match the name of the plugin except with a yaml extension\nThe file must be owned by the dataloop user\n\n\n\n\n\nAn example yaml file to change the settings for a plugin called node_exporter.sh\n\n\n# cat /opt/dataloop/plugins/node_exporter.yaml\n---\ninterval: 10\nparams: ''\nshell: ['/bin/bash']\ntype: INPROCESS\nformat: PROMETHEUS\n\n\n\n\n\n\n\n\n\ninterval: any number in seconds and defaults to 30 seconds if not supplied\n\n\n\n\n\n\nparams: a string to pass into the plugin as an argument. defaults to '' if not supplied\n\n\n\n\n\n\nshell: a list of commands used to execute the plugin. defaults to [''] if not supplied\n\n\n\n\n\n\ntype: either \nINPROCESS\n or \nSCRIPT\n. defaults to SCRIPT which executes plugins as if they were run on the command line with the shebang. \nINPROCESS\n runs Python 2.7 code in the context of the running Dataloop (Outlyer) Agent code.\n\n\n\n\n\n\nformat: \nNAGIOS\n (default) or \nPROMETHEUS\n depending on the output format of the plugin",
            "title": "Solo mode"
        },
        {
            "location": "/agent/solo_mode/#solo-mode",
            "text": "You can configure the dataloop Dataloop (Outlyer) Agent to run in solo mode by updating the agent.yaml and restarting the dataloop-agent service.  When running in solo mode the Dataloop (Outlyer) Agent won't run remote commands from the web interface.  It will also not automatically download plugins.  However, you should still tag solo mode agents in the usual way so that the metrics coming from these agents appear in dashboards and alerts.  Solo mode agents show up in the UI with a little padlock next to their icon. Plugins deployed into the folder will also show up in the agent details page so you can verify they are being run.  When running agents in solo mode you will also need to deploy a copy of  base.py  locally.",
            "title": "Solo Mode"
        },
        {
            "location": "/agent/solo_mode/#agent-config",
            "text": "To enable solo mode edit the  agent.yaml  file and restart the dataloop-agent service.  ---\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo:  yes\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxx\n\n### The dataloop server endpoint\nserver: wss://agent.dataloop.io\n\n##  set tags to a comma separated list of tags applied to this agent\ntags: tag1,tag2,tag3\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:",
            "title": "Agent Config"
        },
        {
            "location": "/agent/solo_mode/#local-plugin-deployment",
            "text": "You can deploy plugins to agents running in Solo mode or normal mode. Local plugins take precedence over plugins stored in Dataloop.  Copy a plugin into /opt/dataloop/plugins (linux) or c:\\dataloop\\plugins (windows).\nEnsure the file has a valid file extension\nEnsure the file is owned by the dataloop user\nEnsure the file is executable  Within 10 seconds the plugin will be automatically loaded into a running Dataloop agent. The agent.log file will display whether plugins have been successfully loaded or not.\nLocal Plugin Config  You can optionally change plugin settings by creating a file in the plugins directory alongside the plugins.  Create a file called plugin_name.yaml\nThe file must exactly match the name of the plugin except with a yaml extension\nThe file must be owned by the dataloop user  An example yaml file to change the settings for a plugin called node_exporter.sh  # cat /opt/dataloop/plugins/node_exporter.yaml\n---\ninterval: 10\nparams: ''\nshell: ['/bin/bash']\ntype: INPROCESS\nformat: PROMETHEUS    interval: any number in seconds and defaults to 30 seconds if not supplied    params: a string to pass into the plugin as an argument. defaults to '' if not supplied    shell: a list of commands used to execute the plugin. defaults to [''] if not supplied    type: either  INPROCESS  or  SCRIPT . defaults to SCRIPT which executes plugins as if they were run on the command line with the shebang.  INPROCESS  runs Python 2.7 code in the context of the running Dataloop (Outlyer) Agent code.    format:  NAGIOS  (default) or  PROMETHEUS  depending on the output format of the plugin",
            "title": "Local Plugin Deployment"
        },
        {
            "location": "/nagios/",
            "text": "Nagios and Prometheus\n\u00b6\n\n\nPlugin Deployment Models\n\n\nNagios Plugins\n\n\nNagios performance data\n\n\nPrometheus Plugins\n\n\nBuilt-In Python Interpreter\n\n\nPowershell",
            "title": "Introduction"
        },
        {
            "location": "/nagios/#nagios-and-prometheus",
            "text": "Plugin Deployment Models  Nagios Plugins  Nagios performance data  Prometheus Plugins  Built-In Python Interpreter  Powershell",
            "title": "Nagios and Prometheus"
        },
        {
            "location": "/nagios/plugin_deployment_models/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything._\n\n\n\n\nPlugin Deployment Models\n\u00b6\n\n\n1. Default  Mode (centralised configuration)\n\u00b6\n\n\nIn this model you create your plugins in the Outlyer web UI. They get stored\ncentrally within your Outlyer account and can be edited and tested using our\nbuilt-in plugin editor. You can add these plugins to either agents directly, or\ntags of agents, so that they deploy instantly and start returning metrics that\ncan be used on dashboards and in alert rules.\n\n\n\n\nThere are two problems this mode solves; firstly, getting adoption so that more\npeople can collaborate on increasing your monitoring coverage, and secondly,\nreducing the round trip time between knowing what to measure and having those\nmetrics available to use (improving the OODA loop).\n\n\n2. Hybrid  Mode (centralised + decentralised configuration)\n\u00b6\n\n\nThe default mode of operation for the Outlyer Agent also supports loading\nplugins from disk. You can still deploy your plugins using the centralised\nmodel shown above, and additionally you can also put plugins into\n\n/opt/dataloop/plugins\n (or \nc:\\dataloop\\plugins\n on windows) so that they get\nloaded automatically by the agent.\n\n\n\n\nIn this mode you get to keep the benefits of the default mode for ad-hoc\nmonitoring but can can use config management as your primary mechanism for\ndetermining what agents run what plugins, as well as using your version\ncontrolled change management process for deploying them.\n\n\n\n\nIn this mode we will never upload the plugins deployed from disk into Outlyer,\n  so they won\u2019t appear in your plugins list. However, the plugins will show as\n  running in the Outlyer Agent details page. Also, the run button will only ever\n  run plugins written in the browser.\n\n\n\n\n3. Solo Mode (decentralised configuration)\n\u00b6\n\n\nSolo mode disables the ability to create, test and deploy plugins in the\nbrowser. For this mode you will need to use configuration management to deploy\nyour plugins directly into the local plugin directory. These plugins then\nautomatically register inside Outlyer and appear in the Outlyer Agent details\npage when running (but never in the plugins list, since they don\u2019t get\nuploaded to us).\n\n\n\n\nSome benefits to this are that you get to work the way you may always have\nworked with monitoring tools like Nagios and Sensu. In fact, with Outlyer it\u2019s\neven simpler than those tools, you simply need to drop plugins into the correct\ndirectory and the Outlyer Agent will automatically run them. You\u2019ll still need\nto tag your agents as before so that metrics appear in dashboards and alert\nrules based on those tags work.\n\n\nThe advantage of using agents in solo mode over hybrid mode are mostly around\nsecurity. You can run a mix of solo mode agents alongside default mode agents\ndepending on how you want to manage risk.",
            "title": "Plugin deployment models"
        },
        {
            "location": "/nagios/plugin_deployment_models/#plugin-deployment-models",
            "text": "",
            "title": "Plugin Deployment Models"
        },
        {
            "location": "/nagios/plugin_deployment_models/#1-default-mode-centralised-configuration",
            "text": "In this model you create your plugins in the Outlyer web UI. They get stored\ncentrally within your Outlyer account and can be edited and tested using our\nbuilt-in plugin editor. You can add these plugins to either agents directly, or\ntags of agents, so that they deploy instantly and start returning metrics that\ncan be used on dashboards and in alert rules.   There are two problems this mode solves; firstly, getting adoption so that more\npeople can collaborate on increasing your monitoring coverage, and secondly,\nreducing the round trip time between knowing what to measure and having those\nmetrics available to use (improving the OODA loop).",
            "title": "1. Default  Mode (centralised configuration)"
        },
        {
            "location": "/nagios/plugin_deployment_models/#2-hybrid-mode-centralised-decentralised-configuration",
            "text": "The default mode of operation for the Outlyer Agent also supports loading\nplugins from disk. You can still deploy your plugins using the centralised\nmodel shown above, and additionally you can also put plugins into /opt/dataloop/plugins  (or  c:\\dataloop\\plugins  on windows) so that they get\nloaded automatically by the agent.   In this mode you get to keep the benefits of the default mode for ad-hoc\nmonitoring but can can use config management as your primary mechanism for\ndetermining what agents run what plugins, as well as using your version\ncontrolled change management process for deploying them.   In this mode we will never upload the plugins deployed from disk into Outlyer,\n  so they won\u2019t appear in your plugins list. However, the plugins will show as\n  running in the Outlyer Agent details page. Also, the run button will only ever\n  run plugins written in the browser.",
            "title": "2. Hybrid  Mode (centralised + decentralised configuration)"
        },
        {
            "location": "/nagios/plugin_deployment_models/#3-solo-mode-decentralised-configuration",
            "text": "Solo mode disables the ability to create, test and deploy plugins in the\nbrowser. For this mode you will need to use configuration management to deploy\nyour plugins directly into the local plugin directory. These plugins then\nautomatically register inside Outlyer and appear in the Outlyer Agent details\npage when running (but never in the plugins list, since they don\u2019t get\nuploaded to us).   Some benefits to this are that you get to work the way you may always have\nworked with monitoring tools like Nagios and Sensu. In fact, with Outlyer it\u2019s\neven simpler than those tools, you simply need to drop plugins into the correct\ndirectory and the Outlyer Agent will automatically run them. You\u2019ll still need\nto tag your agents as before so that metrics appear in dashboards and alert\nrules based on those tags work.  The advantage of using agents in solo mode over hybrid mode are mostly around\nsecurity. You can run a mix of solo mode agents alongside default mode agents\ndepending on how you want to manage risk.",
            "title": "3. Solo Mode (decentralised configuration)"
        },
        {
            "location": "/nagios/nagios_plugins/",
            "text": "Nagios Plugins\n\u00b6\n\n\nOutlyer supports Nagios standard plugins.\n\n\nThe simplest example of a Nagios plugin written in bash is:\n\n\n1\n2\n3\n4\n5\n#!/usr/bin/env bash\n\n\n\necho\n \n\"OK | metric=0\"\n\n\n\nexit\n \n0\n \n\n\n\n\n\n\nNagios plugins write a string to stdout containing a message, a pipe symbol and then key value pairs separated by an equals sign as per above. You can put multiple key value pairs on the same line separated by spaces. Anything after the pipe symbol can be used as performance data in Outlyer. Tip: ensure there is a pipe symbol in your plugins.\n\n\nYou should also place exit codes in logic throughout your plugins. 0 means OK, 1 means warning, 2 means critical and 3 means unknown. These map to \nplugin_name.status\n metrics in Outlyer and can be used in dashboards and alerts to express a state change which will change widget colours and fire off email alerts if you set them up.\n\n\nYou can also add units of measure and other options to the stdout string of Nagios plugins. The full spec can be found here:\n\n\nhttps://nagios-plugins.org/doc/guidelines.html\n\n\nIn Outlyer something is either considered working, or it is broken. So for alerting off exit codes we only alert when the status is critical (2). Which means the majority of your plugins should emit exit code 0 on success and 2 on failure.\n\n\nIf you need to monitor something generic, and we don't have it as an out of the box plugin, then usually the first place to look is the Nagios Exchange:\n\n\nhttp://exchange.nagios.org/directory/Plugins\n\n\nAlternatively, email us at \nsupport[at]outlyer.com\n or \ncome and find us in Slack\n and we'll create a script for you and place it into our plugin library on Github:\n\n\nhttps://github.com/outlyerapp/plugins",
            "title": "Nagios plugins"
        },
        {
            "location": "/nagios/nagios_plugins/#nagios-plugins",
            "text": "Outlyer supports Nagios standard plugins.  The simplest example of a Nagios plugin written in bash is:  1\n2\n3\n4\n5 #!/usr/bin/env bash  echo   \"OK | metric=0\"  exit   0     Nagios plugins write a string to stdout containing a message, a pipe symbol and then key value pairs separated by an equals sign as per above. You can put multiple key value pairs on the same line separated by spaces. Anything after the pipe symbol can be used as performance data in Outlyer. Tip: ensure there is a pipe symbol in your plugins.  You should also place exit codes in logic throughout your plugins. 0 means OK, 1 means warning, 2 means critical and 3 means unknown. These map to  plugin_name.status  metrics in Outlyer and can be used in dashboards and alerts to express a state change which will change widget colours and fire off email alerts if you set them up.  You can also add units of measure and other options to the stdout string of Nagios plugins. The full spec can be found here:  https://nagios-plugins.org/doc/guidelines.html  In Outlyer something is either considered working, or it is broken. So for alerting off exit codes we only alert when the status is critical (2). Which means the majority of your plugins should emit exit code 0 on success and 2 on failure.  If you need to monitor something generic, and we don't have it as an out of the box plugin, then usually the first place to look is the Nagios Exchange:  http://exchange.nagios.org/directory/Plugins  Alternatively, email us at  support[at]outlyer.com  or  come and find us in Slack  and we'll create a script for you and place it into our plugin library on Github:  https://github.com/outlyerapp/plugins",
            "title": "Nagios Plugins"
        },
        {
            "location": "/nagios/nagios_performance_data/",
            "text": "Nagios Performance Data\n\u00b6\n\n\nYou can create, edit, run / test and deploy Nagios format check scripts in any language from within the Outlyer web interface. This is unique to Outlyer and we believe this is the secret sauce that enables adoption.\n\n\nFor performance data we adhere to the full spec listed \nhere\n (the relevant bit copied below)\n\n\nWe support the absolute minimum in terms of what you can get away with and still have us graph the output. The following is a perfectly legitimate bash script that would graph properly if you used it in Outlyer:\n\n\n1\n2\n3\n#!/bin/bash\n\n\n\necho\n \n\"OK | something=\n$RANDOM\n\"\n\n\n\n\n\n\n\nWe graph everything after the pipe \n|\n symbol. You could even add a space after \nsomething=$RANDOM\n and add another \nwhatever=value\n.\n\n\nIf you deployed this via drag and drop to some agents and wait 30 seconds (our default script interval) then you'll see 'something' and 'whatever' appear as metrics in the dashboard side panel and in the rules criteria metrics drop down.\n\n\nObviously in your scripts you will want to build up the performance data string so that it includes metrics worth graphing. If you have metrics that can be grouped our advice is to use dots to separate. For instance:\n\n\nOK | load.load1min=1234;;;; load.load5min=1234;;;; load.load15min=1234;;;;\n\n\n\n\n\nWe do some cool stuff in the Outlyer UI to group on dots.\n\n\nThe full specification\n\u00b6\n\n\nNagios 3 and newer will concatenate the parts following a \n|\n in a) the first line output by the plugin, and b) in the second to last line, into a string it passes to whatever performance data processing it has configured. (Note that it currently does not insert additional whitespace between both, so the plugin needs to provide some to prevent the last pair of a) and the first of b) getting run together.) Please refer to the Nagios documentation for information on how to configure such processing. However, it is the responsibility of the plugin writer to ensure the performance data is in a \"Nagios Plugins\" format.\n\n\nThis is the expected format: \n\n\n'label'=value[UOM];[warn];[crit];[min];[max]\n\n\n\n\n\nNotes:\n\u00b6\n\n\n\n\nspace separated list of label/value pairs \n\n\nlabel can contain any characters except the equals sign or single quote (') \n\n\nthe single quotes for the label are optional. Required if spaces are in the label \n\n\nlabel length is arbitrary, but ideally the first 19 characters are unique (due to a limitation in RRD). Be aware of a limitation in the amount of data that NRPE returns to Nagios \n\n\nto specify a quote character, use two single quotes \n\n\nwarn, crit, min or max may be null (for example, if the threshold is not defined or min and max do not apply). Trailing unfilled semicolons can be dropped \n\n\nmin and max are not required if UOM=% \n\n\nvalue, min and max in class [-0-9.]. Must all be the same UOM. value may be a literal \"U\" instead, this would indicate that the actual value couldn't be determined \n\n\nwarn and crit are in the range format (see the Section called Threshold and ranges). Must be the same UOM \n\n\nUOM (unit of measurement) is one of: \n\n\nno unit specified - assume a number (int or float) of things (eg, users, processes, load averages) \n\n\ns - seconds (also us, ms) \n\n\n% - percentage \n\n\nB - bytes (also KB, MB, TB) \n\n\nc - a continous counter (such as bytes transmitted on an interface)",
            "title": "Nagios performance data"
        },
        {
            "location": "/nagios/nagios_performance_data/#nagios-performance-data",
            "text": "You can create, edit, run / test and deploy Nagios format check scripts in any language from within the Outlyer web interface. This is unique to Outlyer and we believe this is the secret sauce that enables adoption.  For performance data we adhere to the full spec listed  here  (the relevant bit copied below)  We support the absolute minimum in terms of what you can get away with and still have us graph the output. The following is a perfectly legitimate bash script that would graph properly if you used it in Outlyer:  1\n2\n3 #!/bin/bash  echo   \"OK | something= $RANDOM \"    We graph everything after the pipe  |  symbol. You could even add a space after  something=$RANDOM  and add another  whatever=value .  If you deployed this via drag and drop to some agents and wait 30 seconds (our default script interval) then you'll see 'something' and 'whatever' appear as metrics in the dashboard side panel and in the rules criteria metrics drop down.  Obviously in your scripts you will want to build up the performance data string so that it includes metrics worth graphing. If you have metrics that can be grouped our advice is to use dots to separate. For instance:  OK | load.load1min=1234;;;; load.load5min=1234;;;; load.load15min=1234;;;;  We do some cool stuff in the Outlyer UI to group on dots.",
            "title": "Nagios Performance Data"
        },
        {
            "location": "/nagios/nagios_performance_data/#the-full-specification",
            "text": "Nagios 3 and newer will concatenate the parts following a  |  in a) the first line output by the plugin, and b) in the second to last line, into a string it passes to whatever performance data processing it has configured. (Note that it currently does not insert additional whitespace between both, so the plugin needs to provide some to prevent the last pair of a) and the first of b) getting run together.) Please refer to the Nagios documentation for information on how to configure such processing. However, it is the responsibility of the plugin writer to ensure the performance data is in a \"Nagios Plugins\" format.  This is the expected format:   'label'=value[UOM];[warn];[crit];[min];[max]",
            "title": "The full specification"
        },
        {
            "location": "/nagios/nagios_performance_data/#notes",
            "text": "space separated list of label/value pairs   label can contain any characters except the equals sign or single quote (')   the single quotes for the label are optional. Required if spaces are in the label   label length is arbitrary, but ideally the first 19 characters are unique (due to a limitation in RRD). Be aware of a limitation in the amount of data that NRPE returns to Nagios   to specify a quote character, use two single quotes   warn, crit, min or max may be null (for example, if the threshold is not defined or min and max do not apply). Trailing unfilled semicolons can be dropped   min and max are not required if UOM=%   value, min and max in class [-0-9.]. Must all be the same UOM. value may be a literal \"U\" instead, this would indicate that the actual value couldn't be determined   warn and crit are in the range format (see the Section called Threshold and ranges). Must be the same UOM   UOM (unit of measurement) is one of:   no unit specified - assume a number (int or float) of things (eg, users, processes, load averages)   s - seconds (also us, ms)   % - percentage   B - bytes (also KB, MB, TB)   c - a continous counter (such as bytes transmitted on an interface)",
            "title": "Notes:"
        },
        {
            "location": "/nagios/prometheus_plugins/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nPrometheus Plugins\n\u00b6\n\n\nThe Outlyer Agent can run plugins that scrape Prometheus http endpoints. We\nrecommend that you install a Outlyer Agent on each server and then create a\nseparate plugin to monitor each endpoint on localhost.\n\n\nExample for Node Exporter\n\u00b6\n\n\nStart the Prometheus Node Exporter on a server following the instructions here:\n\n\nhttps://github.com/prometheus/node_exporter\n\n\nThen create a new plugin in Outlyer called \nnode_exporter.py\n.\n\n\nPaste in the following code to scrape the endpoint:\n\n\n1\n2\n3\n#!/usr/bin/env python\n\n\nimport\n \nrequests\n\n\nrequests\n.\nget\n(\n'http://localhost:9100/metrics'\n)\n.\ntext\n\n\n\n\n\n\n\nWhere port 9100 is the port the Node Exporter is running. Select the Outlyer\nAgent and press run to confirm that metrics are being received.\n\n\nNow \nset the output format to Prometheus\n and specify the scrape interval\n(the default is to scrape every 30 seconds) on the plugin details page. Save\nand apply to either a single Outlyer Agent or a tag for deployment.\n\n\nA list of other exporters can be found here:\n\n\nhttps://prometheus.io/docs/instrumenting/exporters/\n\n\nExample for Application Instrumentation\n\u00b6\n\n\nSelect a Prometheus client library for your language.\n\n\nhttps://prometheus.io/docs/instrumenting/clientlibs/\n\n\nEach library has a set of documentation for how to instrument your code. In our\nexample we'll pick a Python app for payments processing. The example shows how\nto import the client library and expose metrics on \nhttp://localhost:8000\n.\n\n\nCreate a plugin called \npayments.py\n with the following content:\n\n\n1\n2\n3\n#!/usr/bin/env python\n\n\nimport\n \nrequests\n\n\nrequests\n.\nget\n(\n'http://localhost:8000/metrics'\n)\n.\ntext\n\n\n\n\n\n\n\nTest this by pressing the run button against a node running the payments\napplication. Remember to switch the plugin format to Prometheus on the plugin\ndetails page. Save and apply to either a single Outlyer Agent or a\ntag for deployment.",
            "title": "Prometheus plugins"
        },
        {
            "location": "/nagios/prometheus_plugins/#prometheus-plugins",
            "text": "The Outlyer Agent can run plugins that scrape Prometheus http endpoints. We\nrecommend that you install a Outlyer Agent on each server and then create a\nseparate plugin to monitor each endpoint on localhost.",
            "title": "Prometheus Plugins"
        },
        {
            "location": "/nagios/prometheus_plugins/#example-for-node-exporter",
            "text": "Start the Prometheus Node Exporter on a server following the instructions here:  https://github.com/prometheus/node_exporter  Then create a new plugin in Outlyer called  node_exporter.py .  Paste in the following code to scrape the endpoint:  1\n2\n3 #!/usr/bin/env python  import   requests  requests . get ( 'http://localhost:9100/metrics' ) . text    Where port 9100 is the port the Node Exporter is running. Select the Outlyer\nAgent and press run to confirm that metrics are being received.  Now  set the output format to Prometheus  and specify the scrape interval\n(the default is to scrape every 30 seconds) on the plugin details page. Save\nand apply to either a single Outlyer Agent or a tag for deployment.  A list of other exporters can be found here:  https://prometheus.io/docs/instrumenting/exporters/",
            "title": "Example for Node Exporter"
        },
        {
            "location": "/nagios/prometheus_plugins/#example-for-application-instrumentation",
            "text": "Select a Prometheus client library for your language.  https://prometheus.io/docs/instrumenting/clientlibs/  Each library has a set of documentation for how to instrument your code. In our\nexample we'll pick a Python app for payments processing. The example shows how\nto import the client library and expose metrics on  http://localhost:8000 .  Create a plugin called  payments.py  with the following content:  1\n2\n3 #!/usr/bin/env python  import   requests  requests . get ( 'http://localhost:8000/metrics' ) . text    Test this by pressing the run button against a node running the payments\napplication. Remember to switch the plugin format to Prometheus on the plugin\ndetails page. Save and apply to either a single Outlyer Agent or a\ntag for deployment.",
            "title": "Example for Application Instrumentation"
        },
        {
            "location": "/nagios/built_in_python/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything._\n\n\n\n\nBuilt-In Python Interpreter\n\u00b6\n\n\nWhen creating or editing a Nagios plugin you can select the shell to use when executing the script from the drop down. The default is 'system default' which means scripts get executed as if you ran them from the command line on the server. On Linux this means whatever you have set the shebang to.\n\n\n'Built-In Python' is a shell we ship with the agent. This is a statically compiled Python 2.7 interpreter with a whole bunch of libraries baked in. When you select this we will always run the contents of your script using this interpreter.\n\n\nHere are a list of libraries currently enabled.\n\n\n###\n# Core Agent dependencies. These are require for the Agent to run\n###\nAPScheduler==3.0.3\nnetifaces==0.10.4\nnose==1.3.7\npsutil==4.3.0\npytz==2016.10\nPyYAML==3.11\nrequests==2.12.3\nwebsocket-client==0.37.0\n\n\n\n\n\n##\n##\n# Plugin dependencies. These are required to run the out of the box plugins\n##\nbackports.ssl-match-hostname==3.4.0.2\nBeautifulSoup==3.2.1\nboto==2.23.0\nboto3==1.4.0\ncffi==1.7.0\ncryptography==1.4\ndlcli==0.3.6\ndocker-py==1.9.0\ngevent==1.0.2\ngrequests==0.3.0\ngoogle-api-python-client==1.4.0\njenkinsapi==0.2.22\nmeld3==1.0.0\nmock==1.0.1\nMySQL-python==1.2.5\nnagioscheck==0.1.6\nnagiosplugin==1.2.2\noauth2client==1.5.2\npexpect==4.0.1\npg8000==1.10.1\npika==0.9.14\npsycopg2==2.6\npycparser==2.14\npyjolokia==0.3.1\npymongo==2.6.3\npynag==0.9.1\npynagios==0.1.1\npyOpenSSL==0.14\npysnmp==4.2.5\npyvmomi==5.5.0.2014.1.1\nredis==2.10.3\nrethinkdb==2.2.0.post6\nrobobrowser==0.5.3\nsix==1.10.0\nsplunk-sdk==1.3.1\ntestinfra==1.0.2\nxmltodict==0.9.0\n\n\n\n\n\nYou can add additional libraries on the server by running pip from the embedded interpreter bin folder.\n\n\nOn Linux:\n\n\n/opt/dataloop/embedded/bin/pip install <module>\n\n\n\n\n\nOn Windows:\n\n\nc:\\dataloop\\embedded\\bin\\pip.exe install <module>\n\n\n\n\n\nIf you contact us at \nsupport[at]outlyer.com\n you can request we bundle other libraries.",
            "title": "Built-in Python interpreter"
        },
        {
            "location": "/nagios/built_in_python/#built-in-python-interpreter",
            "text": "When creating or editing a Nagios plugin you can select the shell to use when executing the script from the drop down. The default is 'system default' which means scripts get executed as if you ran them from the command line on the server. On Linux this means whatever you have set the shebang to.  'Built-In Python' is a shell we ship with the agent. This is a statically compiled Python 2.7 interpreter with a whole bunch of libraries baked in. When you select this we will always run the contents of your script using this interpreter.  Here are a list of libraries currently enabled.  ###\n# Core Agent dependencies. These are require for the Agent to run\n###\nAPScheduler==3.0.3\nnetifaces==0.10.4\nnose==1.3.7\npsutil==4.3.0\npytz==2016.10\nPyYAML==3.11\nrequests==2.12.3\nwebsocket-client==0.37.0  ##\n##\n# Plugin dependencies. These are required to run the out of the box plugins\n##\nbackports.ssl-match-hostname==3.4.0.2\nBeautifulSoup==3.2.1\nboto==2.23.0\nboto3==1.4.0\ncffi==1.7.0\ncryptography==1.4\ndlcli==0.3.6\ndocker-py==1.9.0\ngevent==1.0.2\ngrequests==0.3.0\ngoogle-api-python-client==1.4.0\njenkinsapi==0.2.22\nmeld3==1.0.0\nmock==1.0.1\nMySQL-python==1.2.5\nnagioscheck==0.1.6\nnagiosplugin==1.2.2\noauth2client==1.5.2\npexpect==4.0.1\npg8000==1.10.1\npika==0.9.14\npsycopg2==2.6\npycparser==2.14\npyjolokia==0.3.1\npymongo==2.6.3\npynag==0.9.1\npynagios==0.1.1\npyOpenSSL==0.14\npysnmp==4.2.5\npyvmomi==5.5.0.2014.1.1\nredis==2.10.3\nrethinkdb==2.2.0.post6\nrobobrowser==0.5.3\nsix==1.10.0\nsplunk-sdk==1.3.1\ntestinfra==1.0.2\nxmltodict==0.9.0  You can add additional libraries on the server by running pip from the embedded interpreter bin folder.  On Linux:  /opt/dataloop/embedded/bin/pip install <module>  On Windows:  c:\\dataloop\\embedded\\bin\\pip.exe install <module>  If you contact us at  support[at]outlyer.com  you can request we bundle other libraries.",
            "title": "Built-In Python Interpreter"
        },
        {
            "location": "/nagios/powershell/",
            "text": "Powershell\n\u00b6\n\n\nYou can create native Powershell plugins in Outlyer by creating a custom shell and ensuring your Powershell code exits correctly. \n\n\nOpen the Settings page in your account and create a new shell called 'Powershell' with the path set to:\n\n\nC:\\Windows\\System32\\WindowsPowershell\\v1.0\\powershell.exe -executionpolicy bypass -File\n\n\n\n\n\nWhen creating plugins ensure they have the .ps1 extension set and change the shell on the details page to 'Powershell'.\n\n\nWhen writing Powershell plugins please use the following function to set the exit code correctly so that Outlyer gets the correct code returned.\n\n\nfunction ExitWithCode\n{\n    param\n    (\n        $exitcode\n    )  \n    $host.SetShouldExit($exitcode)\n    exit $exitcode\n}\n\n\n\n\n\nThen wherever you would usually exit use ExitWithCode instead. This will enable passing of the standard nagios exit codes 0,1,2 and 3 back to Outlyer for use in dashboard status widgets and alerts.\n\n\nYou can also use arguments by updating the shell arguments on the plugins details page. For example if you set:\n\n\n-arg1 123 -arg2 456\n\n\n\n\n\nAs the shell arguments on a plugin. You can then reference them in your Powershell script by first definining them as params \n\n\n[CmdletBinding()]\nParam\n(\n    [string]$arg1,\n    [string]$arg2\n)\n\n\n\n\n\nThen referencing the arguments in your scripts with \n$arg1\n and \n$arg2\n or whatever you name your arguments.",
            "title": "Powershell"
        },
        {
            "location": "/nagios/powershell/#powershell",
            "text": "You can create native Powershell plugins in Outlyer by creating a custom shell and ensuring your Powershell code exits correctly.   Open the Settings page in your account and create a new shell called 'Powershell' with the path set to:  C:\\Windows\\System32\\WindowsPowershell\\v1.0\\powershell.exe -executionpolicy bypass -File  When creating plugins ensure they have the .ps1 extension set and change the shell on the details page to 'Powershell'.  When writing Powershell plugins please use the following function to set the exit code correctly so that Outlyer gets the correct code returned.  function ExitWithCode\n{\n    param\n    (\n        $exitcode\n    )  \n    $host.SetShouldExit($exitcode)\n    exit $exitcode\n}  Then wherever you would usually exit use ExitWithCode instead. This will enable passing of the standard nagios exit codes 0,1,2 and 3 back to Outlyer for use in dashboard status widgets and alerts.  You can also use arguments by updating the shell arguments on the plugins details page. For example if you set:  -arg1 123 -arg2 456  As the shell arguments on a plugin. You can then reference them in your Powershell script by first definining them as params   [CmdletBinding()]\nParam\n(\n    [string]$arg1,\n    [string]$arg2\n)  Then referencing the arguments in your scripts with  $arg1  and  $arg2  or whatever you name your arguments.",
            "title": "Powershell"
        },
        {
            "location": "/endpoints/",
            "text": "Metric Collection Endpoints\n\u00b6\n\n\nBesides running plugins, Dataloop supports a number of additional ways to\ncollect metrics from your system. Generally these are network endpoints that\nlisten for connections and then forward the metrics on to Dataloop. Some of\nthese are hosted by Dataloop; others can be hosted inside your own network.\n\n\nGraphite, CollectD & InfluxDB\n\u00b6\n\n\nInfluxDB\n\n\nCollectD\n\n\nGraphite\n  \n\n\nStatsD\n\u00b6\n\n\nStatsD\n\n\nHosted StatsD Server\n\n\nSelf-Hosted StatsD\n\n\nSelf-Hosted Statsite\n\n\nStatsD Clients",
            "title": "About custom endpoints"
        },
        {
            "location": "/endpoints/#metric-collection-endpoints",
            "text": "Besides running plugins, Dataloop supports a number of additional ways to\ncollect metrics from your system. Generally these are network endpoints that\nlisten for connections and then forward the metrics on to Dataloop. Some of\nthese are hosted by Dataloop; others can be hosted inside your own network.",
            "title": "Metric Collection Endpoints"
        },
        {
            "location": "/endpoints/#graphite-collectd-influxdb",
            "text": "InfluxDB  CollectD  Graphite",
            "title": "Graphite, CollectD &amp; InfluxDB"
        },
        {
            "location": "/endpoints/#statsd",
            "text": "StatsD  Hosted StatsD Server  Self-Hosted StatsD  Self-Hosted Statsite  StatsD Clients",
            "title": "StatsD"
        },
        {
            "location": "/endpoints/influxdb/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \u201cdataloop agent\u201d, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nInfluxDB\n\u00b6\n\n\nYou can send InfluxDB metrics into Outlyer using the following url:\n\n\nhttp://fingerprint@influxdb.dataloop.io:8086\n\n\nWhere fingerprint is the string found in \n/etc/dataloop/agent.finger\n on a server.",
            "title": "InfluxDB"
        },
        {
            "location": "/endpoints/influxdb/#influxdb",
            "text": "You can send InfluxDB metrics into Outlyer using the following url:  http://fingerprint@influxdb.dataloop.io:8086  Where fingerprint is the string found in  /etc/dataloop/agent.finger  on a server.",
            "title": "InfluxDB"
        },
        {
            "location": "/endpoints/collectd/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \u201cdataloop agent\u201d, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nCollectD\n\u00b6\n\n\nCollectD Configuration\n\u00b6\n\n\nIt's best to use the latest CollectD package. You can find instructions for setting up the repo's for Debian and RedHat based distros here:\n\n\nhttps://github.com/collectd/collectd-ci/blob/master/README.md\n\n\nChange the following lines in your collectd.conf file. Where fingerprint is the string from \n/etc/dataloop/agent.finger\n. You may need to uncomment the \nLoadPlugin\n line.\n\n\nHostname    \"fingerprint\"\n\nLoadPlugin  write_graphite\n\n<Plugin write_graphite>\n  <Node \"fingerprint\">\n    Host \"graphite.dataloop.io\"\n    Port \"2003\"\n    Protocol \"udp\"\n    LogSendErrors true\n    Prefix \"\"\n    Postfix \".collectd\"\n    StoreRates true\n    AlwaysAppendDS false\n    EscapeCharacter \"_\"\n  </Node>\n</Plugin>\n\n\n\n\n\nThe metrics will appear under `collectd' when browsing in the Dataloop web interface.\n\n\nOther Software\n\u00b6\n\n\nLots of open source tools have a Graphite backend. In general you should only need to configure two settings; the graphite server address, and the metric prefix. Where our server address is \ngraphite.dataloop.io\n and our metric prefix is your fingerprint.",
            "title": "CollectD"
        },
        {
            "location": "/endpoints/collectd/#collectd",
            "text": "",
            "title": "CollectD"
        },
        {
            "location": "/endpoints/collectd/#collectd-configuration",
            "text": "It's best to use the latest CollectD package. You can find instructions for setting up the repo's for Debian and RedHat based distros here:  https://github.com/collectd/collectd-ci/blob/master/README.md  Change the following lines in your collectd.conf file. Where fingerprint is the string from  /etc/dataloop/agent.finger . You may need to uncomment the  LoadPlugin  line.  Hostname    \"fingerprint\"\n\nLoadPlugin  write_graphite\n\n<Plugin write_graphite>\n  <Node \"fingerprint\">\n    Host \"graphite.dataloop.io\"\n    Port \"2003\"\n    Protocol \"udp\"\n    LogSendErrors true\n    Prefix \"\"\n    Postfix \".collectd\"\n    StoreRates true\n    AlwaysAppendDS false\n    EscapeCharacter \"_\"\n  </Node>\n</Plugin>  The metrics will appear under `collectd' when browsing in the Dataloop web interface.",
            "title": "CollectD Configuration"
        },
        {
            "location": "/endpoints/collectd/#other-software",
            "text": "Lots of open source tools have a Graphite backend. In general you should only need to configure two settings; the graphite server address, and the metric prefix. Where our server address is  graphite.dataloop.io  and our metric prefix is your fingerprint.",
            "title": "Other Software"
        },
        {
            "location": "/endpoints/graphite/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \u201cdataloop agent\u201d, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nGraphite\n\u00b6\n\n\nYou can stream metrics directly into \ngraphite.dataloop.io\n on tcp port \n2003\n using the Graphite metric format.\n\n\nThis is great for high resolution metrics up to 1 second granularity or for long running jobs that you may want to schedule via cron. Many open source metric collection tools have Graphite backends which can be configured to stream metrics directly into Dataloop.\n\n\nThe plain text protocol is described here:\n\n\nhttp://graphite.readthedocs.org/en/latest/feeding-carbon.html\n\n\nThe metric path section of the string determines how Dataloop will store the data. The format should be:\n\n\nfingerprint.metric.path value timestamp\n\n\n\n\n\nThe timestamp value is optional. If you don't set one we will set the timestamp as the time the metric arrived. You can send us historical timestamps if you wish to migrate data into Dataloop.\n\n\nCurrently we only allow Graphite data to be attached to an Agent fingerprint.\n\n\nThe first step is therefore to install a Dataloop Agent on your server. After that you can find your Agent fingerprint on Linux in \n/etc/dataloop/agent.finger\n, or in the Dataloop UI on the Dataloop (Outlyer) Agent details page.\n\n\nTesting\n\u00b6\n\n\nSet the fingerprint variable so we can use it in the metric path:\n\n\nfinger=$(sed -n -e 's/fingerprint = //p' /etc/dataloop/agent.finger)\n\n\n\n\n\nThen echo a random metric to our graphite port:\n\n\necho \"${finger}.test $RANDOM\" | nc -c graphite.dataloop.io 2003\n\n\n\n\n\nYou can now create a dashboard widget in Outlyer and browse to the Dataloop (Outlyer) Agent you sent the metric in from.\n\n\nSimple Example\n\u00b6\n\n\nStart by finding your fingerprint:\n\n\ncat /etc/dataloop/agent.finger\n\n[private]\nfingerprint = 2afee216-a80f-4f01-9220-14bc3195a3d5\n\n\n\n\n\nThen simply echo the fingerprint followed by a dot separated metric path followed by your value.\n\n\necho \"2afee216-a80f-4f01-9220-14bc3195a3d5.some.metric.path $RANDOM\" | nc -c graphite.dataloop.io 2003\n\n\n\n\n\nIn our example we just sent in a random value. When you login to the Outlyer UI you'll now see some.metric.path show up in the dashboard tree browser and the alerts metric drop-down.\n\n\nWarning: There are a couple of implementations of netcat. The above example with the -c option was for the GNU implementation. If you are using a BSD implementation (like OSX) then use -q0 instead. \n\n\nTroubleshooting\n\u00b6\n\n\nThe easiest way to troubleshoot a Graphite backend integration is to open a netcat instance listening on another port. For example\n\n\nnetcat -l -k 2004\n\n\n\n\n\nThen temporarily redirect your 3rd party software to send to localhost port 2004 tcp. The netcat listener will print out the metrics so you can compare to the format listed above.",
            "title": "Graphite"
        },
        {
            "location": "/endpoints/graphite/#graphite",
            "text": "You can stream metrics directly into  graphite.dataloop.io  on tcp port  2003  using the Graphite metric format.  This is great for high resolution metrics up to 1 second granularity or for long running jobs that you may want to schedule via cron. Many open source metric collection tools have Graphite backends which can be configured to stream metrics directly into Dataloop.  The plain text protocol is described here:  http://graphite.readthedocs.org/en/latest/feeding-carbon.html  The metric path section of the string determines how Dataloop will store the data. The format should be:  fingerprint.metric.path value timestamp  The timestamp value is optional. If you don't set one we will set the timestamp as the time the metric arrived. You can send us historical timestamps if you wish to migrate data into Dataloop.  Currently we only allow Graphite data to be attached to an Agent fingerprint.  The first step is therefore to install a Dataloop Agent on your server. After that you can find your Agent fingerprint on Linux in  /etc/dataloop/agent.finger , or in the Dataloop UI on the Dataloop (Outlyer) Agent details page.",
            "title": "Graphite"
        },
        {
            "location": "/endpoints/graphite/#testing",
            "text": "Set the fingerprint variable so we can use it in the metric path:  finger=$(sed -n -e 's/fingerprint = //p' /etc/dataloop/agent.finger)  Then echo a random metric to our graphite port:  echo \"${finger}.test $RANDOM\" | nc -c graphite.dataloop.io 2003  You can now create a dashboard widget in Outlyer and browse to the Dataloop (Outlyer) Agent you sent the metric in from.",
            "title": "Testing"
        },
        {
            "location": "/endpoints/graphite/#simple-example",
            "text": "Start by finding your fingerprint:  cat /etc/dataloop/agent.finger\n\n[private]\nfingerprint = 2afee216-a80f-4f01-9220-14bc3195a3d5  Then simply echo the fingerprint followed by a dot separated metric path followed by your value.  echo \"2afee216-a80f-4f01-9220-14bc3195a3d5.some.metric.path $RANDOM\" | nc -c graphite.dataloop.io 2003  In our example we just sent in a random value. When you login to the Outlyer UI you'll now see some.metric.path show up in the dashboard tree browser and the alerts metric drop-down.  Warning: There are a couple of implementations of netcat. The above example with the -c option was for the GNU implementation. If you are using a BSD implementation (like OSX) then use -q0 instead.",
            "title": "Simple Example"
        },
        {
            "location": "/endpoints/graphite/#troubleshooting",
            "text": "The easiest way to troubleshoot a Graphite backend integration is to open a netcat instance listening on another port. For example  netcat -l -k 2004  Then temporarily redirect your 3rd party software to send to localhost port 2004 tcp. The netcat listener will print out the metrics so you can compare to the format listed above.",
            "title": "Troubleshooting"
        },
        {
            "location": "/endpoints/statsd/",
            "text": "Statsd\n\u00b6\n\n\nOnce you have your \nStatsD Server\n and \nStatsD Client\n setup you can start sending metrics to Outlyer. This document explains how to use StatsD.\n\n\nThe information below is from: \nhttps://github.com/b/statsd_spec\n\n\nStatsD Metrics Export Specification v0.1\n\u00b6\n\n\nThis document describes current practice for the implementation of the various incarnations of the StatsD metric collection protocol. The protocol originated at Flickr, was further developed at Etsy, and has been subsequently influenced by Coda Hale's Metrics. The intent of this document is not to specify or enforce a standard, but only to act as a snapshot of current practice and a guide to ease implementation.\n\n\nTerminology\n\u00b6\n\n\nStatsD is used to collect metrics from infrastructure. It is push-based: clients export metrics to a collection server, which in turn derives aggregate metrics and often drives graphing systems such as Graphite. Few assumptions are made about how the data is processed or exposed.\n\n\nThe terms metrics and infrastructure are both defined broadly. A metric is a measurement composed of a name, a value, a type, and sometimes additional information describing how a metric should be interpreted. Infrastructure is any part of a technology stack, from datacenter UPS controllers and temperature sensors in servers all the way up to function calls in applications and user interactions in a browser.\n\n\nIf it can be structured as one of the metric types below, it can consumed by StatsD.\n\n\nMetric Types & Formats\n\u00b6\n\n\nThe format of exported metrics is UTF-8 text, with metrics separated by newlines. Metrics are generally of the form \n<metric name>:<value>|<type>\n, with exceptions noted in the metric type definitions below.\n\n\nThe protocol allows for both integer and floating point values. Most implementations store values internally as a IEEE 754 double precision float, but many implementations and graphing systems only support integer values. For compatibility all values should be integers in the range \n(-2^53^, 2^53^)\n.\n\n\nGauges\n\u00b6\n\n\nA gauge is an instantaneous measurement of a value, like the gas gauge in a car. It differs from a counter by being calculated at the client rather than the server. Valid gauge values are in the range \n[0, 2^64^)\n\n\n<metric name>:<value>|g\n\n\n\n\n\nCounters\n\u00b6\n\n\nA counter is a gauge calculated at the server. Metrics sent by the client increment or decrement the value of the gauge rather than giving its current value. Counters may also have an associated sample rate, given as a decimal of the number of samples per event count. For example, a sample rate of 1/10 would be exported as 0.1. Valid counter values are in the range (-2^63^, 2^63^).\n\n\n<metric name>:<value>|c[|@<sample rate>]\n\n\n\n\n\nTimers\n\u00b6\n\n\nA timer is a measure of the number of milliseconds elapsed between a start and end time, for example the time to complete rendering of a web page for a user. Valid timer values are in the range [0, 2^64^).\n\n\n<metric name>:<value>|ms\n\n\n\n\n\nHistograms\n\u00b6\n\n\nA histogram is a measure of the distribution of timer values over time, calculated at the server. As the data exported for timers and histograms is the same, this is currently an alias for a timer. Valid histogram values are in the range [0, 2^64^).\n\n\n<metric name>:<value>|h\n\n\n\n\n\nMeters\n\u00b6\n\n\nA meter measures the rate of events over time, calculated at the server. They may also be thought of as increment-only counters. Valid meter values are in the range [0, 2^64^).\n\n\n<metric name>:<value>|m\n\n\n\n\n\nIn at least one implementation, this is abbreviated for the common case of incrementing the meter by 1.\n\n\n<metric name>\n\n\n\n\n\nWhile this is convenient, the full, explicit metric form should be used. The shortened form is documented here for completeness.",
            "title": "About Statsd"
        },
        {
            "location": "/endpoints/statsd/#statsd",
            "text": "Once you have your  StatsD Server  and  StatsD Client  setup you can start sending metrics to Outlyer. This document explains how to use StatsD.  The information below is from:  https://github.com/b/statsd_spec",
            "title": "Statsd"
        },
        {
            "location": "/endpoints/statsd/#statsd-metrics-export-specification-v01",
            "text": "This document describes current practice for the implementation of the various incarnations of the StatsD metric collection protocol. The protocol originated at Flickr, was further developed at Etsy, and has been subsequently influenced by Coda Hale's Metrics. The intent of this document is not to specify or enforce a standard, but only to act as a snapshot of current practice and a guide to ease implementation.",
            "title": "StatsD Metrics Export Specification v0.1"
        },
        {
            "location": "/endpoints/statsd/#terminology",
            "text": "StatsD is used to collect metrics from infrastructure. It is push-based: clients export metrics to a collection server, which in turn derives aggregate metrics and often drives graphing systems such as Graphite. Few assumptions are made about how the data is processed or exposed.  The terms metrics and infrastructure are both defined broadly. A metric is a measurement composed of a name, a value, a type, and sometimes additional information describing how a metric should be interpreted. Infrastructure is any part of a technology stack, from datacenter UPS controllers and temperature sensors in servers all the way up to function calls in applications and user interactions in a browser.  If it can be structured as one of the metric types below, it can consumed by StatsD.",
            "title": "Terminology"
        },
        {
            "location": "/endpoints/statsd/#metric-types-formats",
            "text": "The format of exported metrics is UTF-8 text, with metrics separated by newlines. Metrics are generally of the form  <metric name>:<value>|<type> , with exceptions noted in the metric type definitions below.  The protocol allows for both integer and floating point values. Most implementations store values internally as a IEEE 754 double precision float, but many implementations and graphing systems only support integer values. For compatibility all values should be integers in the range  (-2^53^, 2^53^) .",
            "title": "Metric Types &amp; Formats"
        },
        {
            "location": "/endpoints/statsd/#gauges",
            "text": "A gauge is an instantaneous measurement of a value, like the gas gauge in a car. It differs from a counter by being calculated at the client rather than the server. Valid gauge values are in the range  [0, 2^64^)  <metric name>:<value>|g",
            "title": "Gauges"
        },
        {
            "location": "/endpoints/statsd/#counters",
            "text": "A counter is a gauge calculated at the server. Metrics sent by the client increment or decrement the value of the gauge rather than giving its current value. Counters may also have an associated sample rate, given as a decimal of the number of samples per event count. For example, a sample rate of 1/10 would be exported as 0.1. Valid counter values are in the range (-2^63^, 2^63^).  <metric name>:<value>|c[|@<sample rate>]",
            "title": "Counters"
        },
        {
            "location": "/endpoints/statsd/#timers",
            "text": "A timer is a measure of the number of milliseconds elapsed between a start and end time, for example the time to complete rendering of a web page for a user. Valid timer values are in the range [0, 2^64^).  <metric name>:<value>|ms",
            "title": "Timers"
        },
        {
            "location": "/endpoints/statsd/#histograms",
            "text": "A histogram is a measure of the distribution of timer values over time, calculated at the server. As the data exported for timers and histograms is the same, this is currently an alias for a timer. Valid histogram values are in the range [0, 2^64^).  <metric name>:<value>|h",
            "title": "Histograms"
        },
        {
            "location": "/endpoints/statsd/#meters",
            "text": "A meter measures the rate of events over time, calculated at the server. They may also be thought of as increment-only counters. Valid meter values are in the range [0, 2^64^).  <metric name>:<value>|m  In at least one implementation, this is abbreviated for the common case of incrementing the meter by 1.  <metric name>  While this is convenient, the full, explicit metric form should be used. The shortened form is documented here for completeness.",
            "title": "Meters"
        },
        {
            "location": "/endpoints/hosted_statsd/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \u201cdataloop agent\u201d, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nHosted StatsD Server\n\u00b6\n\n\nIf you are on an enterprise Outlyer plan, we'd be happy to manage a hosted StatsD and Grafana 3.0 instance for you. Please contact support to make arrangements.\n\n\nTo use your hosted StatsD server (statsite) simply point your StatsD client at the Dataloop (Outlyer) agent in your account. The server address to use is in the format:\n\n\nfingerprint.statsd.dataloop.io\n\n\n\n\n\nTo find your fingerprint log into your account and look for the 'dataloop' Dataloop (Outlyer) Agent on the Setup Monitoring page. Click the little (i) next to it to look up the info.\n\n\n\n\nNow on the Dataloop (Outlyer) Agent details page look for the fingerprint of your agent.\n\n\n\n\nIn this example we would configure our StatsD clients to send to:\n\n\n1a202c1d-77a5-4480-8886-c936e4944fb8.statsd.dataloop.io\n\n\n\n\n\nOn the default UDP port 8125. Your server address will match whatever your fingerprint is set to (fingerprints are unique between agents).\n\n\nYou can then find your StatsD metrics by browsing the Dataloop (Outlyer) Agent or using the statsd tag inside Dataloop.",
            "title": "Hosted statsd server"
        },
        {
            "location": "/endpoints/hosted_statsd/#hosted-statsd-server",
            "text": "If you are on an enterprise Outlyer plan, we'd be happy to manage a hosted StatsD and Grafana 3.0 instance for you. Please contact support to make arrangements.  To use your hosted StatsD server (statsite) simply point your StatsD client at the Dataloop (Outlyer) agent in your account. The server address to use is in the format:  fingerprint.statsd.dataloop.io  To find your fingerprint log into your account and look for the 'dataloop' Dataloop (Outlyer) Agent on the Setup Monitoring page. Click the little (i) next to it to look up the info.   Now on the Dataloop (Outlyer) Agent details page look for the fingerprint of your agent.   In this example we would configure our StatsD clients to send to:  1a202c1d-77a5-4480-8886-c936e4944fb8.statsd.dataloop.io  On the default UDP port 8125. Your server address will match whatever your fingerprint is set to (fingerprints are unique between agents).  You can then find your StatsD metrics by browsing the Dataloop (Outlyer) Agent or using the statsd tag inside Dataloop.",
            "title": "Hosted StatsD Server"
        },
        {
            "location": "/endpoints/self-hosted_statsd/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \u201cdataloop agent\u201d, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nSelf-Hosted StatsD Server\n\u00b6\n\n\nStatsD is a service that aggregates metrics over time. We plan to provide hosted StatsD in the future, but for now you'll need to setup StatsD on your own servers and point them at the Outlyer Graphite port.\n\n\nLike everything in Outlyer you need to bind your metrics to a fingerprint. The easiest way to do this may be to setup an AWS micro instance and install a Dataloop (Outlyer) Agent on it. You could give the server a host name like 'statsd' so that the Dataloop (Outlyer) Agent appears in Dataloop with a friendly name. Drag this Dataloop (Outlyer) Agent into the appropriate part of your hierarchy.\n\n\nInstalling StatsD\n\u00b6\n\n\n\n\nClone the statsd repository from Etsy's Github account\n\n\n\n\ngit clone https://github.com/etsy/statsd.git\n\n\n\n\n\n\n\nCopy \nexampleConfig.js\n to \nconfig.js\n and edit the file so the bottom section is like this:\n\n\n\n\n{\n graphitePort: 2003\n, graphiteHost: \"graphite.dataloop.io\"\n, port: 8125\n, graphite: { legacyNamespace: false, globalPrefix: \"FINGERPRINT\" }\n, backends: [ \"./backends/graphite\" ]\n, deleteIdleStats: true\n, debug: true\n}\n\n\n\n\n\nWhere \nFINGERPRINT\n is the string found in \n/etc/dataloop/agent.finger\n. Often people will install the Dataloop Agent on their StatsD server and use this fingerprint. But you could bind your StatsD metrics to any fingerprint.\n\n\nThen start statsd by running:\n\n\nnode stats.js config.js\n\n\n\n\n\n\n\nOpen Dataloop in a browser. Click on the dashboards tab and then browse down to your StatsD Dataloop (Outlyer) Agent in the tree. You should see some metrics already streaming in.\n\n\n\n\nTesting\n\u00b6\n\n\nFrom on your StatsD server itself you can run:\n\n\necho \"foo:1|c\" | nc -u -w0 127.0.0.1 8125",
            "title": "Self-hosted statsd"
        },
        {
            "location": "/endpoints/self-hosted_statsd/#self-hosted-statsd-server",
            "text": "StatsD is a service that aggregates metrics over time. We plan to provide hosted StatsD in the future, but for now you'll need to setup StatsD on your own servers and point them at the Outlyer Graphite port.  Like everything in Outlyer you need to bind your metrics to a fingerprint. The easiest way to do this may be to setup an AWS micro instance and install a Dataloop (Outlyer) Agent on it. You could give the server a host name like 'statsd' so that the Dataloop (Outlyer) Agent appears in Dataloop with a friendly name. Drag this Dataloop (Outlyer) Agent into the appropriate part of your hierarchy.",
            "title": "Self-Hosted StatsD Server"
        },
        {
            "location": "/endpoints/self-hosted_statsd/#installing-statsd",
            "text": "Clone the statsd repository from Etsy's Github account   git clone https://github.com/etsy/statsd.git   Copy  exampleConfig.js  to  config.js  and edit the file so the bottom section is like this:   {\n graphitePort: 2003\n, graphiteHost: \"graphite.dataloop.io\"\n, port: 8125\n, graphite: { legacyNamespace: false, globalPrefix: \"FINGERPRINT\" }\n, backends: [ \"./backends/graphite\" ]\n, deleteIdleStats: true\n, debug: true\n}  Where  FINGERPRINT  is the string found in  /etc/dataloop/agent.finger . Often people will install the Dataloop Agent on their StatsD server and use this fingerprint. But you could bind your StatsD metrics to any fingerprint.  Then start statsd by running:  node stats.js config.js   Open Dataloop in a browser. Click on the dashboards tab and then browse down to your StatsD Dataloop (Outlyer) Agent in the tree. You should see some metrics already streaming in.",
            "title": "Installing StatsD"
        },
        {
            "location": "/endpoints/self-hosted_statsd/#testing",
            "text": "From on your StatsD server itself you can run:  echo \"foo:1|c\" | nc -u -w0 127.0.0.1 8125",
            "title": "Testing"
        },
        {
            "location": "/endpoints/self-hosted_statsite/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nSelf-Hosted Statsite\n\u00b6\n\n\nStatsite is an awesome alternative to the Etsy StatsD server implementation. Install as per the instructions here:\n\n\nhttps://github.com/armon/statsite\n\n\nThen create a \nstatsite.conf\n with the following configuration:\n\n\n[statsite]\nbind_address = 0.0.0.0\ntcp_port = 8125\nudp_port = 8125\nlog_level = INFO\nflush_interval = 10\ntimer_eps = 0.01\nset_eps = 0.02\nstream_cmd = python sinks/graphite.py graphite.dataloop.io 2003 fingerprint.\ninput_counter = numStats\nextended_counters = true\n\n\n\n\n\nThe important line here is the stream_cmd setting. You may need to update the location of the graphite.py file. The last part of the line is the metric prefix that is sent to Outlyer. Usually people install a Dataloop (Outlyer) Agent onto their Statsite server so they can use the fingerprint string from \n/etc/dataloop/agent.finger\n. Remember to append a fullstop to the end of the fingerprint.\n\n\nAn example of a good stream_cmd line is:\n\n\nstream_cmd = python sinks/graphite.py graphite.dataloop.io 2003 529b7fc2-2de5-4525-983a-184c8f43c085.\n\n\n\n\n\nTroubleshooting\n\u00b6\n\n\nThe only reliable way I have found to troubleshoot this is to open up netcat to listen on a new port, then temporarily change statsite.conf to send metrics to it.\n\n\nFor example open a new tab and run netcat to listen on port 2004\n\n\n# nc -l 2004\n\n\n\n\n\nNow change the stream_cmd to point to port 2004 localhost\n\n\nstream_cmd = python sinks/graphite.py localhost 2004 529b7fc2-2de5-4525-983a-184c8f43c085.\n\n\n\n\n\nStart statsite and wait for 10 seconds. Your netcat listening tab will show you the metrics being sent. An example of some that look correct:\n\n\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.sum 57619.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.sum_sq 571869.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.mean 9.741167 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.lower 1.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.upper 10.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.count 5915 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.stdev 1.338330 1436185723\n\n\n\n\n\nHere we can see the fingerprint first, then the dot seperator between the metric paths followed by the value and timestamp. If you have a malformed \nfingerprint.metric.path\n then metrics won't display in Outlyer.\n\n\nOnce you have confirmed it's working against localhost port 2004 you can flip it back to sending to Outlyer. Metrics will appear under the Dataloop (Outlyer) Agent fingerprint that you used.\n\n\nAlso, be careful about the time on your Statsite server. Installing NTP should help with this. Outlyer will display whatever values against the timestamp you send in, so if your timestamps are wrong strangeness abounds.\n\n\nSending to Graphite + Outlyer\n\u00b6\n\n\nSometimes people want to keep their local Graphite server running in parallel. You can do that quite easily with Statsite by changing the \nsink/graphite.py\n that they provide to a modified version.\n\n\nGrab the modified one from here: \ngist\n\n\nYou'll need to modify the \nself.fingerprint\n variable in the init script with a valid Dataloop (Outlyer) Agent fingerprint as discussed above.",
            "title": "Self-hosted statsite"
        },
        {
            "location": "/endpoints/self-hosted_statsite/#self-hosted-statsite",
            "text": "Statsite is an awesome alternative to the Etsy StatsD server implementation. Install as per the instructions here:  https://github.com/armon/statsite  Then create a  statsite.conf  with the following configuration:  [statsite]\nbind_address = 0.0.0.0\ntcp_port = 8125\nudp_port = 8125\nlog_level = INFO\nflush_interval = 10\ntimer_eps = 0.01\nset_eps = 0.02\nstream_cmd = python sinks/graphite.py graphite.dataloop.io 2003 fingerprint.\ninput_counter = numStats\nextended_counters = true  The important line here is the stream_cmd setting. You may need to update the location of the graphite.py file. The last part of the line is the metric prefix that is sent to Outlyer. Usually people install a Dataloop (Outlyer) Agent onto their Statsite server so they can use the fingerprint string from  /etc/dataloop/agent.finger . Remember to append a fullstop to the end of the fingerprint.  An example of a good stream_cmd line is:  stream_cmd = python sinks/graphite.py graphite.dataloop.io 2003 529b7fc2-2de5-4525-983a-184c8f43c085.",
            "title": "Self-Hosted Statsite"
        },
        {
            "location": "/endpoints/self-hosted_statsite/#troubleshooting",
            "text": "The only reliable way I have found to troubleshoot this is to open up netcat to listen on a new port, then temporarily change statsite.conf to send metrics to it.  For example open a new tab and run netcat to listen on port 2004  # nc -l 2004  Now change the stream_cmd to point to port 2004 localhost  stream_cmd = python sinks/graphite.py localhost 2004 529b7fc2-2de5-4525-983a-184c8f43c085.  Start statsite and wait for 10 seconds. Your netcat listening tab will show you the metrics being sent. An example of some that look correct:  529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.sum 57619.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.sum_sq 571869.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.mean 9.741167 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.lower 1.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.upper 10.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.count 5915 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.stdev 1.338330 1436185723  Here we can see the fingerprint first, then the dot seperator between the metric paths followed by the value and timestamp. If you have a malformed  fingerprint.metric.path  then metrics won't display in Outlyer.  Once you have confirmed it's working against localhost port 2004 you can flip it back to sending to Outlyer. Metrics will appear under the Dataloop (Outlyer) Agent fingerprint that you used.  Also, be careful about the time on your Statsite server. Installing NTP should help with this. Outlyer will display whatever values against the timestamp you send in, so if your timestamps are wrong strangeness abounds.",
            "title": "Troubleshooting"
        },
        {
            "location": "/endpoints/self-hosted_statsite/#sending-to-graphite-outlyer",
            "text": "Sometimes people want to keep their local Graphite server running in parallel. You can do that quite easily with Statsite by changing the  sink/graphite.py  that they provide to a modified version.  Grab the modified one from here:  gist  You'll need to modify the  self.fingerprint  variable in the init script with a valid Dataloop (Outlyer) Agent fingerprint as discussed above.",
            "title": "Sending to Graphite + Outlyer"
        },
        {
            "location": "/endpoints/statsd_clients/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \u201cdataloop agent\u201d, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nStatsD Clients\n\u00b6\n\n\nStatsD enables developers to instrument server side code and send metrics to Outlyer for visualisation and alerting. Data is sent using UDP so network issues will never slow your application down.\n\n\nThings you need to get started\n\u00b6\n\n\n\n\nConnection details for a StatsD server\n\n\nA StatsD client library\n\n\n\n\nBasic one-time setup\n\u00b6\n\n\nEach accounts gets a docker container hosting a StatsD server. \nDetails for hosted StatsD can be found here\n. Alternatively, you can setup your own StatsD server following our \nconfig guide\n.\n\n\nOnce you have a server setup find an appropriate StatsD Client for the language you are using.\n\n\nFor our example we'll use Python and the \npystatsd\n library. Full documentation can be found \nhere\n\n\nAll StatsD client libraries will need to be configured with a StatsD server and port to connect to. In our example we've spun up a StatsD server at demo-statsd.dataloop.io on port 8125 udp.\n\n\nConfiguration within the app is as simple as adding:\n\n\nfrom statsd import StatsClient\n\nstatsd = StatsClient(host='demo=statsd.dataloop.io',\n                     port=8125,\n                     prefix=None,\n                     maxudpsize=512)\n\n\n\n\n\nThat's pretty much it. You can now use the statsd object in your code to start sending metrics.",
            "title": "Statsd clients"
        },
        {
            "location": "/endpoints/statsd_clients/#statsd-clients",
            "text": "StatsD enables developers to instrument server side code and send metrics to Outlyer for visualisation and alerting. Data is sent using UDP so network issues will never slow your application down.",
            "title": "StatsD Clients"
        },
        {
            "location": "/endpoints/statsd_clients/#things-you-need-to-get-started",
            "text": "Connection details for a StatsD server  A StatsD client library",
            "title": "Things you need to get started"
        },
        {
            "location": "/endpoints/statsd_clients/#basic-one-time-setup",
            "text": "Each accounts gets a docker container hosting a StatsD server.  Details for hosted StatsD can be found here . Alternatively, you can setup your own StatsD server following our  config guide .  Once you have a server setup find an appropriate StatsD Client for the language you are using.  For our example we'll use Python and the  pystatsd  library. Full documentation can be found  here  All StatsD client libraries will need to be configured with a StatsD server and port to connect to. In our example we've spun up a StatsD server at demo-statsd.dataloop.io on port 8125 udp.  Configuration within the app is as simple as adding:  from statsd import StatsClient\n\nstatsd = StatsClient(host='demo=statsd.dataloop.io',\n                     port=8125,\n                     prefix=None,\n                     maxudpsize=512)  That's pretty much it. You can now use the statsd object in your code to start sending metrics.",
            "title": "Basic one-time setup"
        },
        {
            "location": "/integrations/thirdparty/aws/",
            "text": "Amazon Web Services\n\u00b6\n\n\nSome of the Outlyer plugins need an \nAWS Access Key ID\n and \nSecret Access Key\n\nin order to extract metrics from the CloudWatch API.\n\n\nTo generate them, use the following instructions:\n\n\n\n\nTo get started, open the \nAWS Management Console\n\n\nClick the IAM tab.\n\n\nClick the Create a New Group of Users button.\n\n\nEnter a Group Name called Outlyer.\n\n\nSelect the Read Only Access Policy Template then click Continue.\n\n\nClick the Create New Users tab.\n\n\nEnter a new User Name called Outlyer and click Continue and then Finish.\n\n\nClick Show User Security Credentials.\n\n\nCopy and paste your \nAccess Key Id\n and the \nSecret Access Key\n somewhere safe.",
            "title": "Importing metrics from Amazon CloudWatch"
        },
        {
            "location": "/integrations/thirdparty/aws/#amazon-web-services",
            "text": "Some of the Outlyer plugins need an  AWS Access Key ID  and  Secret Access Key \nin order to extract metrics from the CloudWatch API.  To generate them, use the following instructions:   To get started, open the  AWS Management Console  Click the IAM tab.  Click the Create a New Group of Users button.  Enter a Group Name called Outlyer.  Select the Read Only Access Policy Template then click Continue.  Click the Create New Users tab.  Enter a new User Name called Outlyer and click Continue and then Finish.  Click Show User Security Credentials.  Copy and paste your  Access Key Id  and the  Secret Access Key  somewhere safe.",
            "title": "Amazon Web Services"
        },
        {
            "location": "/dashboards/grafana/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nGrafana\n\u00b6\n\n\nGrafana 3.0 Plugin\n\u00b6\n\n\nYou can add a Dataloop (Outlyer) Agent as a datasource in Grafana by following these instructions. This is especially useful when used with a StatsD server. Install a Dataloop Dataloop (Outlyer) Agent on the StatsD server and use the fingerprint to view all of your StatsD metrics in Grafana.\n\n\n\n\nInstall Grafana 3.0\n\n\n\n\nhttp://docs.grafana.org/installation/\n\n\n\n\nAdd the Outlyer data source\n\n\n\n\n#> cd /var/lib/grafana/plugins\n#> git clone https://github.com/dataloop/dalmatinerdb-datasource dalmatinerdb\n\n\n\n\n\n\n\nRestart the grafana-server service\n\n\n\n\n#> sudo service grafana-server restart\n\n\n\n\n\n\n\nCreate an API token\n\n\n\n\nLogin to Outlyer and click account settings in the top right corner. Then generate an API token. You may want to create a service user account to restrict access to certain metrics in Grafana. \n\n\n\n\nCreate a new Data source in Grafana as per below\n\n\n\n\n\n\n* Name: Dataloop\n* Type: DalmatinerDB\n* URL: https://grafana.dataloop.io\n* Access: proxy\n* Auth Token: Ticked, and use the API token from step 4\n\n\n\n\n\nNote\n: Wildcards in metric paths only currently work within a single fingerprint. This is not a problem for dimensional data (coming from Prometheus plugins) but is required for StatsD. Therefore to use wildcards in metric paths you must always specify a \ndl:source\n in the \nWHERE\n clause.",
            "title": "Exporting data to Grafana"
        },
        {
            "location": "/dashboards/grafana/#grafana",
            "text": "",
            "title": "Grafana"
        },
        {
            "location": "/dashboards/grafana/#grafana-30-plugin",
            "text": "You can add a Dataloop (Outlyer) Agent as a datasource in Grafana by following these instructions. This is especially useful when used with a StatsD server. Install a Dataloop Dataloop (Outlyer) Agent on the StatsD server and use the fingerprint to view all of your StatsD metrics in Grafana.   Install Grafana 3.0   http://docs.grafana.org/installation/   Add the Outlyer data source   #> cd /var/lib/grafana/plugins\n#> git clone https://github.com/dataloop/dalmatinerdb-datasource dalmatinerdb   Restart the grafana-server service   #> sudo service grafana-server restart   Create an API token   Login to Outlyer and click account settings in the top right corner. Then generate an API token. You may want to create a service user account to restrict access to certain metrics in Grafana.    Create a new Data source in Grafana as per below    * Name: Dataloop\n* Type: DalmatinerDB\n* URL: https://grafana.dataloop.io\n* Access: proxy\n* Auth Token: Ticked, and use the API token from step 4  Note : Wildcards in metric paths only currently work within a single fingerprint. This is not a problem for dimensional data (coming from Prometheus plugins) but is required for StatsD. Therefore to use wildcards in metric paths you must always specify a  dl:source  in the  WHERE  clause.",
            "title": "Grafana 3.0 Plugin"
        },
        {
            "location": "/alerting/webhook/",
            "text": "Note\n\n\nWe have recently rebranded and changed our name from Dataloop.IO to Outlyer. Our agent is still called \ndataloop agent\n, and relevant code reflects the old name (Dataloop) as well. Thank you for your patience as we update everything.\n\n\n\n\nWebhook\n\u00b6\n\n\nWhen adding an action to a rule you can select the webhook option. This is a great way to connect your alerts into other systems like Slack or iFTTT (if this then that).\n\n\nTo configure a webhook simply change the URL field and hit test. By default we send the payload as JSON but you can change the drop down to send encoded in the URL if the service you are sending to prefers that.\n\n\nYou can add additional fields to the payload in the 'Extra Payload' box. This can also be used to override any of the current fields that we send.\n\n\nFinally, click the test button to verify your webhook is configured correctly.\n\n\nExample delivery\n\u00b6\n\n\nUser-Agent: Dataloop\nContent-Type: application/json\nX-Dataloop-Event: alert-webhook\n\n{\n  \"event\": \"alert\",\n  \"rule\": \"All Sytems\",\n  \"account\": \"noreply@outlyer.com\",\n  \"triggers\": [\n    {\n      \"criteria\": \"hosts are reporting down\",\n      \"sources\": [\n        {\n          \"name\": \"dataloop\",\n          \"tags\": [ \"all\" ],\n          \"timestamp\": \"Wed Dec 09 2015 15:21:20 GMT+0000 (GMT)\"\n        }\n    }\n  ],\n  \"text\": \"a formatted summary of the payload\"\n}\n\n\n\n\n\nDetails\n\u00b6\n\n\n\n\n\n\nevent: [\"alert\" | \"recovered\"]\n\n\n\n\n\n\ncriteria: [\"hosts are reporting down\" | \"check \n is failing\" | \"\n above threshold of \n\"]\n\n\n\n\n\n\ntext: will be picked up automatically by slack\n\n\n\n\n\n\nno triggers details when event is 'recovered'",
            "title": "Sending alerts to third-party services (webhooks)"
        },
        {
            "location": "/alerting/webhook/#webhook",
            "text": "When adding an action to a rule you can select the webhook option. This is a great way to connect your alerts into other systems like Slack or iFTTT (if this then that).  To configure a webhook simply change the URL field and hit test. By default we send the payload as JSON but you can change the drop down to send encoded in the URL if the service you are sending to prefers that.  You can add additional fields to the payload in the 'Extra Payload' box. This can also be used to override any of the current fields that we send.  Finally, click the test button to verify your webhook is configured correctly.",
            "title": "Webhook"
        },
        {
            "location": "/alerting/webhook/#example-delivery",
            "text": "User-Agent: Dataloop\nContent-Type: application/json\nX-Dataloop-Event: alert-webhook\n\n{\n  \"event\": \"alert\",\n  \"rule\": \"All Sytems\",\n  \"account\": \"noreply@outlyer.com\",\n  \"triggers\": [\n    {\n      \"criteria\": \"hosts are reporting down\",\n      \"sources\": [\n        {\n          \"name\": \"dataloop\",\n          \"tags\": [ \"all\" ],\n          \"timestamp\": \"Wed Dec 09 2015 15:21:20 GMT+0000 (GMT)\"\n        }\n    }\n  ],\n  \"text\": \"a formatted summary of the payload\"\n}",
            "title": "Example delivery"
        },
        {
            "location": "/alerting/webhook/#details",
            "text": "event: [\"alert\" | \"recovered\"]    criteria: [\"hosts are reporting down\" | \"check   is failing\" | \"  above threshold of  \"]    text: will be picked up automatically by slack    no triggers details when event is 'recovered'",
            "title": "Details"
        },
        {
            "location": "/endpoints/",
            "text": "Metric Collection Endpoints\n\u00b6\n\n\nBesides running plugins, Dataloop supports a number of additional ways to\ncollect metrics from your system. Generally these are network endpoints that\nlisten for connections and then forward the metrics on to Dataloop. Some of\nthese are hosted by Dataloop; others can be hosted inside your own network.\n\n\nGraphite, CollectD & InfluxDB\n\u00b6\n\n\nInfluxDB\n\n\nCollectD\n\n\nGraphite\n  \n\n\nStatsD\n\u00b6\n\n\nStatsD\n\n\nHosted StatsD Server\n\n\nSelf-Hosted StatsD\n\n\nSelf-Hosted Statsite\n\n\nStatsD Clients",
            "title": "Collecting metrics from custom endpoints"
        },
        {
            "location": "/endpoints/#metric-collection-endpoints",
            "text": "Besides running plugins, Dataloop supports a number of additional ways to\ncollect metrics from your system. Generally these are network endpoints that\nlisten for connections and then forward the metrics on to Dataloop. Some of\nthese are hosted by Dataloop; others can be hosted inside your own network.",
            "title": "Metric Collection Endpoints"
        },
        {
            "location": "/endpoints/#graphite-collectd-influxdb",
            "text": "InfluxDB  CollectD  Graphite",
            "title": "Graphite, CollectD &amp; InfluxDB"
        },
        {
            "location": "/endpoints/#statsd",
            "text": "StatsD  Hosted StatsD Server  Self-Hosted StatsD  Self-Hosted Statsite  StatsD Clients",
            "title": "StatsD"
        },
        {
            "location": "/getting_started/command_line_utility/",
            "text": "Command Line Utility (DLCLI)\n\u00b6\n\n\nUp-to-date installation instructions can be found here:\n\n\nhttps://github.com/dataloop/dlcli",
            "title": "Using the command line utility (dlcli)"
        },
        {
            "location": "/getting_started/command_line_utility/#command-line-utility-dlcli",
            "text": "Up-to-date installation instructions can be found here:  https://github.com/dataloop/dlcli",
            "title": "Command Line Utility (DLCLI)"
        },
        {
            "location": "/getting_started/API/",
            "text": "API Documentation\n\u00b6\n\n\nYou can view our API docs, hosted with postman here:\n\n\nAPI Docs",
            "title": "Using the REST API"
        },
        {
            "location": "/getting_started/API/#api-documentation",
            "text": "You can view our API docs, hosted with postman here:  API Docs",
            "title": "API Documentation"
        },
        {
            "location": "/getting_started/getting_help/",
            "text": "Getting Help\n\u00b6\n\n\nLogging a Ticket\n\u00b6\n\n\nFor long running issues we recommend that you \nlog a support ticket\n via Zendesk web interface so that you can keep track of the status. We try hard to be extremely responsive to tickets.\n\n\n\n\nChat Support\n\u00b6\n\n\nFor quick questions or help troubleshooting issues in real time please visit our Slack channel at \nhttps://slack.outlyer.com\n. There's usually somebody online from Outlyer to help out. Feel free to ask us any kind of questions related to monitoring.\n\n\n\n\nE-Mail Support\n\u00b6\n\n\nIf you're more comfortable firing off a quick email with a question then send them to \nsupport[at]outlyer.com\n This is a group mailbox that our support team looks at and you'll get a reply back as quick as humanly possible. \n\n\nFeel free to send us any type of enquiry to these support channels: product, bugs, billing, security, or any question at all. We're always happy to help.",
            "title": "Getting help"
        },
        {
            "location": "/getting_started/getting_help/#getting-help",
            "text": "",
            "title": "Getting Help"
        },
        {
            "location": "/getting_started/getting_help/#logging-a-ticket",
            "text": "For long running issues we recommend that you  log a support ticket  via Zendesk web interface so that you can keep track of the status. We try hard to be extremely responsive to tickets.",
            "title": "Logging a Ticket"
        },
        {
            "location": "/getting_started/getting_help/#chat-support",
            "text": "For quick questions or help troubleshooting issues in real time please visit our Slack channel at  https://slack.outlyer.com . There's usually somebody online from Outlyer to help out. Feel free to ask us any kind of questions related to monitoring.",
            "title": "Chat Support"
        },
        {
            "location": "/getting_started/getting_help/#e-mail-support",
            "text": "If you're more comfortable firing off a quick email with a question then send them to  support[at]outlyer.com  This is a group mailbox that our support team looks at and you'll get a reply back as quick as humanly possible.   Feel free to send us any type of enquiry to these support channels: product, bugs, billing, security, or any question at all. We're always happy to help.",
            "title": "E-Mail Support"
        },
        {
            "location": "/troubleshooting/har_archive/",
            "text": "Generating an HTTP Archive (HAR) file\n\u00b6\n\n\nIf you report problems with a page on our site, we may ask you to send us a HAR file for debugging. HAR stands for the HTTP Archive file format. A HAR file logs all of your browser's interactions with our site. It can help us to diagnose and resolve certain types of issues:\n\n\n\n\nPerformance: slow page load, timeout when performing certain task\n\n\nPage rendering: incorrect page format, missing information\nHow you generate a HAR file depends on the browser you are using. Please choose your browser from the list below. If you do not see your browser listed, please contact us for assistance.\n\n\n\n\nChrome\n\u00b6\n\n\nGoogle Chrome includes the Developer Tools, which can generate HAR files.\n\n\n\n\nBring up the developer tools by clicking the Chrome menu in the top right corner of your browser window, then selecting More Tools > Developer Tools.\nNavigate to the Network tab.\nCheck Disable Cache to force all content to be loaded from the server.\nRefresh the page to start capturing the traffic from the browser to the server.\nComplete the steps that trigger or demonstrate your issue. For example, if a certain dashboard is slow, click on the dashboard and wait for it to completely load.\nRight-click in the DevTool window and select Save as HAR with content.\n\n\nFirefox\n\u00b6\n\n\n\n\nBring up the Developer Tools by clicking the Firefox Menu button, then selecting Developer > Network.\nRefresh the page to start capturing the traffic between the browser to the server.\nComplete the steps that trigger or demonstrate your issue. For example, if a certain dashboard is slow, click on the dashboard and wait for it to completely load.\nRight-click in the Development Tool window and select Save All As HAR.\n\n\nInternet Explorer\n\u00b6\n\n\nInternet Explorer is not a browser supported for use with Outlyer.",
            "title": "Generating an HTTP Archive File for support"
        },
        {
            "location": "/troubleshooting/har_archive/#generating-an-http-archive-har-file",
            "text": "If you report problems with a page on our site, we may ask you to send us a HAR file for debugging. HAR stands for the HTTP Archive file format. A HAR file logs all of your browser's interactions with our site. It can help us to diagnose and resolve certain types of issues:   Performance: slow page load, timeout when performing certain task  Page rendering: incorrect page format, missing information\nHow you generate a HAR file depends on the browser you are using. Please choose your browser from the list below. If you do not see your browser listed, please contact us for assistance.",
            "title": "Generating an HTTP Archive (HAR) file"
        },
        {
            "location": "/troubleshooting/har_archive/#chrome",
            "text": "Google Chrome includes the Developer Tools, which can generate HAR files.   Bring up the developer tools by clicking the Chrome menu in the top right corner of your browser window, then selecting More Tools > Developer Tools.\nNavigate to the Network tab.\nCheck Disable Cache to force all content to be loaded from the server.\nRefresh the page to start capturing the traffic from the browser to the server.\nComplete the steps that trigger or demonstrate your issue. For example, if a certain dashboard is slow, click on the dashboard and wait for it to completely load.\nRight-click in the DevTool window and select Save as HAR with content.",
            "title": "Chrome"
        },
        {
            "location": "/troubleshooting/har_archive/#firefox",
            "text": "Bring up the Developer Tools by clicking the Firefox Menu button, then selecting Developer > Network.\nRefresh the page to start capturing the traffic between the browser to the server.\nComplete the steps that trigger or demonstrate your issue. For example, if a certain dashboard is slow, click on the dashboard and wait for it to completely load.\nRight-click in the Development Tool window and select Save All As HAR.",
            "title": "Firefox"
        },
        {
            "location": "/troubleshooting/har_archive/#internet-explorer",
            "text": "Internet Explorer is not a browser supported for use with Outlyer.",
            "title": "Internet Explorer"
        },
        {
            "location": "/troubleshooting/expired_gpg_key/",
            "text": "Updating an Expired GPG Key\n\u00b6\n\n\nWe set our GPG signing keys to be valid for only a few years at a time.\n\n\nIf you exprience an expired key on your servers you will need to update it.\n\n\nApt\n\u00b6\n\n\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 113E2B8D",
            "title": "Updating an expired GPG signing key"
        },
        {
            "location": "/troubleshooting/expired_gpg_key/#updating-an-expired-gpg-key",
            "text": "We set our GPG signing keys to be valid for only a few years at a time.  If you exprience an expired key on your servers you will need to update it.",
            "title": "Updating an Expired GPG Key"
        },
        {
            "location": "/troubleshooting/expired_gpg_key/#apt",
            "text": "sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 113E2B8D",
            "title": "Apt"
        }
    ]
}