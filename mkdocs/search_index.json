{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to Dataloop.IO Support Knowledgebase\n\n\nHere you will find a body of knowledge to get your monitoring started with Dataloop.IO\n\n\nStart with the \nInstallation\n pages to get Dataloop.IO agents installed on your servers.\n\n\nHead on over to our \nZendesk ticketing system\n to raise a support ticket with the Dataloop Team, or send an support request to \nsupport@dataloop.io\n\n\nSetup and Configuration\n\n\nGetting Started\n\n\nDataloop Agent\n\n\nGraphite, StatsD & InfluxDB\n\n\nNagios and Prometheus\n\n\nAlerting\n\n\nDashboards\n\n\nOrganizational Account Model\n  \n\n\nIntegrations\n\n\nThird Party\n\n\nWebhook\n\n\nTroubleshooting\n\n\nGenerating an HTTP Archive (HAR) file\n\n\nLatest Pages",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-dataloopio-support-knowledgebase",
            "text": "Here you will find a body of knowledge to get your monitoring started with Dataloop.IO  Start with the  Installation  pages to get Dataloop.IO agents installed on your servers.  Head on over to our  Zendesk ticketing system  to raise a support ticket with the Dataloop Team, or send an support request to  support@dataloop.io",
            "title": "Welcome to Dataloop.IO Support Knowledgebase"
        },
        {
            "location": "/#setup-and-configuration",
            "text": "Getting Started  Dataloop Agent  Graphite, StatsD & InfluxDB  Nagios and Prometheus  Alerting  Dashboards  Organizational Account Model",
            "title": "Setup and Configuration"
        },
        {
            "location": "/#integrations",
            "text": "Third Party  Webhook",
            "title": "Integrations"
        },
        {
            "location": "/#troubleshooting",
            "text": "Generating an HTTP Archive (HAR) file",
            "title": "Troubleshooting"
        },
        {
            "location": "/#latest-pages",
            "text": "",
            "title": "Latest Pages"
        },
        {
            "location": "/account_model/backup_restore/",
            "text": "Backup and Restore\n\n\nYou can backup and restore entire organizations, accounts or objects with the Dataloop Command Line Tool and a Linux backup server.\n\n\n\n\nInstall the Dataloop Command Line Utility\n\n\n\n\npip install dlcli\n\n\n\n\n\n\nCreate the file /etc/dataloop/dlcli.yaml and enter your details\n\n\n\n\n---\naccount: default\nkey: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\norg: acme-ltd\nurl: https://app.dataloop.io/api/v1\n\n\n\n\n\n\nCreate a directory for your backups\n\n\n\n\nmkdir /backup\n\n\n\n\n\n\n\n\nCreate a private Git repo called 'dataloop', add deploy keys for the root user with push access and optionally add a webhook to notify a Slack channel. Then clone into /backup so you have /backup/dataloop as your local copy.\n\n\n\n\n\n\nCreate a file called \ndataloop-backup.sh\n in \n/usr/local/bin\n with the following content\n\n\n\n\n\n\n#!/usr/bin/env bash\ncd /backup/dataloop\nrm -fr /backup/dataloop/*\n/usr/local/bin/dlcli --backupdir /backup/dataloop --settingsfile /etc/dataloop/dlcli.yaml backup org acme-ltd\ngit add .\ngit commit -m 'backup of acme-ltd org'\ngit push\n\n\n\n\nChange the \nacme-ltd\n to whatever your \norg\n is called.\n\n\n\n\nSetup a cron to run the backup every 10 mins\n\n\n\n\n*/10 * * * * /usr/local/bin/dataloop-backup.sh > /var/log/backup.log 2>&1",
            "title": "Backup restore"
        },
        {
            "location": "/account_model/backup_restore/#backup-and-restore",
            "text": "You can backup and restore entire organizations, accounts or objects with the Dataloop Command Line Tool and a Linux backup server.   Install the Dataloop Command Line Utility   pip install dlcli   Create the file /etc/dataloop/dlcli.yaml and enter your details   ---\naccount: default\nkey: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\norg: acme-ltd\nurl: https://app.dataloop.io/api/v1   Create a directory for your backups   mkdir /backup    Create a private Git repo called 'dataloop', add deploy keys for the root user with push access and optionally add a webhook to notify a Slack channel. Then clone into /backup so you have /backup/dataloop as your local copy.    Create a file called  dataloop-backup.sh  in  /usr/local/bin  with the following content    #!/usr/bin/env bash\ncd /backup/dataloop\nrm -fr /backup/dataloop/*\n/usr/local/bin/dlcli --backupdir /backup/dataloop --settingsfile /etc/dataloop/dlcli.yaml backup org acme-ltd\ngit add .\ngit commit -m 'backup of acme-ltd org'\ngit push  Change the  acme-ltd  to whatever your  org  is called.   Setup a cron to run the backup every 10 mins   */10 * * * * /usr/local/bin/dataloop-backup.sh > /var/log/backup.log 2>&1",
            "title": "Backup and Restore"
        },
        {
            "location": "/account_model/overview/",
            "text": "Overview\n\n\nThe Dataloop account model is designed around the concept of organizations and accounts. Users can exist in multiple organizations and accounts and can switch between them by using the dropdowns in the global header. \n\n\nOrganizations\n\n\nAn organization typically maps directly to your company name. On a new sign-up users are prompted to provide an organization name.\n\n\nFor example your organization might be called \nAcme-Inc\n.\n\n\nOnce an organization is created additional members can be invited in.\n\n\nAccounts\n\n\nAccounts can be created within an organization. For smaller companies these typically map to environments. At scale they can map to services or product service groupings. Accounts allow you to segment your monitoring setup. For MSP's these can even map to customer company names.\n\n\nFor example your accounts under the Acme-Inc organization might be called \nTest\n, \nStaging\n and \nProduction\n. \n\n\nOrganization Admins\n\n\nThe first user to sign up and create an organization automatically becomes an \norganization admin\n. Additional organization admins can be invited via the user icon with the cog on the organization overview page sidebar.\n\n\nOrganization admins have full access to every account. They can create, rename and delete accounts in addition to managing the members and roles within each account.\n\n\nWhen creating new accounts an organization admin should also enter the account and invite initial account members. Account member setup can be delegated by setting the role to admin.\n\n\nAccount Members\n\n\nAccount members can only see accounts that they are a member of. Additional account members can be invited into an account by either organization admins or account admins via the user icon within each account.\n\n\nMembers can be given the following roles:\n\n\n\n\nAdmin - Full account access, invite additional members, assign the admin role\n\n\nMembers - Full account access\n\n\nView Only - View only account access (coming soon)\n\n\n\n\nInvites\n\n\nInvitations are valid for 7 days and can be resent from the member management area. Invitations can also be cancelled from this area.",
            "title": "Overview"
        },
        {
            "location": "/account_model/overview/#overview",
            "text": "The Dataloop account model is designed around the concept of organizations and accounts. Users can exist in multiple organizations and accounts and can switch between them by using the dropdowns in the global header.",
            "title": "Overview"
        },
        {
            "location": "/account_model/overview/#organizations",
            "text": "An organization typically maps directly to your company name. On a new sign-up users are prompted to provide an organization name.  For example your organization might be called  Acme-Inc .  Once an organization is created additional members can be invited in.",
            "title": "Organizations"
        },
        {
            "location": "/account_model/overview/#accounts",
            "text": "Accounts can be created within an organization. For smaller companies these typically map to environments. At scale they can map to services or product service groupings. Accounts allow you to segment your monitoring setup. For MSP's these can even map to customer company names.  For example your accounts under the Acme-Inc organization might be called  Test ,  Staging  and  Production .",
            "title": "Accounts"
        },
        {
            "location": "/account_model/overview/#organization-admins",
            "text": "The first user to sign up and create an organization automatically becomes an  organization admin . Additional organization admins can be invited via the user icon with the cog on the organization overview page sidebar.  Organization admins have full access to every account. They can create, rename and delete accounts in addition to managing the members and roles within each account.  When creating new accounts an organization admin should also enter the account and invite initial account members. Account member setup can be delegated by setting the role to admin.",
            "title": "Organization Admins"
        },
        {
            "location": "/account_model/overview/#account-members",
            "text": "Account members can only see accounts that they are a member of. Additional account members can be invited into an account by either organization admins or account admins via the user icon within each account.  Members can be given the following roles:   Admin - Full account access, invite additional members, assign the admin role  Members - Full account access  View Only - View only account access (coming soon)",
            "title": "Account Members"
        },
        {
            "location": "/account_model/overview/#invites",
            "text": "Invitations are valid for 7 days and can be resent from the member management area. Invitations can also be cancelled from this area.",
            "title": "Invites"
        },
        {
            "location": "/account_model/private_packs/",
            "text": "Private Packs\n\n\nPrivate packs can be used to keep your custom plugins, dashboards and alert rules locked together as a single versioned entity within Dataloop.\n\n\nThey are designed to be small containers of monitoring configuration that can be shared around between accounts.\n\n\nCurrently private packs can only be managed with the Dataloop command line utility (dlcli). We will be adding private pack management to the UI soon\n\n\nTerminology:\n\n\n\n\nPack: The installed entity\n\n\nTemplate: The entity that can be installed from, that creates a pack in the account\n\n\n\n\nCreate a new pack\n\n\nIn our example we'll create a pack called microservice1. Names must be unique and friendly, ideally something that matches the service the pack will monitor. When used with autodiscovery the pack name is also used to tag agents.\n\n\ndlcli create pack microservice1\n\n\n\n\nThis should create a new directory on disk with the following structure:\n\n\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 dashboards\n\u2502   \u2514\u2500\u2500 microservice1.yaml\n\u251c\u2500\u2500 package.yaml\n\u251c\u2500\u2500 plugins\n\u2502   \u2514\u2500\u2500 microservice1.py\n\u2514\u2500\u2500 rules\n    \u2514\u2500\u2500 microservice1.yaml\n\n\n\n\nYou can now start to populate this structure with the content created in the Dataloop UI by copying and pasting the plugin content and pressing export to yaml on the dashboards and rules pages. Or alternatively by using the dlcli pull commands. You may also want to update the pack metadata file package.yaml with additional information.\n\n\nOnce you have finished editing your pack locally it's time to upload it into Dataloop as a Template that can be installed.\n\n\nTo upload the pack as a Template in Dataloop run the following command from inside the pack directory:\n\n\ndlcli push template microservice1 .\n\n\n\n\nOnce the pack is pushed you can check it is there by running:\n\n\ndlcli get templates\n\n\n\n\nAssuming you are happy to install the pack you can now run\n\n\ndlcli install template microservice1\n\n\n\n\nTo uninstall the pack at a later date simply run\n\n\ndlcli rm pack microservice1\n\n\n\n\nTo delete the template run:\n\n\ndlcli rm template microservice1\n\n\n\n\nNote\n: Installing a template results in a tag being created that matches the pack name. You can therefore tag your agents with the same name as the pack and on install a link will automatically be created.\n\n\nTroubleshooting\n\n\nYou can show the contents of a template by running:\n\n\ndlcli get template microservice1\n\n\n\n\nWhere \nmicroservice1\n is the name of the template you wish to inspect",
            "title": "Private packs"
        },
        {
            "location": "/account_model/private_packs/#private-packs",
            "text": "Private packs can be used to keep your custom plugins, dashboards and alert rules locked together as a single versioned entity within Dataloop.  They are designed to be small containers of monitoring configuration that can be shared around between accounts.  Currently private packs can only be managed with the Dataloop command line utility (dlcli). We will be adding private pack management to the UI soon",
            "title": "Private Packs"
        },
        {
            "location": "/account_model/private_packs/#terminology",
            "text": "Pack: The installed entity  Template: The entity that can be installed from, that creates a pack in the account",
            "title": "Terminology:"
        },
        {
            "location": "/account_model/private_packs/#create-a-new-pack",
            "text": "In our example we'll create a pack called microservice1. Names must be unique and friendly, ideally something that matches the service the pack will monitor. When used with autodiscovery the pack name is also used to tag agents.  dlcli create pack microservice1  This should create a new directory on disk with the following structure:  .\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 dashboards\n\u2502   \u2514\u2500\u2500 microservice1.yaml\n\u251c\u2500\u2500 package.yaml\n\u251c\u2500\u2500 plugins\n\u2502   \u2514\u2500\u2500 microservice1.py\n\u2514\u2500\u2500 rules\n    \u2514\u2500\u2500 microservice1.yaml  You can now start to populate this structure with the content created in the Dataloop UI by copying and pasting the plugin content and pressing export to yaml on the dashboards and rules pages. Or alternatively by using the dlcli pull commands. You may also want to update the pack metadata file package.yaml with additional information.  Once you have finished editing your pack locally it's time to upload it into Dataloop as a Template that can be installed.  To upload the pack as a Template in Dataloop run the following command from inside the pack directory:  dlcli push template microservice1 .  Once the pack is pushed you can check it is there by running:  dlcli get templates  Assuming you are happy to install the pack you can now run  dlcli install template microservice1  To uninstall the pack at a later date simply run  dlcli rm pack microservice1  To delete the template run:  dlcli rm template microservice1  Note : Installing a template results in a tag being created that matches the pack name. You can therefore tag your agents with the same name as the pack and on install a link will automatically be created.",
            "title": "Create a new pack"
        },
        {
            "location": "/account_model/private_packs/#troubleshooting",
            "text": "You can show the contents of a template by running:  dlcli get template microservice1  Where  microservice1  is the name of the template you wish to inspect",
            "title": "Troubleshooting"
        },
        {
            "location": "/agent/command_line_interface/",
            "text": "Command Line Interface\n\n\nOn Linux you can simply type \ndataloop-agent\n or call directly from \n/usr/bin/dataloop-agent\n\n\nOn Windows you can run:\n\n\nC:\\Dataloop\\embedded\\bin\\python.exe C:\\Dataloop\\agent\\agent.py\n\n\nRun the \ndataloop-agent\n command with the \n-h\n argument to show the full help\n\n\n# dataloop-agent -h\nusage: dataloop-agent [-h] [--config PATH] -a KEY [-t START_TAGS] [-s]\n                      [--name NAME] [-v]\n                      {tags,agent} ...\n\n## DataLoop.io Agent\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config PATH         optional: path to config yaml file\n  -a KEY, --api-key KEY\n                        required: your agent api-key. This can be found in the\n                        app\n  -t START_TAGS, --tags START_TAGS\n                        optional: start agent with tags\n  -s, --solo            optional: run from a local plugin source\n  --name NAME           optional: set the agent name\n  -v, --version         show program's version number and exit\n\ncli:\n  {tags,agent}          CLI commands\n    tags                update agent tags\n    agent               update agent details\n\n\n\n\nYou can also pass -h into the options:\n\n\n# dataloop-agent tags -h\nusage: dataloop-agent tags [-h] {list,add,remove,clear} ...\n\noptional arguments:\n  -h, --help            show this help message and exit\n\ntags:\n  {list,add,remove,clear}\n                        tags commands\n    list                list this agents tags\n    add                 add tags to this agent\n    remove              remove tags from this agent\n    clear               clear all tags from this agent\n\n\n\n\nThe most common use for the CLI is for tagging either in automated scripts or in config management.\n\n\nAs an example, while the agent is running, you can list what tags it is a member of and then modify them:\n\n\n#> dataloop-agent tags list\nall\nriak\nprod\ntag1\n\n#> dataloop-agent tags remove tag1\n\n#> dataloop-agent tags list\nall\nriak\nprod\n\n#> dataloop-agent tags add tag2\n\n#> dataloop-agent tags list\nall\nriak\nprod\ntag2",
            "title": "Command line interface"
        },
        {
            "location": "/agent/command_line_interface/#command-line-interface",
            "text": "On Linux you can simply type  dataloop-agent  or call directly from  /usr/bin/dataloop-agent  On Windows you can run:  C:\\Dataloop\\embedded\\bin\\python.exe C:\\Dataloop\\agent\\agent.py  Run the  dataloop-agent  command with the  -h  argument to show the full help  # dataloop-agent -h\nusage: dataloop-agent [-h] [--config PATH] -a KEY [-t START_TAGS] [-s]\n                      [--name NAME] [-v]\n                      {tags,agent} ...\n\n## DataLoop.io Agent\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config PATH         optional: path to config yaml file\n  -a KEY, --api-key KEY\n                        required: your agent api-key. This can be found in the\n                        app\n  -t START_TAGS, --tags START_TAGS\n                        optional: start agent with tags\n  -s, --solo            optional: run from a local plugin source\n  --name NAME           optional: set the agent name\n  -v, --version         show program's version number and exit\n\ncli:\n  {tags,agent}          CLI commands\n    tags                update agent tags\n    agent               update agent details  You can also pass -h into the options:  # dataloop-agent tags -h\nusage: dataloop-agent tags [-h] {list,add,remove,clear} ...\n\noptional arguments:\n  -h, --help            show this help message and exit\n\ntags:\n  {list,add,remove,clear}\n                        tags commands\n    list                list this agents tags\n    add                 add tags to this agent\n    remove              remove tags from this agent\n    clear               clear all tags from this agent  The most common use for the CLI is for tagging either in automated scripts or in config management.  As an example, while the agent is running, you can list what tags it is a member of and then modify them:  #> dataloop-agent tags list\nall\nriak\nprod\ntag1\n\n#> dataloop-agent tags remove tag1\n\n#> dataloop-agent tags list\nall\nriak\nprod\n\n#> dataloop-agent tags add tag2\n\n#> dataloop-agent tags list\nall\nriak\nprod\ntag2",
            "title": "Command Line Interface"
        },
        {
            "location": "/agent/config_management/",
            "text": "Chef, Ansible, Puppet and Salt\n\n\nDataloop currently has modules for Chef, Ansible, Puppet and Salt to help with rolling out the agent.\n\n\nYou can find these in our public Github account:\n\n\nhttps://github.com/dataloop",
            "title": "Config management"
        },
        {
            "location": "/agent/config_management/#chef-ansible-puppet-and-salt",
            "text": "Dataloop currently has modules for Chef, Ansible, Puppet and Salt to help with rolling out the agent.  You can find these in our public Github account:  https://github.com/dataloop",
            "title": "Chef, Ansible, Puppet and Salt"
        },
        {
            "location": "/agent/configuration/",
            "text": "Configuration\n\n\nBy default the dataloop-agent will read \nagent.yaml\n for configuration from \n/etc/dataloop\n on Linux and \nc:\\dataloop\n on Windows.\n\n\nThe only mandatory configuration required in this file is the api-key.\n\n\nChanges will be picked up on dataloop-agent service restart.\n\n\n\n\nExample:\n\n\n\n\n---\n##\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo_mode: no\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n\n##  set tags to a comma separated list of tags applied to this agent\n#tags:\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:\n\n\n\n\nOn Linux there are also a couple of tweaks that can be made in \n/etc/default/dataloop-agent\n for Debian based distros and \n/etc/sysconfig/dataloop-agent\n for Redhat.\n\n\n\n\nExample:\n\n\n\n\n# Deregister node with dataloop when the agent stops?\n# Only 'no' will make this stop.\nDEREGISTER_ONSTOP=yes\nexport HTTP_PROXY=http://squid.box\n\n\n\n\nBy default the Dataloop agent will deregister on server and service shutdown. This is extremely useful when you have dynamically scaled environments e.g. Amazon ASG's.\n\n\nIf you have a more static infrastructure or want to perform the deregister at your orchestration later then set to no.\n\n\nYou can also specify \nHTTP_PROXY\n or \nHTTPS_PROXY\n if the agent needs to proxy out to the internet.\n\n\nOn Windows you can configure these as environment variables and the agent will connect out via a proxy.",
            "title": "Configuration"
        },
        {
            "location": "/agent/configuration/#configuration",
            "text": "By default the dataloop-agent will read  agent.yaml  for configuration from  /etc/dataloop  on Linux and  c:\\dataloop  on Windows.  The only mandatory configuration required in this file is the api-key.  Changes will be picked up on dataloop-agent service restart.   Example:   ---\n##\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo_mode: no\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n\n##  set tags to a comma separated list of tags applied to this agent\n#tags:\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:  On Linux there are also a couple of tweaks that can be made in  /etc/default/dataloop-agent  for Debian based distros and  /etc/sysconfig/dataloop-agent  for Redhat.   Example:   # Deregister node with dataloop when the agent stops?\n# Only 'no' will make this stop.\nDEREGISTER_ONSTOP=yes\nexport HTTP_PROXY=http://squid.box  By default the Dataloop agent will deregister on server and service shutdown. This is extremely useful when you have dynamically scaled environments e.g. Amazon ASG's.  If you have a more static infrastructure or want to perform the deregister at your orchestration later then set to no.  You can also specify  HTTP_PROXY  or  HTTPS_PROXY  if the agent needs to proxy out to the internet.  On Windows you can configure these as environment variables and the agent will connect out via a proxy.",
            "title": "Configuration"
        },
        {
            "location": "/agent/fingerprints/",
            "text": "Fingerprints\n\n\nWhen an Agent starts for the first time on a new server it creates an agent.finger file (in \n/etc/dataloop\n on Linux boxes and \nC:\\Dataloop\n on Windows). This is used to uniquely identify the Agent.\n\n\nDeleting the \nagent.finger\n file and restarting the service will cause the agent to generate a new identity and it will lose its old metrics.\n\n\nConversely, starting servers with known fingerprint files allows you to connect new boxes to existing metrics which is extremely useful for dynamically changing environments where configuration management tools are used.\n\n\n\n\nExample\n\n\n\n\nYou may have an environment made up of 6 different server roles.\n\nEach role is scaled up to 10 servers.\n\nYour deployment mechanism may mean that every server gets recycled every few days as features are released.\n\nYou may decide to store two fingerprints for each role in your configuration management system so that 2 / 10 of the nodes rejoin their old metrics.\n\nThe remaining 8 servers per role get a new identify each time.\n\nIn this way you can look back across many releases to determine what effects the changes have had on your service.",
            "title": "Fingerprints"
        },
        {
            "location": "/agent/fingerprints/#fingerprints",
            "text": "When an Agent starts for the first time on a new server it creates an agent.finger file (in  /etc/dataloop  on Linux boxes and  C:\\Dataloop  on Windows). This is used to uniquely identify the Agent.  Deleting the  agent.finger  file and restarting the service will cause the agent to generate a new identity and it will lose its old metrics.  Conversely, starting servers with known fingerprint files allows you to connect new boxes to existing metrics which is extremely useful for dynamically changing environments where configuration management tools are used.   Example   You may have an environment made up of 6 different server roles. \nEach role is scaled up to 10 servers. \nYour deployment mechanism may mean that every server gets recycled every few days as features are released. \nYou may decide to store two fingerprints for each role in your configuration management system so that 2 / 10 of the nodes rejoin their old metrics. \nThe remaining 8 servers per role get a new identify each time. \nIn this way you can look back across many releases to determine what effects the changes have had on your service.",
            "title": "Fingerprints"
        },
        {
            "location": "/agent/",
            "text": "Dataloop Agent\n\n\nInstallation Linux\n\n\nInstallation Windows\n\n\nInstallation Docker\n\n\nInstallation Raspberry Pi\n\n\nUninstall\n\n\nChef, Puppet, Ansible and Salt\n\n\nConfiguration\n\n\nSolo Mode\n\n\nCommand Line Interface\n\n\nFingerprints\n\n\nSupported Operating Systems",
            "title": "Home"
        },
        {
            "location": "/agent/#dataloop-agent",
            "text": "Installation Linux  Installation Windows  Installation Docker  Installation Raspberry Pi  Uninstall  Chef, Puppet, Ansible and Salt  Configuration  Solo Mode  Command Line Interface  Fingerprints  Supported Operating Systems",
            "title": "Dataloop Agent"
        },
        {
            "location": "/agent/installation_docker/",
            "text": "Installation on Docker\n\n\nYou can run a containerised agent in a docker container by running:\n\n\nDATALOOP_AGENT_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nDATALOOP_NAME=docker_container_name\nsudo docker run -d -e \"DATALOOP_AGENT_KEY=${DATALOOP_AGENT_KEY}\" \\\n-e \"DATALOOP_NAME=${DATALOOP_NAME}\" \\\n-p 8000:8000 \\\n-p 8080:8080 \\\n--volume=/:/rootfs:ro \\\n--volume=/var/run:/var/run:rw \\\n--volume=/sys:/sys:ro \\\n--volume=/var/lib/docker/:/var/lib/docker:ro \\\ndataloop/dataloop-docker:latest\n\n\n\n\nThis will download the container directly from the docker hub. For more info see \nhttps://github.com/dataloop/docker-alpine/dataloop-docker",
            "title": "Installation docker"
        },
        {
            "location": "/agent/installation_docker/#installation-on-docker",
            "text": "You can run a containerised agent in a docker container by running:  DATALOOP_AGENT_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nDATALOOP_NAME=docker_container_name\nsudo docker run -d -e \"DATALOOP_AGENT_KEY=${DATALOOP_AGENT_KEY}\" \\\n-e \"DATALOOP_NAME=${DATALOOP_NAME}\" \\\n-p 8000:8000 \\\n-p 8080:8080 \\\n--volume=/:/rootfs:ro \\\n--volume=/var/run:/var/run:rw \\\n--volume=/sys:/sys:ro \\\n--volume=/var/lib/docker/:/var/lib/docker:ro \\\ndataloop/dataloop-docker:latest  This will download the container directly from the docker hub. For more info see  https://github.com/dataloop/docker-alpine/dataloop-docker",
            "title": "Installation on Docker"
        },
        {
            "location": "/agent/installation_linux/",
            "text": "Installation on Linux\n\n\nDebian based Linux (Debian, Ubuntu etc)\n\n\n\n\nImport the apt repository gpg key\n\n\n\n\ncurl -s https://download.dataloop.io/pubkey.gpg | apt-key add -\n\n\n\n\n\n\nAdd the Dataloop apt repository\n\n\n\n\necho 'deb https://download.dataloop.io/deb/ stable main' > /etc/apt/sources.list.d/dataloop.list\n\n\n\n\n\n\nInstall the dataloop agent\n\n\n\n\nsudo apt-get update && sudo apt-get install dataloop-agent\n\n\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the agent\n\n\n\n\n\n\nsudo service dataloop-agent start\n\n\n\n\n\n\nSet the agent to run on reboot\n\n\n\n\nsudo update-rc.d dataloop-agent defaults\n\n\n\n\n\n\nRedhat based Linux (Redhat, Centos etc)\n\n\n\n\nAdd the Dataloop yum repository\n\n\n\n\nPaste the following into /etc/yum.repos.d/dataloop.repo\n\n\n[dataloop]\nname=Dataloop Agent\nbaseurl=https://download.dataloop.io/packages/stable/el$releasever/$basearch/\nenabled=1\ngpgcheck=0\n\n\n\n\n\n\nInstall the Datalooop Agent\n\n\n\n\nsudo yum install dataloop-agent\n\n\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the Dataloop Agent service\n\n\n\n\n\n\nsudo service dataloop-agent start\n\n\n\n\n\n\nSet the agent to run on reboot\n\n\n\n\nsudo chkconfig --level 345 dataloop-agent on\n\n\n\n\n\n\nSuse based Linux\n\n\n\n\nDownload the RPM\n\n\n\n\nwget https://download.dataloop.io/suse/x86_64/dataloop-agent_latest-suse-x86_64.rpm\n\n\n\n\n\n\nInstall it\n\n\n\n\nsudo rpm -i dataloop-agent-1.1.18-1.x86_64.rpm\n\n\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the Dataloop Agent service\n\n\n\n\n\n\nsudo systemctl start dataloop-agent\n\n\n\n\n\n\nSet the agent to run on reboot\n\n\n\n\nsudo systemctl enable dataloop-agent\n\n\n\n\n\n\nArchlinux based Linux (x86_64)\n\n\n\n\nDownload the PKG\n\n\n\n\nwget https://download.dataloop.io/archlinux/dataloop-agent_latest-archlinux-x86_64.pkg.tar.xz\n\n\n\n\n\n\nInstall it\n\n\n\n\npacman -U dataloop-agent-1.1.20-1-x86_64.pkg.tar.xz\n\n\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the Dataloop Agent service\n\n\n\n\n\n\nsudo systemctl start dataloop-agent\n\n\n\n\n\n\nSet the agent to run on reboot\n\n\n\n\nsudo systemctl enable dataloop-agent\n\n\n\n\nAlternatively you can make the package yourself by following the instructions at https://github.com/dataloop/dataloop-archlinux\n\n\n\n\nCurl sudo bash installer (only recommended on test machines)\n\n\nThis essentially automates the above.\n\n\n\n\nRun the following command \n\n\n\n\ncurl -s https://download.dataloop.io/setup.sh | sudo bash -s xxx\n\n\n\n\nWhere xxx is your API key that you would have been emailed on signup. Or you can log into the web interface and click 'Install New Agent' on the 'Setup Monitoring' page.",
            "title": "Installation linux"
        },
        {
            "location": "/agent/installation_linux/#installation-on-linux",
            "text": "",
            "title": "Installation on Linux"
        },
        {
            "location": "/agent/installation_linux/#debian-based-linux-debian-ubuntu-etc",
            "text": "Import the apt repository gpg key   curl -s https://download.dataloop.io/pubkey.gpg | apt-key add -   Add the Dataloop apt repository   echo 'deb https://download.dataloop.io/deb/ stable main' > /etc/apt/sources.list.d/dataloop.list   Install the dataloop agent   sudo apt-get update && sudo apt-get install dataloop-agent    Update the /etc/dataloop/agent.yaml file with your API key    Start the agent    sudo service dataloop-agent start   Set the agent to run on reboot   sudo update-rc.d dataloop-agent defaults",
            "title": "Debian based Linux (Debian, Ubuntu etc)"
        },
        {
            "location": "/agent/installation_linux/#redhat-based-linux-redhat-centos-etc",
            "text": "Add the Dataloop yum repository   Paste the following into /etc/yum.repos.d/dataloop.repo  [dataloop]\nname=Dataloop Agent\nbaseurl=https://download.dataloop.io/packages/stable/el$releasever/$basearch/\nenabled=1\ngpgcheck=0   Install the Datalooop Agent   sudo yum install dataloop-agent    Update the /etc/dataloop/agent.yaml file with your API key    Start the Dataloop Agent service    sudo service dataloop-agent start   Set the agent to run on reboot   sudo chkconfig --level 345 dataloop-agent on",
            "title": "Redhat based Linux (Redhat, Centos etc)"
        },
        {
            "location": "/agent/installation_linux/#suse-based-linux",
            "text": "Download the RPM   wget https://download.dataloop.io/suse/x86_64/dataloop-agent_latest-suse-x86_64.rpm   Install it   sudo rpm -i dataloop-agent-1.1.18-1.x86_64.rpm    Update the /etc/dataloop/agent.yaml file with your API key    Start the Dataloop Agent service    sudo systemctl start dataloop-agent   Set the agent to run on reboot   sudo systemctl enable dataloop-agent",
            "title": "Suse based Linux"
        },
        {
            "location": "/agent/installation_linux/#archlinux-based-linux-x86_64",
            "text": "Download the PKG   wget https://download.dataloop.io/archlinux/dataloop-agent_latest-archlinux-x86_64.pkg.tar.xz   Install it   pacman -U dataloop-agent-1.1.20-1-x86_64.pkg.tar.xz    Update the /etc/dataloop/agent.yaml file with your API key    Start the Dataloop Agent service    sudo systemctl start dataloop-agent   Set the agent to run on reboot   sudo systemctl enable dataloop-agent  Alternatively you can make the package yourself by following the instructions at https://github.com/dataloop/dataloop-archlinux",
            "title": "Archlinux based Linux (x86_64)"
        },
        {
            "location": "/agent/installation_linux/#curl-sudo-bash-installer-only-recommended-on-test-machines",
            "text": "This essentially automates the above.   Run the following command    curl -s https://download.dataloop.io/setup.sh | sudo bash -s xxx  Where xxx is your API key that you would have been emailed on signup. Or you can log into the web interface and click 'Install New Agent' on the 'Setup Monitoring' page.",
            "title": "Curl sudo bash installer (only recommended on test machines)"
        },
        {
            "location": "/agent/installation_raspberry_pi/",
            "text": "Installation on Raspberry Pi\n\n\nDebian based Linux (Raspbian Jessie only for now)\n\n\n\n\nImport the apt repository gpg key\n\n\n\n\ncurl -s https://download.dataloop.io/pubkey.gpg | apt-key add -\n\n\n\n\n\n\nAdd the Dataloop apt repository\n\n\n\n\necho 'deb https://download.dataloop.io/deb/ unstable main' > /etc/apt/sources.list.d/dataloop.list\n\n\n\n\n\n\nInstall the dataloop agent\n\n\n\n\nsudo apt-get update && sudo apt-get install dataloop-agent\n\n\n\n\n\n\n\n\nUpdate the /etc/dataloop/agent.yaml file with your API key\n\n\n\n\n\n\nStart the agent\n\n\n\n\n\n\nsudo systemctl start dataloop-agent\n\n\n\n\n\n\nSet the agent to run on reboot\n\n\n\n\nsudo systemctl enable dataloop-agent",
            "title": "Installation raspberry pi"
        },
        {
            "location": "/agent/installation_raspberry_pi/#installation-on-raspberry-pi",
            "text": "",
            "title": "Installation on Raspberry Pi"
        },
        {
            "location": "/agent/installation_raspberry_pi/#debian-based-linux-raspbian-jessie-only-for-now",
            "text": "Import the apt repository gpg key   curl -s https://download.dataloop.io/pubkey.gpg | apt-key add -   Add the Dataloop apt repository   echo 'deb https://download.dataloop.io/deb/ unstable main' > /etc/apt/sources.list.d/dataloop.list   Install the dataloop agent   sudo apt-get update && sudo apt-get install dataloop-agent    Update the /etc/dataloop/agent.yaml file with your API key    Start the agent    sudo systemctl start dataloop-agent   Set the agent to run on reboot   sudo systemctl enable dataloop-agent",
            "title": "Debian based Linux (Raspbian Jessie only for now)"
        },
        {
            "location": "/agent/installation_windows/",
            "text": "Installation on windows\n\n\nWindows Systems\n\n\n\n\nInstall the latest Dataloop Agent package from\n\n\n\n\nhttps://download.dataloop.io/windows/latest/dataloop-1.1-windows-installer.exe\n\n\nFor some systems you may need to install the Microsoft Visual C++ DLLs, they can be found here\n\n\nhttp://www.microsoft.com/en-us/download/details.aspx?id=29\n\n\n\n\nSilent Installation\n\n\nStarting with version 1.1.27 it is possible to install Dataloop-agent silently, passing the API Key to be put in the agent.yaml configuration file.\n\n\nThe command line usage for this is:\n\n\ndataloop-agent-1.1.27-1_x86.exe /S /apikey 1111-2222-3333-4444-5555",
            "title": "Installation windows"
        },
        {
            "location": "/agent/installation_windows/#installation-on-windows",
            "text": "",
            "title": "Installation on windows"
        },
        {
            "location": "/agent/installation_windows/#windows-systems",
            "text": "Install the latest Dataloop Agent package from   https://download.dataloop.io/windows/latest/dataloop-1.1-windows-installer.exe  For some systems you may need to install the Microsoft Visual C++ DLLs, they can be found here  http://www.microsoft.com/en-us/download/details.aspx?id=29",
            "title": "Windows Systems"
        },
        {
            "location": "/agent/installation_windows/#silent-installation",
            "text": "Starting with version 1.1.27 it is possible to install Dataloop-agent silently, passing the API Key to be put in the agent.yaml configuration file.  The command line usage for this is:  dataloop-agent-1.1.27-1_x86.exe /S /apikey 1111-2222-3333-4444-5555",
            "title": "Silent Installation"
        },
        {
            "location": "/agent/solo_mode/",
            "text": "Solo Mode\n\n\nYou can configure the dataloop agent to run in solo mode by updating the agent.yaml and restarting the dataloop-agent service.\n\n\nWhen running in solo mode the agent won't run remote commands from the web interface.\n\n\nIt will also not automatically download plugins.\n\n\nHowever, you should still tag solo mode agents in the usual way so that the metrics coming from these agents appear in dashboards and alerts.\n\n\nSolo mode agents show up in the UI with a little padlock next to their icon. Plugins deployed into the folder will also show up in the agent details page so you can verify they are being run.\n\n\nWhen running agents in solo mode you will also need to deploy a copy of \nbase.py\n locally.\n\n\nAgent Config\n\n\nTo enable solo mode edit the \nagent.yaml\n file and restart the dataloop-agent service.\n\n\n---\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo:  yes\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxx\n\n### The dataloop server endpoint\nserver: wss://agent.dataloop.io\n\n##  set tags to a comma separated list of tags applied to this agent\ntags: tag1,tag2,tag3\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:\n\n\n\n\nLocal Plugin Deployment\n\n\nYou can deploy plugins to agents running in Solo mode or normal mode. Local plugins take precedence over plugins stored in Dataloop.\n\n\nCopy a plugin into /opt/dataloop/plugins (linux) or c:\\dataloop\\plugins (windows).\nEnsure the file has a valid file extension\nEnsure the file is owned by the dataloop user\nEnsure the file is executable\n\n\n\nWithin 10 seconds the plugin will be automatically loaded into a running Dataloop agent. The agent.log file will display whether plugins have been successfully loaded or not.\nLocal Plugin Config\n\n\nYou can optionally change plugin settings by creating a file in the plugins directory alongside the plugins.\n\n\nCreate a file called plugin_name.yaml\nThe file must exactly match the name of the plugin except with a yaml extension\nThe file must be owned by the dataloop user\n\n\n\nAn example yaml file to change the settings for a plugin called node_exporter.sh\n\n\n# cat /opt/dataloop/plugins/node_exporter.yaml\n---\ninterval: 10\nparams: ''\nshell: ['/bin/bash']\ntype: INPROCESS\nformat: PROMETHEUS\n\n\n\n\n\n\n\n\ninterval: any number in seconds and defaults to 30 seconds if not supplied\n\n\n\n\n\n\nparams: a string to pass into the plugin as an argument. defaults to '' if not supplied\n\n\n\n\n\n\nshell: a list of commands used to execute the plugin. defaults to [''] if not supplied\n\n\n\n\n\n\ntype: either \nINPROCESS\n or \nSCRIPT\n. defaults to SCRIPT which executes plugins as if they were run on the command line with the shebang. \nINPROCESS\n runs Python 2.7 code in the context of the running agent code.\n\n\n\n\n\n\nformat: \nNAGIOS\n (default) or \nPROMETHEUS\n depending on the output format of the plugin",
            "title": "Solo mode"
        },
        {
            "location": "/agent/solo_mode/#solo-mode",
            "text": "You can configure the dataloop agent to run in solo mode by updating the agent.yaml and restarting the dataloop-agent service.  When running in solo mode the agent won't run remote commands from the web interface.  It will also not automatically download plugins.  However, you should still tag solo mode agents in the usual way so that the metrics coming from these agents appear in dashboards and alerts.  Solo mode agents show up in the UI with a little padlock next to their icon. Plugins deployed into the folder will also show up in the agent details page so you can verify they are being run.  When running agents in solo mode you will also need to deploy a copy of  base.py  locally.",
            "title": "Solo Mode"
        },
        {
            "location": "/agent/solo_mode/#agent-config",
            "text": "To enable solo mode edit the  agent.yaml  file and restart the dataloop-agent service.  ---\n## Set to yes to disable RPC and run plugins from local plugin source\nsolo:  yes\n\n## Set to yes to create lots of logs\ndebug: no\n\n## The dataloop api key\napi-key: xxxxxx\n\n### The dataloop server endpoint\nserver: wss://agent.dataloop.io\n\n##  set tags to a comma separated list of tags applied to this agent\ntags: tag1,tag2,tag3\n\n## Set if you would like a custom name for this agent. Default is hostname\n#name:",
            "title": "Agent Config"
        },
        {
            "location": "/agent/solo_mode/#local-plugin-deployment",
            "text": "You can deploy plugins to agents running in Solo mode or normal mode. Local plugins take precedence over plugins stored in Dataloop.  Copy a plugin into /opt/dataloop/plugins (linux) or c:\\dataloop\\plugins (windows).\nEnsure the file has a valid file extension\nEnsure the file is owned by the dataloop user\nEnsure the file is executable  Within 10 seconds the plugin will be automatically loaded into a running Dataloop agent. The agent.log file will display whether plugins have been successfully loaded or not.\nLocal Plugin Config  You can optionally change plugin settings by creating a file in the plugins directory alongside the plugins.  Create a file called plugin_name.yaml\nThe file must exactly match the name of the plugin except with a yaml extension\nThe file must be owned by the dataloop user  An example yaml file to change the settings for a plugin called node_exporter.sh  # cat /opt/dataloop/plugins/node_exporter.yaml\n---\ninterval: 10\nparams: ''\nshell: ['/bin/bash']\ntype: INPROCESS\nformat: PROMETHEUS    interval: any number in seconds and defaults to 30 seconds if not supplied    params: a string to pass into the plugin as an argument. defaults to '' if not supplied    shell: a list of commands used to execute the plugin. defaults to [''] if not supplied    type: either  INPROCESS  or  SCRIPT . defaults to SCRIPT which executes plugins as if they were run on the command line with the shebang.  INPROCESS  runs Python 2.7 code in the context of the running agent code.    format:  NAGIOS  (default) or  PROMETHEUS  depending on the output format of the plugin",
            "title": "Local Plugin Deployment"
        },
        {
            "location": "/agent/supported_operating_systems/",
            "text": "Supported Operating Systems\n\n\nWe build the Dataloop agent on Centos 6 and Debian 7 which means that anything released in May 2010 or later is supported.\n\n\n\n\nSometimes we get requests for older servers. This page is the resting place for hacks to get the agents working in extreme cases. They are officially unsupported, but we understand the pressures that sysadmins are put under to support oddities, so we will help out on a best effort basis.\n\n\nIf you have a unique snowflake running an operating system released over 5 years ago then hit us up on \nSlack\n.",
            "title": "Supported operating systems"
        },
        {
            "location": "/agent/supported_operating_systems/#supported-operating-systems",
            "text": "We build the Dataloop agent on Centos 6 and Debian 7 which means that anything released in May 2010 or later is supported.   Sometimes we get requests for older servers. This page is the resting place for hacks to get the agents working in extreme cases. They are officially unsupported, but we understand the pressures that sysadmins are put under to support oddities, so we will help out on a best effort basis.  If you have a unique snowflake running an operating system released over 5 years ago then hit us up on  Slack .",
            "title": "Supported Operating Systems"
        },
        {
            "location": "/agent/uninstall/",
            "text": "Uninstall\n\n\nOn linux you can simply remove the \ndataloop-agent\n package.\n\n\nRedhat based:\n\n\nyum remove dataloop-agent\n\n\n\n\n\n\nDebian based:\n\n\napt-get remove dataloop-agent\n\n\n\n\n\n\nWindows\n\n\nOn windows you can remove via add/remove programs.",
            "title": "Uninstall"
        },
        {
            "location": "/agent/uninstall/#uninstall",
            "text": "On linux you can simply remove the  dataloop-agent  package.",
            "title": "Uninstall"
        },
        {
            "location": "/agent/uninstall/#redhat-based",
            "text": "yum remove dataloop-agent",
            "title": "Redhat based:"
        },
        {
            "location": "/agent/uninstall/#debian-based",
            "text": "apt-get remove dataloop-agent",
            "title": "Debian based:"
        },
        {
            "location": "/agent/uninstall/#windows",
            "text": "On windows you can remove via add/remove programs.",
            "title": "Windows"
        },
        {
            "location": "/alerting/agent_alerts/",
            "text": "Agent Alerts\n\n\nAgents create a persistent websocket connection to agent.dataloop.io when they start.\n\n\nIf this connection drops you will see agent names will change to either orange or red.\n\n\nThe Agent will do everything within its power to try to reconnect. So you may see some drops and reconnects depending on network issues.\n\n\nA large portion of the Dataloop web application depends on real time connectivity (remote command execution and live streaming graphs for instance) so we therefore make it obvious when a connection has dropped to prevent frustration when buttons are clicked but nothing happens.\n\n\nTo alert on agent failure create a new rule and select your scope, be that either an individual agent or a group of agents in a tag. In the 'Alert On' drop down you can now select the \nagent.status\n metric. You'll then get the option to specify how long the agent(s) need to have been down for in order to trigger an alert.",
            "title": "Agent alerts"
        },
        {
            "location": "/alerting/agent_alerts/#agent-alerts",
            "text": "Agents create a persistent websocket connection to agent.dataloop.io when they start.  If this connection drops you will see agent names will change to either orange or red.  The Agent will do everything within its power to try to reconnect. So you may see some drops and reconnects depending on network issues.  A large portion of the Dataloop web application depends on real time connectivity (remote command execution and live streaming graphs for instance) so we therefore make it obvious when a connection has dropped to prevent frustration when buttons are clicked but nothing happens.  To alert on agent failure create a new rule and select your scope, be that either an individual agent or a group of agents in a tag. In the 'Alert On' drop down you can now select the  agent.status  metric. You'll then get the option to specify how long the agent(s) need to have been down for in order to trigger an alert.",
            "title": "Agent Alerts"
        },
        {
            "location": "/alerting/metric_alerts/",
            "text": "Metric Alerts\n\n\nDataloop supports Nagios script performance metrics and Graphite metrics. We convert both of these metric sources to our own internal format so that you can graph and alert on both.\n\n\nMetric alerts can be configured via rules. Simply create a new rule, add a criteria and then select either an agent, or a tag of agents as the scope. The 'Alert On' drop down will then show all of the available metrics that can be alerted on.\n\n\nCurrently you can set a threshold and a duration. We will be adding more advanced features soon.\n\n\nThe rules engine is real-time. You can stream 1 second granularity metrics into Dataloop and we'll alert you instantly when a rule is triggered. We can do this across multiple sources and with multiple criteria. Most importantly, we do this without any single points of failure so you can trust us to monitor production.",
            "title": "Metric alerts"
        },
        {
            "location": "/alerting/metric_alerts/#metric-alerts",
            "text": "Dataloop supports Nagios script performance metrics and Graphite metrics. We convert both of these metric sources to our own internal format so that you can graph and alert on both.  Metric alerts can be configured via rules. Simply create a new rule, add a criteria and then select either an agent, or a tag of agents as the scope. The 'Alert On' drop down will then show all of the available metrics that can be alerted on.  Currently you can set a threshold and a duration. We will be adding more advanced features soon.  The rules engine is real-time. You can stream 1 second granularity metrics into Dataloop and we'll alert you instantly when a rule is triggered. We can do this across multiple sources and with multiple criteria. Most importantly, we do this without any single points of failure so you can trust us to monitor production.",
            "title": "Metric Alerts"
        },
        {
            "location": "/alerting/plugin_alerts/",
            "text": "Plugin Alerts\n\n\nDataloop uses Nagios format plugins which means that we adhere to these exit codes:\n\n\n0 OK\n1 WARNING\n2 CRITICAL\n3 UNKNOWN\n\n\n\n\nWe treat these exit codes like a metric stream in Dataloop. For every Nagios script that returns data you'll see \n<script name>.status\n in the \nAlert On\n drop down.\n\n\nIf you select this you'll be prompted with a slightly different set of options from metrics. Essentially you just need to set how long the script needs to have failed for before the criteria is triggered.\n\n\nWe actually treat exit code \n2\n and \n3\n as down. So if your script returns exit code \n2\n or \n3\n it will trigger a script.status rule. This means we ignore OK and Warning for the purposes of alerting.",
            "title": "Plugin alerts"
        },
        {
            "location": "/alerting/plugin_alerts/#plugin-alerts",
            "text": "Dataloop uses Nagios format plugins which means that we adhere to these exit codes:  0 OK\n1 WARNING\n2 CRITICAL\n3 UNKNOWN  We treat these exit codes like a metric stream in Dataloop. For every Nagios script that returns data you'll see  <script name>.status  in the  Alert On  drop down.  If you select this you'll be prompted with a slightly different set of options from metrics. Essentially you just need to set how long the script needs to have failed for before the criteria is triggered.  We actually treat exit code  2  and  3  as down. So if your script returns exit code  2  or  3  it will trigger a script.status rule. This means we ignore OK and Warning for the purposes of alerting.",
            "title": "Plugin Alerts"
        },
        {
            "location": "/alerting/rules/",
            "text": "Rules\n\n\nRules are comprised of \ncriteria\n and \nactions\n. It is recommended that all rules be created based on tags and that criteria for a given service are grouped together based on severity.\n\n\n\n\nExample: The rule 'ElasticSearch Warning' may contain criteria that need attention but wouldn't necessarily wake anyone up. The actions for this rule would email a group and send a webhook to Slack. Whereas the 'ElasticSearch Critical' rule would contain a few critical checks, such as service down and would send a webhook to Pagerduty.\n\n\n\n\nCriterias are made up of a scope, the metric to alert on and options like comparator, duration and threshold.\n\n\nSupported actions include sending an email or a webhook. When a webhook is configured for a supported integration (listed under the Integrations section of the support docs) we detect the url and send additional fields.\n\n\nIf multiple criteria are created within a rule then \nANY\n need to be met before the rule is triggered and \nALL\n actions are run.\n\n\nDataloop rules are very flexible, to the point where you can shoot yourself in the foot if you choose to do so.\n\n\n\n\nExample: You can create a plugin to monitor a service and apply it to one tag of agents. Then configure a criteria to alert on that same plugin using a different tag that only contains a subset of agents that the plugin is deployed to. For this reason we have defined a simple traffic light system to help uncover mistakes in setup.\n\n\n\n\nCriteria and Rule status\n\n\nYou can view the overall health of rules in the alerts page in Dataloop. Each rule is colour coded as per the table below. To be confident that you have effective alerting setup the aim should be to keep all rules in a green state.\n\n\n\n\n\n\n\n\nColour\n\n\nState\n\n\n\n\n\n\n\n\n\n\nGreen\n\n\nClear\n\n\n\n\n\n\nOrange\n\n\nPending\n\n\n\n\n\n\nGrey\n\n\nUnknown\n\n\n\n\n\n\nRed\n\n\nTriggered\n\n\n\n\n\n\n\n\nWhen a criteria is defined in Dataloop a query is added to a background set of workers that poll every 10 seconds and compare desired state to current state.\n\n\nIf a complete set of data is returned for \nall\n criteria in a rule and all checks pass then a criteria is considered clear. If all criteria within a rule are clear then the rule is clear.\n\n\nPending is the transitional state between Clear and Triggered. A criteria will stay pending for the duration specified.\n\n\n\n\nExample: A criteria duration is set to 5 minutes and a problem occurs the criteria will switch from green to orange within 10 seconds and then stay pending for 5 minutes before turning red. The same is true when a problem is fixed. The criteria will change from red to orange and stay pending for 5 minutes before turning green.\n\n\n\n\nA rule will appear red if \nany\n criteria within it are red.\n\n\nA rule will appear grey if \nany\n of the criteria are not receiving data, of if the rule contains no criteria.\n\n\nWhen evaluating the order of state both within criteria and for criteria within rules we use the following matrix.\n\n\n\n\n\n\n\n\n\n\nunknown\n\n\nclear\n\n\ntriggered\n\n\n\n\n\n\n\n\n\n\nunknown\n\n\nunknown\n\n\nunknown\n\n\ntriggered\n\n\n\n\n\n\nclear\n\n\nunknown\n\n\nclear\n\n\ntriggered\n\n\n\n\n\n\ntriggered\n\n\ntriggered\n\n\ntriggered\n\n\ntriggered\n\n\n\n\n\n\n\n\nThis means that a triggered criteria will always turn a rule red. Also, that unknown criteria will always turn a rule grey (in the absence of any triggered criteria).\n\n\nAlert Lifecycle\n\n\nFor a rule that is currently \ngreen\n the only state change possible is to change to \ngrey\n if any of the criteria stop returning data for any defined metric path. Or, alternatively, it may turn \nred\n if an agent or plugin changes status or a threshold is met.\n\n\nActions are run when state changes to \nred\n with a message beginning \nALERT:\n which signifies the start of an incident. Integration fields are configured in such a way that this will create a unique new incident.\n\n\nWhen a rule is red, but criteria within it change state, the actions are triggered again with a message beginning \nUPDATE:\n which signifies that something has changed (either got better or worse). These are correlated with supported integrations so that updates append additional information to the open incident, and don't create new incidents.\n\n\nWhen a rule is either grey or red it may change state to green. On state change to green the actions are triggered with a message beginning \nRESOLVED:\n. This signifies the end of the incident and supported integrations are configured to automatically close the incident.\n\n\nIt is worth noting that a rule that has criteria not receiving data will never send a resolved email or webhook. However, changes to criteria within a grey rule will still send update notifications. It is recommended that an effort is made to keep all rules in a green state.",
            "title": "Rules"
        },
        {
            "location": "/alerting/rules/#rules",
            "text": "Rules are comprised of  criteria  and  actions . It is recommended that all rules be created based on tags and that criteria for a given service are grouped together based on severity.   Example: The rule 'ElasticSearch Warning' may contain criteria that need attention but wouldn't necessarily wake anyone up. The actions for this rule would email a group and send a webhook to Slack. Whereas the 'ElasticSearch Critical' rule would contain a few critical checks, such as service down and would send a webhook to Pagerduty.   Criterias are made up of a scope, the metric to alert on and options like comparator, duration and threshold.  Supported actions include sending an email or a webhook. When a webhook is configured for a supported integration (listed under the Integrations section of the support docs) we detect the url and send additional fields.  If multiple criteria are created within a rule then  ANY  need to be met before the rule is triggered and  ALL  actions are run.  Dataloop rules are very flexible, to the point where you can shoot yourself in the foot if you choose to do so.   Example: You can create a plugin to monitor a service and apply it to one tag of agents. Then configure a criteria to alert on that same plugin using a different tag that only contains a subset of agents that the plugin is deployed to. For this reason we have defined a simple traffic light system to help uncover mistakes in setup.",
            "title": "Rules"
        },
        {
            "location": "/alerting/rules/#criteria-and-rule-status",
            "text": "You can view the overall health of rules in the alerts page in Dataloop. Each rule is colour coded as per the table below. To be confident that you have effective alerting setup the aim should be to keep all rules in a green state.     Colour  State      Green  Clear    Orange  Pending    Grey  Unknown    Red  Triggered     When a criteria is defined in Dataloop a query is added to a background set of workers that poll every 10 seconds and compare desired state to current state.  If a complete set of data is returned for  all  criteria in a rule and all checks pass then a criteria is considered clear. If all criteria within a rule are clear then the rule is clear.  Pending is the transitional state between Clear and Triggered. A criteria will stay pending for the duration specified.   Example: A criteria duration is set to 5 minutes and a problem occurs the criteria will switch from green to orange within 10 seconds and then stay pending for 5 minutes before turning red. The same is true when a problem is fixed. The criteria will change from red to orange and stay pending for 5 minutes before turning green.   A rule will appear red if  any  criteria within it are red.  A rule will appear grey if  any  of the criteria are not receiving data, of if the rule contains no criteria.  When evaluating the order of state both within criteria and for criteria within rules we use the following matrix.      unknown  clear  triggered      unknown  unknown  unknown  triggered    clear  unknown  clear  triggered    triggered  triggered  triggered  triggered     This means that a triggered criteria will always turn a rule red. Also, that unknown criteria will always turn a rule grey (in the absence of any triggered criteria).",
            "title": "Criteria and Rule status"
        },
        {
            "location": "/alerting/rules/#alert-lifecycle",
            "text": "For a rule that is currently  green  the only state change possible is to change to  grey  if any of the criteria stop returning data for any defined metric path. Or, alternatively, it may turn  red  if an agent or plugin changes status or a threshold is met.  Actions are run when state changes to  red  with a message beginning  ALERT:  which signifies the start of an incident. Integration fields are configured in such a way that this will create a unique new incident.  When a rule is red, but criteria within it change state, the actions are triggered again with a message beginning  UPDATE:  which signifies that something has changed (either got better or worse). These are correlated with supported integrations so that updates append additional information to the open incident, and don't create new incidents.  When a rule is either grey or red it may change state to green. On state change to green the actions are triggered with a message beginning  RESOLVED: . This signifies the end of the incident and supported integrations are configured to automatically close the incident.  It is worth noting that a rule that has criteria not receiving data will never send a resolved email or webhook. However, changes to criteria within a grey rule will still send update notifications. It is recommended that an effort is made to keep all rules in a green state.",
            "title": "Alert Lifecycle"
        },
        {
            "location": "/alerting/webhook/",
            "text": "Webhook\n\n\nWhen adding an action to a rule you can select the webhook option. This is a great way to connect your alerts into other systems like Slack or iFTTT (if this then that).\n\n\nTo configure a webhook simply change the URL field and hit test. By default we send the payload as JSON but you can change the drop down to send encoded in the URL if the service you are sending to prefers that.\n\n\nYou can add additional fields to the payload in the 'Extra Payload' box. This can also be used to override any of the current fields that we send.\n\n\nFinally, click the test button to verify your webhook is configured correctly.\n\n\nExample delivery\n\n\nUser-Agent: Dataloop\nContent-Type: application/json\nX-Dataloop-Event: alert-webhook\n\n{\n  \"event\": \"alert\",\n  \"rule\": \"All Sytems\",\n  \"account\": \"noreply@dataloop.io\",\n  \"triggers\": [\n    {\n      \"criteria\": \"hosts are reporting down\",\n      \"sources\": [\n        {\n          \"name\": \"dataloop\",\n          \"tags\": [ \"all\" ],\n          \"timestamp\": \"Wed Dec 09 2015 15:21:20 GMT+0000 (GMT)\"\n        }\n    }\n  ],\n  \"text\": \"a formatted summary of the payload\"\n}\n\n\n\n\nDetails\n\n\n\n\n\n\nevent: [\"alert\" | \"recovered\"]\n\n\n\n\n\n\ncriteria: [\"hosts are reporting down\" | \"check \n is failing\" | \"\n above threshold of \n\"]\n\n\n\n\n\n\ntext: will be picked up automatically by slack\n\n\n\n\n\n\nno triggers details when event is 'recovered'",
            "title": "Webhook"
        },
        {
            "location": "/alerting/webhook/#webhook",
            "text": "When adding an action to a rule you can select the webhook option. This is a great way to connect your alerts into other systems like Slack or iFTTT (if this then that).  To configure a webhook simply change the URL field and hit test. By default we send the payload as JSON but you can change the drop down to send encoded in the URL if the service you are sending to prefers that.  You can add additional fields to the payload in the 'Extra Payload' box. This can also be used to override any of the current fields that we send.  Finally, click the test button to verify your webhook is configured correctly.",
            "title": "Webhook"
        },
        {
            "location": "/alerting/webhook/#example-delivery",
            "text": "User-Agent: Dataloop\nContent-Type: application/json\nX-Dataloop-Event: alert-webhook\n\n{\n  \"event\": \"alert\",\n  \"rule\": \"All Sytems\",\n  \"account\": \"noreply@dataloop.io\",\n  \"triggers\": [\n    {\n      \"criteria\": \"hosts are reporting down\",\n      \"sources\": [\n        {\n          \"name\": \"dataloop\",\n          \"tags\": [ \"all\" ],\n          \"timestamp\": \"Wed Dec 09 2015 15:21:20 GMT+0000 (GMT)\"\n        }\n    }\n  ],\n  \"text\": \"a formatted summary of the payload\"\n}",
            "title": "Example delivery"
        },
        {
            "location": "/alerting/webhook/#details",
            "text": "event: [\"alert\" | \"recovered\"]    criteria: [\"hosts are reporting down\" | \"check   is failing\" | \"  above threshold of  \"]    text: will be picked up automatically by slack    no triggers details when event is 'recovered'",
            "title": "Details"
        },
        {
            "location": "/dashboards/annotations/",
            "text": "Annotations\n\n\nYou can post annotations into Dataloop via either the API or command line utility.\n\n\nAPI Docs\n\n\nAnnotations are posted into \nstreams\n. These streams can then be switched on and off within the Dataloop dashboards page via the annotations drop down at the top.\n\n\nCurl\n\n\ncurl -X POST -H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer {{your_api_key}}\" \\\n-d '{\"name\": \"name\", \"description\": \"description\"}' \\\n\"http://api.dataloop.io/v1/orgs/{{your_org}}/accounts/{{your_account}}/annotations/{{stream_name}}\"\n\n\n\n\nCommand Line\n\n\nSetup the Dataloop command line utility (\ndlcli\n)\n\n\nPost a new annotation for a new deployment.\n\n\ndlcli push annotation deployments --name 'deployed micro-service-1' --description 'version 100'\n\n\n\n\nNow in Dataloop browse to a dashboard and tick the \ndeployments\n stream from the annotations drop down.\n\n\nAnnotations will then be visible on graph widgets and the details can be seen by hovering over the dot on the horizontal axis.\n\n\nAnsible\n\n\nCreate a handler to annotate Dataloop whenever you deploy some code to production. In our example we are posting the package name and version under the stream deployments which is configured in the url.\n\n\n  handlers:\n  - name: annotate_dataloop\n    uri:\n      url: https://app.dataloop.io/api/v1/orgs/{{ dataloop_org_name }}/accounts/{{ dataloop_account_name }}/annotations/deployments\n      method: POST\n      HEADER_Authorization: \"Bearer {{ dataloop_api_key }}\"\n      body_format: json\n      body: {\"name\": \"{{ package }}\", \"description\": \"{{ version }}\"}\n      run_once: true\n\n\n\n\nThen call \nannotate_dataloop\n with a notify on the task that does the package deployment.\nChef\n\n\nChef\n\n\nWe have a Chef handler for posting annotations into Dataloop on every chef run.\n\n\nhttps://github.com/dataloop/dataloop-chef-handler",
            "title": "Annotations"
        },
        {
            "location": "/dashboards/annotations/#annotations",
            "text": "You can post annotations into Dataloop via either the API or command line utility.  API Docs  Annotations are posted into  streams . These streams can then be switched on and off within the Dataloop dashboards page via the annotations drop down at the top.",
            "title": "Annotations"
        },
        {
            "location": "/dashboards/annotations/#curl",
            "text": "curl -X POST -H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer {{your_api_key}}\" \\\n-d '{\"name\": \"name\", \"description\": \"description\"}' \\\n\"http://api.dataloop.io/v1/orgs/{{your_org}}/accounts/{{your_account}}/annotations/{{stream_name}}\"",
            "title": "Curl"
        },
        {
            "location": "/dashboards/annotations/#command-line",
            "text": "Setup the Dataloop command line utility ( dlcli )  Post a new annotation for a new deployment.  dlcli push annotation deployments --name 'deployed micro-service-1' --description 'version 100'  Now in Dataloop browse to a dashboard and tick the  deployments  stream from the annotations drop down.  Annotations will then be visible on graph widgets and the details can be seen by hovering over the dot on the horizontal axis.",
            "title": "Command Line"
        },
        {
            "location": "/dashboards/annotations/#ansible",
            "text": "Create a handler to annotate Dataloop whenever you deploy some code to production. In our example we are posting the package name and version under the stream deployments which is configured in the url.    handlers:\n  - name: annotate_dataloop\n    uri:\n      url: https://app.dataloop.io/api/v1/orgs/{{ dataloop_org_name }}/accounts/{{ dataloop_account_name }}/annotations/deployments\n      method: POST\n      HEADER_Authorization: \"Bearer {{ dataloop_api_key }}\"\n      body_format: json\n      body: {\"name\": \"{{ package }}\", \"description\": \"{{ version }}\"}\n      run_once: true  Then call  annotate_dataloop  with a notify on the task that does the package deployment.\nChef",
            "title": "Ansible"
        },
        {
            "location": "/dashboards/annotations/#chef",
            "text": "We have a Chef handler for posting annotations into Dataloop on every chef run.  https://github.com/dataloop/dataloop-chef-handler",
            "title": "Chef"
        },
        {
            "location": "/dashboards/grafana/",
            "text": "Grafana\n\n\nGrafana 3.0 Plugin\n\n\nYou can add a Dataloop agent as a datasource in Grafana by following these instructions. This is especially useful when used with a StatsD server. Install a Dataloop agent on the StatsD server and use the fingerprint to view all of your StatsD metrics in Grafana.\n\n\n\n\nInstall Grafana 3.0\n\n\n\n\nhttp://docs.grafana.org/installation/\n\n\n\n\nAdd the Dataloop data source\n\n\n\n\ncd /var/lib/grafana/plugins\n\n\n\n\ngit clone https://github.com/dataloop/dalmatinerdb-datasource dalmatinerdb\n\n\n\n\nRestart the grafana-server service\n\n\n\n\nsudo service grafana-server restart\n\n\n\n\n\n\nCreate an API token\n\n\n\n\nLogin into Dataloop and click account settings in the top right corner. Then generate an API token. You may want to create a service user account to restrict access to certain metrics in Grafana. \n\n\n\n\nCreate a new Data source in Grafana as per below\n\n\n\n\n\n\n* Name: Dataloop\n* Type: DalmatinerDB\n* URL: https://grafana.dataloop.io\n* Access: proxy\n* Auth Token: Ticked, and use the API token from step 4\n\n\n\nNote\n: Wildcards in metric paths only currently work within a single fingerprint. This is not a problem for dimensional data (coming from Prometheus plugins) but is required for StatsD. Therefore to use wildcards in metric paths you must always specify a dl:source in the WHERE clause.",
            "title": "Grafana"
        },
        {
            "location": "/dashboards/grafana/#grafana",
            "text": "",
            "title": "Grafana"
        },
        {
            "location": "/dashboards/grafana/#grafana-30-plugin",
            "text": "You can add a Dataloop agent as a datasource in Grafana by following these instructions. This is especially useful when used with a StatsD server. Install a Dataloop agent on the StatsD server and use the fingerprint to view all of your StatsD metrics in Grafana.   Install Grafana 3.0   http://docs.grafana.org/installation/   Add the Dataloop data source   cd /var/lib/grafana/plugins  git clone https://github.com/dataloop/dalmatinerdb-datasource dalmatinerdb   Restart the grafana-server service   sudo service grafana-server restart   Create an API token   Login into Dataloop and click account settings in the top right corner. Then generate an API token. You may want to create a service user account to restrict access to certain metrics in Grafana.    Create a new Data source in Grafana as per below    * Name: Dataloop\n* Type: DalmatinerDB\n* URL: https://grafana.dataloop.io\n* Access: proxy\n* Auth Token: Ticked, and use the API token from step 4  Note : Wildcards in metric paths only currently work within a single fingerprint. This is not a problem for dimensional data (coming from Prometheus plugins) but is required for StatsD. Therefore to use wildcards in metric paths you must always specify a dl:source in the WHERE clause.",
            "title": "Grafana 3.0 Plugin"
        },
        {
            "location": "/dashboards/graph_widgets/",
            "text": "Graph Widgets\n\n\nChart\n\n\nA simple widget displaying the area under all metrics.\n\n\nThis is useful for displaying a trend across multiple metric sources. Using the Chart widget will improve dashboard performance if you have hundreds of metric sources. They are also the most readable from a distance if you are creating dashboards for TV screens.\n\n\nDetailed\n\n\nThis simply draws the data as received. You will get multiple series lines in different shades of the widget colour. There is a legend option available.\n\n\nThese widgets are most useful when troubleshooting problems. Easily spot outliers by clicking the graph widget to get a list of agents and values.\n\n\nStacked\n\n\nDraws all series lines stacked on top of each other and ordered by highest average first. There is a legend option available.\n\n\nThese widgets are useful for visually seeing where the bottleneck is. Plot multiple metrics on a widget and see if there are any humps caused by something specific.",
            "title": "Graph widgets"
        },
        {
            "location": "/dashboards/graph_widgets/#graph-widgets",
            "text": "",
            "title": "Graph Widgets"
        },
        {
            "location": "/dashboards/graph_widgets/#chart",
            "text": "A simple widget displaying the area under all metrics.  This is useful for displaying a trend across multiple metric sources. Using the Chart widget will improve dashboard performance if you have hundreds of metric sources. They are also the most readable from a distance if you are creating dashboards for TV screens.",
            "title": "Chart"
        },
        {
            "location": "/dashboards/graph_widgets/#detailed",
            "text": "This simply draws the data as received. You will get multiple series lines in different shades of the widget colour. There is a legend option available.  These widgets are most useful when troubleshooting problems. Easily spot outliers by clicking the graph widget to get a list of agents and values.",
            "title": "Detailed"
        },
        {
            "location": "/dashboards/graph_widgets/#stacked",
            "text": "Draws all series lines stacked on top of each other and ordered by highest average first. There is a legend option available.  These widgets are useful for visually seeing where the bottleneck is. Plot multiple metrics on a widget and see if there are any humps caused by something specific.",
            "title": "Stacked"
        },
        {
            "location": "/dashboards/numeric_widgets/",
            "text": "Numeric Widgets\n\n\nWidgets available when browsing a single agent\n\n\nLast Value\n\n\nThe last value received for the given metric. Basically a gauge.\n\n\nTotal Value\n\n\nThe sum of all values for a given metric over the time period set the dashboard.\n\n\nA good use case for this is adding up counter values from StatsD. We'll add up all of those 10 second flushes and help you display things like 'signups in the last 24 hours'.\n\n\nWidgets available when browsing multiple agents (using tags)\n\n\nAverage\n\n\nAn average of the last values received by all agents in the tag(s).\n\n\nLowest\n\n\nThe lowest of the last values received by all agents in the tag(s)\n\n\nThis widget is useful for displaying metrics like best current response time.\n\n\nHighest\n\n\nThe highest of the last values received by all agents in the tag(s)\n\n\nThis widget is useful for displaying metrics like worst current response time.\n\n\nCombined\n\n\nThe summation of the last values received by all agents in the tag(s)\n\n\nThis widget is useful for displaying metrics like current used disk space across a cluster",
            "title": "Numeric widgets"
        },
        {
            "location": "/dashboards/numeric_widgets/#numeric-widgets",
            "text": "",
            "title": "Numeric Widgets"
        },
        {
            "location": "/dashboards/numeric_widgets/#widgets-available-when-browsing-a-single-agent",
            "text": "",
            "title": "Widgets available when browsing a single agent"
        },
        {
            "location": "/dashboards/numeric_widgets/#last-value",
            "text": "The last value received for the given metric. Basically a gauge.",
            "title": "Last Value"
        },
        {
            "location": "/dashboards/numeric_widgets/#total-value",
            "text": "The sum of all values for a given metric over the time period set the dashboard.  A good use case for this is adding up counter values from StatsD. We'll add up all of those 10 second flushes and help you display things like 'signups in the last 24 hours'.",
            "title": "Total Value"
        },
        {
            "location": "/dashboards/numeric_widgets/#widgets-available-when-browsing-multiple-agents-using-tags",
            "text": "",
            "title": "Widgets available when browsing multiple agents (using tags)"
        },
        {
            "location": "/dashboards/numeric_widgets/#average",
            "text": "An average of the last values received by all agents in the tag(s).",
            "title": "Average"
        },
        {
            "location": "/dashboards/numeric_widgets/#lowest",
            "text": "The lowest of the last values received by all agents in the tag(s)  This widget is useful for displaying metrics like best current response time.",
            "title": "Lowest"
        },
        {
            "location": "/dashboards/numeric_widgets/#highest",
            "text": "The highest of the last values received by all agents in the tag(s)  This widget is useful for displaying metrics like worst current response time.",
            "title": "Highest"
        },
        {
            "location": "/dashboards/numeric_widgets/#combined",
            "text": "The summation of the last values received by all agents in the tag(s)  This widget is useful for displaying metrics like current used disk space across a cluster",
            "title": "Combined"
        },
        {
            "location": "/dashboards/public_dashboards/",
            "text": "Public Dashboards\n\n\nYou can generate a public dashboard URL by clicking the share icon at the top of the dashboards page.\n\n\nPublic dashboards provide a read only unauthenticated view that never times out. These are great for quickly sharing a dashboard with a colleague or for use on TV screens.\n\n\nUpon request we can set a global password on public dashboards so that you can restrict access to your company.\n\n\nWhen tiling Dataloop public dashboards with other dashboards on the same screen you may want to turn off the chrome around the edge of the widgets to save space. To do this simply append &chrome=false to the end of the public dashboard url.",
            "title": "Public dashboards"
        },
        {
            "location": "/dashboards/public_dashboards/#public-dashboards",
            "text": "You can generate a public dashboard URL by clicking the share icon at the top of the dashboards page.  Public dashboards provide a read only unauthenticated view that never times out. These are great for quickly sharing a dashboard with a colleague or for use on TV screens.  Upon request we can set a global password on public dashboards so that you can restrict access to your company.  When tiling Dataloop public dashboards with other dashboards on the same screen you may want to turn off the chrome around the edge of the widgets to save space. To do this simply append &chrome=false to the end of the public dashboard url.",
            "title": "Public Dashboards"
        },
        {
            "location": "/dashboards/troubleshooting/",
            "text": "Troubleshooting\n\n\nHovering over a widget will display some options in the top right corner.\n\n\nThe best way to troubleshoot a widget is to click the (i) button and confirm the metric, scope, sources and last update time.\n\n\nIf troubleshooting a plugin then try to run it on an agent in the scope specified on the widget to check that the correct data is being returned.\n\n\nThe Dataloop Agent sends plugin data back using the wall clock time of the computer it is running on. Ensure you are running NTP or similar otherwise widgets can display odd looking data.",
            "title": "Troubleshooting"
        },
        {
            "location": "/dashboards/troubleshooting/#troubleshooting",
            "text": "Hovering over a widget will display some options in the top right corner.  The best way to troubleshoot a widget is to click the (i) button and confirm the metric, scope, sources and last update time.  If troubleshooting a plugin then try to run it on an agent in the scope specified on the widget to check that the correct data is being returned.  The Dataloop Agent sends plugin data back using the wall clock time of the computer it is running on. Ensure you are running NTP or similar otherwise widgets can display odd looking data.",
            "title": "Troubleshooting"
        },
        {
            "location": "/endpoints/collectd/",
            "text": "Collectd\n\n\nCollectD Configuration\n\n\nIt's best to use the latest CollectD package. You can find instructions for setting up the repo's for Debian and Redhat based distros here:\n\n\nhttps://github.com/collectd/collectd-ci/blob/master/README.md\n\n\nChange the following lines in your collectd.conf file. Where fingerprint is the string from \n/etc/dataloop/agent.finger\n. You may need to uncomment the \nLoadPlugin\n line.\n\n\nHostname    \"fingerprint\"\n\nLoadPlugin  write_graphite\n\n<Plugin write_graphite>\n  <Node \"fingerprint\">\n    Host \"graphite.dataloop.io\"\n    Port \"2003\"\n    Protocol \"udp\"\n    LogSendErrors true\n    Prefix \"\"\n    Postfix \".collectd\"\n    StoreRates true\n    AlwaysAppendDS false\n    EscapeCharacter \"_\"\n  </Node>\n</Plugin>\n\n\n\n\nThe metrics will appear under `collectd' when browsing in the Dataloop web interface.\n\n\nOther Software\n\n\nLots of open source tools have a Graphite backend. In general you should only need to configure two settings; the graphite server address, and the metric prefix. Where our server address is \ngraphite.dataloop.io\n and our metric prefix is your fingerprint.",
            "title": "Collectd"
        },
        {
            "location": "/endpoints/collectd/#collectd",
            "text": "",
            "title": "Collectd"
        },
        {
            "location": "/endpoints/collectd/#collectd-configuration",
            "text": "It's best to use the latest CollectD package. You can find instructions for setting up the repo's for Debian and Redhat based distros here:  https://github.com/collectd/collectd-ci/blob/master/README.md  Change the following lines in your collectd.conf file. Where fingerprint is the string from  /etc/dataloop/agent.finger . You may need to uncomment the  LoadPlugin  line.  Hostname    \"fingerprint\"\n\nLoadPlugin  write_graphite\n\n<Plugin write_graphite>\n  <Node \"fingerprint\">\n    Host \"graphite.dataloop.io\"\n    Port \"2003\"\n    Protocol \"udp\"\n    LogSendErrors true\n    Prefix \"\"\n    Postfix \".collectd\"\n    StoreRates true\n    AlwaysAppendDS false\n    EscapeCharacter \"_\"\n  </Node>\n</Plugin>  The metrics will appear under `collectd' when browsing in the Dataloop web interface.",
            "title": "CollectD Configuration"
        },
        {
            "location": "/endpoints/collectd/#other-software",
            "text": "Lots of open source tools have a Graphite backend. In general you should only need to configure two settings; the graphite server address, and the metric prefix. Where our server address is  graphite.dataloop.io  and our metric prefix is your fingerprint.",
            "title": "Other Software"
        },
        {
            "location": "/endpoints/graphite/",
            "text": "Graphite\n\n\nYou can stream metrics directly into \ngraphite.dataloop.io\n on tcp port \n2003\n using the Graphite metric format.\n\n\nThis is great for high resolution metrics up to 1 second granularity or for long running jobs that you may want to schedule via cron. Many open source metric collection tools have Graphite backends which can be configured to stream metrics directly into Dataloop.\n\n\nThe plain text protocol is described here:\n\n\nhttp://graphite.readthedocs.org/en/latest/feeding-carbon.html\n\n\nThe metric path section of the string determines how Dataloop will store the data. The format should be:\n\n\nfingerprint.metric.path value timestamp\n\n\n\n\nThe timestamp value is optional. If you don't set one we will set the timestamp as the time the metric arrived. You can send us historical timestamps if you wish to migrate data into Dataloop.\n\n\nCurrently we only allow Graphite data to be attached to an Agent fingerprint.\n\n\nThe first step is therefore to install a Dataloop Agent on your server. After that you can find your Agent fingerprint on Linux in \n/etc/dataloop/agent.finger\n, or in the Dataloop UI on the agent details page.\n\n\nTesting\n\n\nSet the fingerprint variable so we can use it in the metric path:\n\n\nfinger=$(sed -n -e 's/fingerprint = //p' /etc/dataloop/agent.finger)\n\n\n\n\nThen echo a random metric to our graphite port:\n\n\necho \"${finger}.test $RANDOM\" | nc -c graphite.dataloop.io 2003\n\n\n\n\nYou can now create a dashboard widget in Dataloop and browse to the agent you sent the metric in from.\n\n\nSimple Example\n\n\nStart by finding your fingerprint:\n\n\ncat /etc/dataloop/agent.finger\n\n[private]\nfingerprint = 2afee216-a80f-4f01-9220-14bc3195a3d5\n\n\n\n\nThen simply echo the fingerprint followed by a dot separated metric path followed by your value.\n\n\necho \"2afee216-a80f-4f01-9220-14bc3195a3d5.some.metric.path $RANDOM\" | nc -c graphite.dataloop.io 2003\n\n\n\n\nIn our example we just sent in a random value. When you login to the Dataloop UI you'll now see some.metric.path show up in the dashboard tree browser and the alerts metric drop-down.\n\n\nWarning: There are a couple of implementations of netcat. The above example with the -c option was for the GNU implementation. If you are using a BSD implementation (like OSX) then use -q0 instead. \n\n\nTroubleshooting\n\n\nThe easiest way to troubleshoot a Graphite backend integration is to open a netcat instance listening on another port. For example\n\n\nnetcat -l -k 2004\n\n\n\n\nThen temporarily redirect your 3rd party software to send to localhost port 2004 tcp. The netcat listener will print out the metrics so you can compare to the format listed above.",
            "title": "Graphite"
        },
        {
            "location": "/endpoints/graphite/#graphite",
            "text": "You can stream metrics directly into  graphite.dataloop.io  on tcp port  2003  using the Graphite metric format.  This is great for high resolution metrics up to 1 second granularity or for long running jobs that you may want to schedule via cron. Many open source metric collection tools have Graphite backends which can be configured to stream metrics directly into Dataloop.  The plain text protocol is described here:  http://graphite.readthedocs.org/en/latest/feeding-carbon.html  The metric path section of the string determines how Dataloop will store the data. The format should be:  fingerprint.metric.path value timestamp  The timestamp value is optional. If you don't set one we will set the timestamp as the time the metric arrived. You can send us historical timestamps if you wish to migrate data into Dataloop.  Currently we only allow Graphite data to be attached to an Agent fingerprint.  The first step is therefore to install a Dataloop Agent on your server. After that you can find your Agent fingerprint on Linux in  /etc/dataloop/agent.finger , or in the Dataloop UI on the agent details page.",
            "title": "Graphite"
        },
        {
            "location": "/endpoints/graphite/#testing",
            "text": "Set the fingerprint variable so we can use it in the metric path:  finger=$(sed -n -e 's/fingerprint = //p' /etc/dataloop/agent.finger)  Then echo a random metric to our graphite port:  echo \"${finger}.test $RANDOM\" | nc -c graphite.dataloop.io 2003  You can now create a dashboard widget in Dataloop and browse to the agent you sent the metric in from.",
            "title": "Testing"
        },
        {
            "location": "/endpoints/graphite/#simple-example",
            "text": "Start by finding your fingerprint:  cat /etc/dataloop/agent.finger\n\n[private]\nfingerprint = 2afee216-a80f-4f01-9220-14bc3195a3d5  Then simply echo the fingerprint followed by a dot separated metric path followed by your value.  echo \"2afee216-a80f-4f01-9220-14bc3195a3d5.some.metric.path $RANDOM\" | nc -c graphite.dataloop.io 2003  In our example we just sent in a random value. When you login to the Dataloop UI you'll now see some.metric.path show up in the dashboard tree browser and the alerts metric drop-down.  Warning: There are a couple of implementations of netcat. The above example with the -c option was for the GNU implementation. If you are using a BSD implementation (like OSX) then use -q0 instead.",
            "title": "Simple Example"
        },
        {
            "location": "/endpoints/graphite/#troubleshooting",
            "text": "The easiest way to troubleshoot a Graphite backend integration is to open a netcat instance listening on another port. For example  netcat -l -k 2004  Then temporarily redirect your 3rd party software to send to localhost port 2004 tcp. The netcat listener will print out the metrics so you can compare to the format listed above.",
            "title": "Troubleshooting"
        },
        {
            "location": "/endpoints/hosted_statsd/",
            "text": "Hosted StatsD Server\n\n\nIf you are on an enterprise Dataloop plan we'd be happy to manage a hosted StatsD and Grafana 3.0 instance for you. Please contact support to make arrangements.\n\n\nTo use your hosted StatsD server (statsite) simply point your StatsD client at the Dataloop agent in your account. The server address to use is in the format:\n\n\nfingerprint.statsd.dataloop.io\n\n\n\n\nTo find your fingerprint log into your account and look for the 'dataloop' agent on the Setup Monitoring page. Click the little (i) next to it to look up the info.\n\n\n\n\nNow on the agent details page look for the fingerprint of your agent.\n\n\n\n\nIn this example we would configure our StatsD clients to send to:\n\n\n1a202c1d-77a5-4480-8886-c936e4944fb8.statsd.dataloop.io\n\n\n\n\nOn the default UDP port 8125. Your server address will match whatever your fingerprint is set to (fingerprints are unique between agents).\n\n\nYou can then find your StatsD metrics by browsing the dataloop agent or using the statsd tag inside Dataloop.",
            "title": "Hosted statsd"
        },
        {
            "location": "/endpoints/hosted_statsd/#hosted-statsd-server",
            "text": "If you are on an enterprise Dataloop plan we'd be happy to manage a hosted StatsD and Grafana 3.0 instance for you. Please contact support to make arrangements.  To use your hosted StatsD server (statsite) simply point your StatsD client at the Dataloop agent in your account. The server address to use is in the format:  fingerprint.statsd.dataloop.io  To find your fingerprint log into your account and look for the 'dataloop' agent on the Setup Monitoring page. Click the little (i) next to it to look up the info.   Now on the agent details page look for the fingerprint of your agent.   In this example we would configure our StatsD clients to send to:  1a202c1d-77a5-4480-8886-c936e4944fb8.statsd.dataloop.io  On the default UDP port 8125. Your server address will match whatever your fingerprint is set to (fingerprints are unique between agents).  You can then find your StatsD metrics by browsing the dataloop agent or using the statsd tag inside Dataloop.",
            "title": "Hosted StatsD Server"
        },
        {
            "location": "/endpoints/",
            "text": "Graphite, StatsD & InfluxDB\n\n\nInfluxDB\n\n\nCollectD\n\n\nStatsD\n\n\nGraphite\n\n\nHosted StatsD Server\n\n\nSelf-Hosted StatsD\n\n\nSelf-Hosted Statsite\n\n\nStatsD Clients",
            "title": "Home"
        },
        {
            "location": "/endpoints/#graphite-statsd-influxdb",
            "text": "InfluxDB  CollectD  StatsD  Graphite  Hosted StatsD Server  Self-Hosted StatsD  Self-Hosted Statsite  StatsD Clients",
            "title": "Graphite, StatsD &amp; InfluxDB"
        },
        {
            "location": "/endpoints/influxdb/",
            "text": "InfluxDB\n\n\nYou can send InfluxDB metrics into Dataloop using the following url:\n\n\nhttp://fingerprint@influxdb.dataloop.io:8086\n\n\nWhere fingerprint is the string found in \n/etc/dataloop/agent.finger\n on a server.",
            "title": "Influxdb"
        },
        {
            "location": "/endpoints/influxdb/#influxdb",
            "text": "You can send InfluxDB metrics into Dataloop using the following url:  http://fingerprint@influxdb.dataloop.io:8086  Where fingerprint is the string found in  /etc/dataloop/agent.finger  on a server.",
            "title": "InfluxDB"
        },
        {
            "location": "/endpoints/self-hosted_statsd/",
            "text": "Self-Hosted StatsD Server\n\n\nStatsD is a service that aggregates metrics over time. We plan to provide hosted StatsD in the future, but for now you'll need to setup StatsD on your own servers and point them at the Dataloop graphite port.\n\n\nLike everything in Dataloop you need to bind your metrics to a fingerprint. The easiest way to do this may be to setup an AWS micro instance and install an agent on it. You could give the server a host name like 'statsd' so that the agent appears in Dataloop with a friendly name. Drag this agent into the appropriate part of your hierarchy.\n\n\nInstalling StatsD\n\n\n\n\nClone the statsd repository from Etsy's Github account\n\n\n\n\ngit clone https://github.com/etsy/statsd.git\n\n\n\n\n\n\nCopy \nexampleConfig.js\n to \nconfig.js\n and edit the file so the bottom section is like this:\n\n\n\n\n{\n graphitePort: 2003\n, graphiteHost: \"graphite.dataloop.io\"\n, port: 8125\n, graphite: { legacyNamespace: false, globalPrefix: \"FINGERPRINT\" }\n, backends: [ \"./backends/graphite\" ]\n, deleteIdleStats: true\n, debug: true\n}\n\n\n\n\nWhere \nFINGERPRINT\n is the string found in \n/etc/dataloop/agent.finger\n. Often people will install the Dataloop Agent on their StatsD server and use this fingerprint. But you could bind your StatsD metrics to any fingerprint.\n\n\nThen start statsd by running:\n\n\nnode stats.js config.js\n\n\n\n\n\n\nOpen Dataloop in a browser. Click on the dashboards tab and then browse down to your StatsD agent in the tree. You should see some metrics already streaming in.\n\n\n\n\nTesting\n\n\nFrom on your StatsD server itself you can run:\n\n\necho \"foo:1|c\" | nc -u -w0 127.0.0.1 8125",
            "title": "Self hosted statsd"
        },
        {
            "location": "/endpoints/self-hosted_statsd/#self-hosted-statsd-server",
            "text": "StatsD is a service that aggregates metrics over time. We plan to provide hosted StatsD in the future, but for now you'll need to setup StatsD on your own servers and point them at the Dataloop graphite port.  Like everything in Dataloop you need to bind your metrics to a fingerprint. The easiest way to do this may be to setup an AWS micro instance and install an agent on it. You could give the server a host name like 'statsd' so that the agent appears in Dataloop with a friendly name. Drag this agent into the appropriate part of your hierarchy.",
            "title": "Self-Hosted StatsD Server"
        },
        {
            "location": "/endpoints/self-hosted_statsd/#installing-statsd",
            "text": "Clone the statsd repository from Etsy's Github account   git clone https://github.com/etsy/statsd.git   Copy  exampleConfig.js  to  config.js  and edit the file so the bottom section is like this:   {\n graphitePort: 2003\n, graphiteHost: \"graphite.dataloop.io\"\n, port: 8125\n, graphite: { legacyNamespace: false, globalPrefix: \"FINGERPRINT\" }\n, backends: [ \"./backends/graphite\" ]\n, deleteIdleStats: true\n, debug: true\n}  Where  FINGERPRINT  is the string found in  /etc/dataloop/agent.finger . Often people will install the Dataloop Agent on their StatsD server and use this fingerprint. But you could bind your StatsD metrics to any fingerprint.  Then start statsd by running:  node stats.js config.js   Open Dataloop in a browser. Click on the dashboards tab and then browse down to your StatsD agent in the tree. You should see some metrics already streaming in.",
            "title": "Installing StatsD"
        },
        {
            "location": "/endpoints/self-hosted_statsd/#testing",
            "text": "From on your StatsD server itself you can run:  echo \"foo:1|c\" | nc -u -w0 127.0.0.1 8125",
            "title": "Testing"
        },
        {
            "location": "/endpoints/self-hosted_statsite/",
            "text": "Self-Hosted Statsite\n\n\nStatsite is an awesome alternative to the Etsy StatsD server implementation. Install as per the instructions here:\n\n\nhttps://github.com/armon/statsite\n\n\nThen create a \nstatsite.conf\n with the following configuration:\n\n\n[statsite]\nbind_address = 0.0.0.0\ntcp_port = 8125\nudp_port = 8125\nlog_level = INFO\nflush_interval = 10\ntimer_eps = 0.01\nset_eps = 0.02\nstream_cmd = python sinks/graphite.py graphite.dataloop.io 2003 fingerprint.\ninput_counter = numStats\nextended_counters = true\n\n\n\n\nThe important line here is the stream_cmd setting. You may need to update the location of the graphite.py file. The last part of the line is the metric prefix that is sent to dataloop. Usually people install a Dataloop agent onto their Statsite server so they can use the fingerprint string from \n/etc/dataloop/agent.finger\n. Remember to append a fullstop to the end of the fingerprint.\n\n\nAn example of a good stream_cmd line is:\n\n\nstream_cmd = python sinks/graphite.py graphite.dataloop.io 2003 529b7fc2-2de5-4525-983a-184c8f43c085.\n\n\n\n\nTroubleshooting\n\n\nThe only reliable way I have found to troubleshoot this is to open up netcat to listen on a new port, then temporarily change statsite.conf to send metrics to it.\n\n\nFor example open a new tab and run netcat to listen on port 2004\n\n\n# nc -l 2004\n\n\n\n\nNow change the stream_cmd to point to port 2004 localhost\n\n\nstream_cmd = python sinks/graphite.py localhost 2004 529b7fc2-2de5-4525-983a-184c8f43c085.\n\n\n\n\nStart statsite and wait for 10 seconds. Your netcat listening tab will show you the metrics being sent. An example of some that look correct:\n\n\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.sum 57619.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.sum_sq 571869.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.mean 9.741167 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.lower 1.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.upper 10.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.count 5915 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.stdev 1.338330 1436185723\n\n\n\n\nHere we can see the fingerprint first, then the dot seperator between the metric paths followed by the value and timestamp. If you have a malformed \nfingerprint.metric.path\n then metrics won't display in Dataloop\n\n\nOnce you have confirmed it's working against localhost port 2004 you can flip it back to sending to dataloop. Metrics will appear under the agent fingerprint that you used.\n\n\nAlso, be careful about the time on your Statsite server. Installing NTP should help with this. Dataloop will display whatever values against the timestamp you send in, so if your timestamps are wrong strangeness abounds.\n\n\nSending to Graphite + Dataloop\n\n\nSometimes people want to keep their local Graphite server running in parallel. You can do that quite easily with Statsite by changing the \nsink/graphite.py\n that they provide to a modified version.\n\n\nGrab the modified one from here: \ngist\n\n\nYou'll need to modify the \nself.fingerprint\n variable in the init script with a valid agent fingerprint as discussed above.",
            "title": "Self hosted statsite"
        },
        {
            "location": "/endpoints/self-hosted_statsite/#self-hosted-statsite",
            "text": "Statsite is an awesome alternative to the Etsy StatsD server implementation. Install as per the instructions here:  https://github.com/armon/statsite  Then create a  statsite.conf  with the following configuration:  [statsite]\nbind_address = 0.0.0.0\ntcp_port = 8125\nudp_port = 8125\nlog_level = INFO\nflush_interval = 10\ntimer_eps = 0.01\nset_eps = 0.02\nstream_cmd = python sinks/graphite.py graphite.dataloop.io 2003 fingerprint.\ninput_counter = numStats\nextended_counters = true  The important line here is the stream_cmd setting. You may need to update the location of the graphite.py file. The last part of the line is the metric prefix that is sent to dataloop. Usually people install a Dataloop agent onto their Statsite server so they can use the fingerprint string from  /etc/dataloop/agent.finger . Remember to append a fullstop to the end of the fingerprint.  An example of a good stream_cmd line is:  stream_cmd = python sinks/graphite.py graphite.dataloop.io 2003 529b7fc2-2de5-4525-983a-184c8f43c085.",
            "title": "Self-Hosted Statsite"
        },
        {
            "location": "/endpoints/self-hosted_statsite/#troubleshooting",
            "text": "The only reliable way I have found to troubleshoot this is to open up netcat to listen on a new port, then temporarily change statsite.conf to send metrics to it.  For example open a new tab and run netcat to listen on port 2004  # nc -l 2004  Now change the stream_cmd to point to port 2004 localhost  stream_cmd = python sinks/graphite.py localhost 2004 529b7fc2-2de5-4525-983a-184c8f43c085.  Start statsite and wait for 10 seconds. Your netcat listening tab will show you the metrics being sent. An example of some that look correct:  529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.sum 57619.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.sum_sq 571869.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.mean 9.741167 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.lower 1.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.upper 10.000000 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.count 5915 1436185723\n529b7fc2-2de5-4525-983a-184c8f43c085.production.agent.flush.size.stdev 1.338330 1436185723  Here we can see the fingerprint first, then the dot seperator between the metric paths followed by the value and timestamp. If you have a malformed  fingerprint.metric.path  then metrics won't display in Dataloop  Once you have confirmed it's working against localhost port 2004 you can flip it back to sending to dataloop. Metrics will appear under the agent fingerprint that you used.  Also, be careful about the time on your Statsite server. Installing NTP should help with this. Dataloop will display whatever values against the timestamp you send in, so if your timestamps are wrong strangeness abounds.",
            "title": "Troubleshooting"
        },
        {
            "location": "/endpoints/self-hosted_statsite/#sending-to-graphite-dataloop",
            "text": "Sometimes people want to keep their local Graphite server running in parallel. You can do that quite easily with Statsite by changing the  sink/graphite.py  that they provide to a modified version.  Grab the modified one from here:  gist  You'll need to modify the  self.fingerprint  variable in the init script with a valid agent fingerprint as discussed above.",
            "title": "Sending to Graphite + Dataloop"
        },
        {
            "location": "/endpoints/statsd/",
            "text": "Statsd\n\n\nOnce you have your \nStatsD Server\n and \nStatsD Client\n setup you can start sending metrics to Dataloop. This document explains how to use StatsD.\n\n\nThe information below is from: \nhttps://github.com/b/statsd_spec\n\n\nStatsD Metrics Export Specification v0.1\n\n\nThis document describes current practice for the implementation of the various incarnations of the StatsD metric collection protocol. The protocol originated at Flickr, was further developed at Etsy, and has been subsequently influenced by Coda Hale's Metrics. The intent of this document is not to specify or enforce a standard, but only to act as a snapshot of current practice and a guide to ease implementation.\n\n\nTerminology\n\n\nStatsD is used to collect metrics from infrastructure. It is push-based: clients export metrics to a collection server, which in turn derives aggregate metrics and often drives graphing systems such as Graphite. Few assumptions are made about how the data is processed or exposed.\n\n\nThe terms metrics and infrastructure are both defined broadly. A metric is a measurement composed of a name, a value, a type, and sometimes additional information describing how a metric should be interpreted. Infrastructure is any part of a technology stack, from datacenter UPS controllers and temperature sensors in servers all the way up to function calls in applications and user interactions in a browser.\n\n\nIf it can be structured as one of the metric types below, it can consumed by StatsD.\n\n\nMetric Types & Formats\n\n\nThe format of exported metrics is UTF-8 text, with metrics separated by newlines. Metrics are generally of the form \n<metric name>:<value>|<type>\n, with exceptions noted in the metric type definitions below.\n\n\nThe protocol allows for both integer and floating point values. Most implementations store values internally as a IEEE 754 double precision float, but many implementations and graphing systems only support integer values. For compatibility all values should be integers in the range \n(-2^53^, 2^53^)\n.\n\n\nGauges\n\n\nA gauge is an instantaneous measurement of a value, like the gas gauge in a car. It differs from a counter by being calculated at the client rather than the server. Valid gauge values are in the range \n[0, 2^64^)\n\n\n<metric name>:<value>|g\n\n\n\n\nCounters\n\n\nA counter is a gauge calculated at the server. Metrics sent by the client increment or decrement the value of the gauge rather than giving its current value. Counters may also have an associated sample rate, given as a decimal of the number of samples per event count. For example, a sample rate of 1/10 would be exported as 0.1. Valid counter values are in the range (-2^63^, 2^63^).\n\n\n<metric name>:<value>|c[|@<sample rate>]\n\n\n\n\nTimers\n\n\nA timer is a measure of the number of milliseconds elapsed between a start and end time, for example the time to complete rendering of a web page for a user. Valid timer values are in the range [0, 2^64^).\n\n\n<metric name>:<value>|ms\n\n\n\n\nHistograms\n\n\nA histogram is a measure of the distribution of timer values over time, calculated at the server. As the data exported for timers and histograms is the same, this is currently an alias for a timer. Valid histogram values are in the range [0, 2^64^).\n\n\n<metric name>:<value>|h\n\n\n\n\nMeters\n\n\nA meter measures the rate of events over time, calculated at the server. They may also be thought of as increment-only counters. Valid meter values are in the range [0, 2^64^).\n\n\n<metric name>:<value>|m\n\n\n\n\nIn at least one implementation, this is abbreviated for the common case of incrementing the meter by 1.\n\n\n<metric name>\n\n\n\n\nWhile this is convenient, the full, explicit metric form should be used. The shortened form is documented here for completeness.",
            "title": "Statsd"
        },
        {
            "location": "/endpoints/statsd/#statsd",
            "text": "Once you have your  StatsD Server  and  StatsD Client  setup you can start sending metrics to Dataloop. This document explains how to use StatsD.  The information below is from:  https://github.com/b/statsd_spec",
            "title": "Statsd"
        },
        {
            "location": "/endpoints/statsd/#statsd-metrics-export-specification-v01",
            "text": "This document describes current practice for the implementation of the various incarnations of the StatsD metric collection protocol. The protocol originated at Flickr, was further developed at Etsy, and has been subsequently influenced by Coda Hale's Metrics. The intent of this document is not to specify or enforce a standard, but only to act as a snapshot of current practice and a guide to ease implementation.",
            "title": "StatsD Metrics Export Specification v0.1"
        },
        {
            "location": "/endpoints/statsd/#terminology",
            "text": "StatsD is used to collect metrics from infrastructure. It is push-based: clients export metrics to a collection server, which in turn derives aggregate metrics and often drives graphing systems such as Graphite. Few assumptions are made about how the data is processed or exposed.  The terms metrics and infrastructure are both defined broadly. A metric is a measurement composed of a name, a value, a type, and sometimes additional information describing how a metric should be interpreted. Infrastructure is any part of a technology stack, from datacenter UPS controllers and temperature sensors in servers all the way up to function calls in applications and user interactions in a browser.  If it can be structured as one of the metric types below, it can consumed by StatsD.",
            "title": "Terminology"
        },
        {
            "location": "/endpoints/statsd/#metric-types-formats",
            "text": "The format of exported metrics is UTF-8 text, with metrics separated by newlines. Metrics are generally of the form  <metric name>:<value>|<type> , with exceptions noted in the metric type definitions below.  The protocol allows for both integer and floating point values. Most implementations store values internally as a IEEE 754 double precision float, but many implementations and graphing systems only support integer values. For compatibility all values should be integers in the range  (-2^53^, 2^53^) .",
            "title": "Metric Types &amp; Formats"
        },
        {
            "location": "/endpoints/statsd/#gauges",
            "text": "A gauge is an instantaneous measurement of a value, like the gas gauge in a car. It differs from a counter by being calculated at the client rather than the server. Valid gauge values are in the range  [0, 2^64^)  <metric name>:<value>|g",
            "title": "Gauges"
        },
        {
            "location": "/endpoints/statsd/#counters",
            "text": "A counter is a gauge calculated at the server. Metrics sent by the client increment or decrement the value of the gauge rather than giving its current value. Counters may also have an associated sample rate, given as a decimal of the number of samples per event count. For example, a sample rate of 1/10 would be exported as 0.1. Valid counter values are in the range (-2^63^, 2^63^).  <metric name>:<value>|c[|@<sample rate>]",
            "title": "Counters"
        },
        {
            "location": "/endpoints/statsd/#timers",
            "text": "A timer is a measure of the number of milliseconds elapsed between a start and end time, for example the time to complete rendering of a web page for a user. Valid timer values are in the range [0, 2^64^).  <metric name>:<value>|ms",
            "title": "Timers"
        },
        {
            "location": "/endpoints/statsd/#histograms",
            "text": "A histogram is a measure of the distribution of timer values over time, calculated at the server. As the data exported for timers and histograms is the same, this is currently an alias for a timer. Valid histogram values are in the range [0, 2^64^).  <metric name>:<value>|h",
            "title": "Histograms"
        },
        {
            "location": "/endpoints/statsd/#meters",
            "text": "A meter measures the rate of events over time, calculated at the server. They may also be thought of as increment-only counters. Valid meter values are in the range [0, 2^64^).  <metric name>:<value>|m  In at least one implementation, this is abbreviated for the common case of incrementing the meter by 1.  <metric name>  While this is convenient, the full, explicit metric form should be used. The shortened form is documented here for completeness.",
            "title": "Meters"
        },
        {
            "location": "/endpoints/statsd_clients/",
            "text": "StatsD Clients\n\n\nStatsD enables developers to instrument server side code and send metrics to Dataloop for visualisation and alerting. Data is sent using UDP so network issues will never slow your application down.\n\n\nThings you need to get started\n\n\n\n\nConnection details for a StatsD server\n\n\nA StatsD client library\n\n\n\n\nBasic one-time setup\n\n\nEach accounts gets a docker container hosting a StatsD server. \nDetails for hosted StatsD can be found here\n. Alternatively, you can setup your own StatsD server following our \nconfig guide\n.\n\n\nOnce you have a server setup find an appropriate StatsD Client for the language you are using.\n\n\nFor our example we'll use Python and the \npystatsd\n library. Full documentation can be found \nhere\n\n\nAll StatsD client libraries will need to be configured with a StatsD server and port to connect to. In our example we've spun up a StatsD server at demo-statsd.dataloop.io on port 8125 udp.\n\n\nConfiguration within the app is as simple as adding:\n\n\nfrom statsd import StatsClient\n\nstatsd = StatsClient(host='demo=statsd.dataloop.io',\n                     port=8125,\n                     prefix=None,\n                     maxudpsize=512)\n\n\n\n\nThat's pretty much it. You can now use the statsd object in your code to start sending metrics.",
            "title": "Statsd clients"
        },
        {
            "location": "/endpoints/statsd_clients/#statsd-clients",
            "text": "StatsD enables developers to instrument server side code and send metrics to Dataloop for visualisation and alerting. Data is sent using UDP so network issues will never slow your application down.",
            "title": "StatsD Clients"
        },
        {
            "location": "/endpoints/statsd_clients/#things-you-need-to-get-started",
            "text": "Connection details for a StatsD server  A StatsD client library",
            "title": "Things you need to get started"
        },
        {
            "location": "/endpoints/statsd_clients/#basic-one-time-setup",
            "text": "Each accounts gets a docker container hosting a StatsD server.  Details for hosted StatsD can be found here . Alternatively, you can setup your own StatsD server following our  config guide .  Once you have a server setup find an appropriate StatsD Client for the language you are using.  For our example we'll use Python and the  pystatsd  library. Full documentation can be found  here  All StatsD client libraries will need to be configured with a StatsD server and port to connect to. In our example we've spun up a StatsD server at demo-statsd.dataloop.io on port 8125 udp.  Configuration within the app is as simple as adding:  from statsd import StatsClient\n\nstatsd = StatsClient(host='demo=statsd.dataloop.io',\n                     port=8125,\n                     prefix=None,\n                     maxudpsize=512)  That's pretty much it. You can now use the statsd object in your code to start sending metrics.",
            "title": "Basic one-time setup"
        },
        {
            "location": "/getting_started/API/",
            "text": "API Documentation\n\n\nYou can view our API docs, hosted with postman here:\n\n\nAPI Docs",
            "title": "API"
        },
        {
            "location": "/getting_started/API/#api-documentation",
            "text": "You can view our API docs, hosted with postman here:  API Docs",
            "title": "API Documentation"
        },
        {
            "location": "/getting_started/command_line_utility/",
            "text": "Command Line Utility (DLCLI)\n\n\nUp to date installation instructions can be found here:\n\n\nhttps://github.com/dataloop/dlcli/blob/master/README.rst",
            "title": "Command line utility"
        },
        {
            "location": "/getting_started/command_line_utility/#command-line-utility-dlcli",
            "text": "Up to date installation instructions can be found here:  https://github.com/dataloop/dlcli/blob/master/README.rst",
            "title": "Command Line Utility (DLCLI)"
        },
        {
            "location": "/getting_started/data_retention_policy/",
            "text": "Data Retention Policy\n\n\nDataloop supports Nagios check scripts which send back metrics at 30 second intervals (by default) and Graphite / StatsD metrics which are 1 second resolution.\n\n\nWe store:\n\n\n* 1 second resolution for 48 hours\n* 1 minute resolution for 60 days\n* 1 hour resolution for 10 years\n\n\n\nThe 1 second resolution metrics are great for troubleshooting issues as they happen. They also allow us to alert extremely quickly if something does break.\n\n\nHolding 1 minute resolution for 60 days provides a high level of detail for dashboards, reports and looking at trends.\n\n\nWe will effectively keep every metrics you enter into Dataloop forever at a 1 hour resolution.",
            "title": "Data retention policy"
        },
        {
            "location": "/getting_started/data_retention_policy/#data-retention-policy",
            "text": "Dataloop supports Nagios check scripts which send back metrics at 30 second intervals (by default) and Graphite / StatsD metrics which are 1 second resolution.  We store:  * 1 second resolution for 48 hours\n* 1 minute resolution for 60 days\n* 1 hour resolution for 10 years  The 1 second resolution metrics are great for troubleshooting issues as they happen. They also allow us to alert extremely quickly if something does break.  Holding 1 minute resolution for 60 days provides a high level of detail for dashboards, reports and looking at trends.  We will effectively keep every metrics you enter into Dataloop forever at a 1 hour resolution.",
            "title": "Data Retention Policy"
        },
        {
            "location": "/getting_started/faq/",
            "text": "FAQ\n\n\nDataloop.IO was built to be the fastest and simplest way to setup monitoring for your rapidly changing infrastructure. We\u2019re constantly looking for ways to help users get up to speed in minutes and this FAQ is our first step.\n\n\n\n\nQ. My question isn\u2019t covered here, where can I get help?\n\n\n\n\nA. Email us at support@dataloop.io or join our Slack room\n\n\n\n\n\n\nQ. I signed up and logged in for the first time but I\u2019m confused. How  do I start monitoring?\n\n\n\n\nA. The best way to start is to install an agent via curl onto a test virtual machine and have a play with installing packs and writing your own plugins. If you get stuck contact support and we'll happily guide you through everything.\n\n\n\n\n\n\nQ. I'm sending metrics into Dataloop. How do I browse them?\n\n\n\n\nA. You can browse your metrics on the dashboards and rules pages.\n\n\n\n\nCreate a new dashboard and then click on the + widget to bring up the metric browser. Click on one tag, or a combination of tags, or a single agent and click 'select'. You will now be shown a tree of metrics available across the agents selected.\n\n\nThe metrics you see will be determined by the Nagios plugins running and the Graphite or StatsD metrics you are sending in. You can now tick one or many metrics to display on graph and number widgets.\n\n\nOn the rules page add a new rule and then add a new criteria. You will be presented with several drop downs that allow metric browsing and selection.\n\n\n\n\nQ. I can't see my StatsD metrics. Where should they appear?\n\n\n\n\nA. If you are using the Dataloop hosted StatsD then metrics can be browsed under the 'statsd' tag, or by browsing the 'dataloop' agent directly. If you cannot see your StatsD metrics in the tree then check your client settings or contact us for support by email or Slack.\n\n\n\n\nIf you have setup your own StatsD server then metrics will appear under whatever agent fingerprint they are being sent from.\n\n\n\n\nQ. How do I troubleshoot what the Agent is doing?\n\n\n\n\nA. You\u2019ll hopefully never have to troubleshoot, but just in case..\n\n\n\n\nThe Agent installer creates a \u2018dataloop\u2019 directory in /var/log. In this directory you\u2019ll find an agent.log file that shows you exactly what the agent is doing.\n\n\nYou can increase the logging information by turning on debugging. In the agent.yaml configuration file, set debug to yes:\n\n\ndebug: yes\n\n\n\n\nTo restart the agent just run:\n\n\nservice dataloop-agent restart\n\n\n\n\nOn Windows the installer creates a service called \u2018Dataloop\u2019. To restart the agent just restart the Windows service from computer management. Windows puts the log file in C:\\Dataloop.\n\n\n\n\nQ. How does the Agent connect to the web interface? Do I need to open any ports?\n\n\n\n\nA. The agent connects outbound on tcp port 443 to agent.dataloop.io.\n\n\n\n\n\n\nQ. Can I connect the Agent to the internet via a proxy?\n\n\n\n\nA. Yes, export the HTTPS_PROXY environment variable in /etc/default/dataloop-agent and restart the dataloop-agent service\n\n\n\n\n\n\nQ. What data are you collecting from me?\n\n\n\n\nA. The agent sends the hostname, mac address, fingerprint (a random unique string that we create to identify your server) and the data from your scripts. We also send back a list of processes (process.name from the psutils library) and some metadata.\n\n\n\n\nWe use Nagios format scripts so the output is limited to 0,1,2 and 3 for alert status and something=value for the performance metrics. What gets sent back will largely be decided by you when you assign plugins.\n\n\n\n\nQ. How exactly do you keep up with the rate of change in my dynamic environments?\n\n\n\n\nA. All configuration is driven by tags. Simply run dataloop-agent --add-tags on the command line (or automate it via config management). Plugin deployment, dashboards and alerts are all bound to tags so purely by manipulating what tags your agent belongs to we keep up with change.\n\n\n\n\nWe provide secure agent registration, de-registration and presence detection. Simply run dataloop-agent --deregister to remove your agent from the system.\n\n\nWe also support agent finger printing so that you can recreate nodes and reattach to old metrics. This is especially useful for phoenix deployments.\n\n\nBy default agents will deregister on graceful shutdown which works well on AWS with auto scaling groups. You can spin up and down instances and your dashboards and alert rules will dynamically update.\n\n\n\n\nQ. Where is your product roadmap?\n\n\n\n\nA. We are very customer focused and are constantly iterating the product based on feedback. You can see and vote on which features you would like to see in Dataloop here: https://dataloop.uservoice.com/forums/289987-general/filters/top\n\n\n\n\n\n\nQ. What are shells?\n\n\n\n\nA. Something we added mostly for Windows.\n\n\n\n\nLinux and OSX run scripts with a shebang at the top of the file. Windows treats everything as if it\u2019s a .cmd or a .exe. For those wanting to run Powershell or WSF / vbs / js files you can setup shell paths under the settings page. You could for instance setup a WSF shell pointing to c:\\windows\\system32\\cscript.exe and apply it to your WSF script using the shell dropdown in the editor.\n\n\nShells can also be used on Linux / OSX and we\u2019ve found them very helpful for running PhantomJS scripts.\n\n\n\n\nQ. How do I remove the agent?\n\n\n\n\nA. On Linux remove the dataloop-agent package\n\n\n\n\nOn Windows remove the agent via the add/remove programs interface.\n\n\n\n\nQ. What\u2019s the quickest way to write Nagios plugins?\n\n\n\n\nA. If you\u2019re using Linux then you\u2019re spoilt for choice as you have Bash, Ruby and Python installed by default. Pick your favourite scripting language and start writing.\n\n\n\n\nOn Windows you can use batch, or setup a shell for Powershell. Or, alternately use our built in Python interpreter by selecting 'Built-In Python' from the drop down at the top of the plugin editor page.\n\n\nYou can find the Nagios Plugin Development Guide here: link\n\n\nIf you need any help with writing scripts let us know.\n\n\n\n\nQ. Will you support other operating systems and architectures?\n\n\n\n\nA. We\u2019ll probably only ever create installers for Linux and Windows. However, for those that want to install our agent on more exotic platforms (like Raspberry Pi or AIX / HP-UX / Solaris etc) we may be open to providing special builds. The agent is written in Python so in theory should run everywhere. If you\u2019d like another operating system package please contact us.\n\n\n\n\n\n\nQ. How can I secure the Agent?\n\n\n\n\nA. We have a support page for security.\n\n\n\n\n\n\nQ. The share dashboard button has gone missing. How can I get it back?\n\n\n\n\nA. This can happen when you have browser extensions like Adblock installed",
            "title": "Faq"
        },
        {
            "location": "/getting_started/faq/#faq",
            "text": "Dataloop.IO was built to be the fastest and simplest way to setup monitoring for your rapidly changing infrastructure. We\u2019re constantly looking for ways to help users get up to speed in minutes and this FAQ is our first step.   Q. My question isn\u2019t covered here, where can I get help?   A. Email us at support@dataloop.io or join our Slack room    Q. I signed up and logged in for the first time but I\u2019m confused. How  do I start monitoring?   A. The best way to start is to install an agent via curl onto a test virtual machine and have a play with installing packs and writing your own plugins. If you get stuck contact support and we'll happily guide you through everything.    Q. I'm sending metrics into Dataloop. How do I browse them?   A. You can browse your metrics on the dashboards and rules pages.   Create a new dashboard and then click on the + widget to bring up the metric browser. Click on one tag, or a combination of tags, or a single agent and click 'select'. You will now be shown a tree of metrics available across the agents selected.  The metrics you see will be determined by the Nagios plugins running and the Graphite or StatsD metrics you are sending in. You can now tick one or many metrics to display on graph and number widgets.  On the rules page add a new rule and then add a new criteria. You will be presented with several drop downs that allow metric browsing and selection.   Q. I can't see my StatsD metrics. Where should they appear?   A. If you are using the Dataloop hosted StatsD then metrics can be browsed under the 'statsd' tag, or by browsing the 'dataloop' agent directly. If you cannot see your StatsD metrics in the tree then check your client settings or contact us for support by email or Slack.   If you have setup your own StatsD server then metrics will appear under whatever agent fingerprint they are being sent from.   Q. How do I troubleshoot what the Agent is doing?   A. You\u2019ll hopefully never have to troubleshoot, but just in case..   The Agent installer creates a \u2018dataloop\u2019 directory in /var/log. In this directory you\u2019ll find an agent.log file that shows you exactly what the agent is doing.  You can increase the logging information by turning on debugging. In the agent.yaml configuration file, set debug to yes:  debug: yes  To restart the agent just run:  service dataloop-agent restart  On Windows the installer creates a service called \u2018Dataloop\u2019. To restart the agent just restart the Windows service from computer management. Windows puts the log file in C:\\Dataloop.   Q. How does the Agent connect to the web interface? Do I need to open any ports?   A. The agent connects outbound on tcp port 443 to agent.dataloop.io.    Q. Can I connect the Agent to the internet via a proxy?   A. Yes, export the HTTPS_PROXY environment variable in /etc/default/dataloop-agent and restart the dataloop-agent service    Q. What data are you collecting from me?   A. The agent sends the hostname, mac address, fingerprint (a random unique string that we create to identify your server) and the data from your scripts. We also send back a list of processes (process.name from the psutils library) and some metadata.   We use Nagios format scripts so the output is limited to 0,1,2 and 3 for alert status and something=value for the performance metrics. What gets sent back will largely be decided by you when you assign plugins.   Q. How exactly do you keep up with the rate of change in my dynamic environments?   A. All configuration is driven by tags. Simply run dataloop-agent --add-tags on the command line (or automate it via config management). Plugin deployment, dashboards and alerts are all bound to tags so purely by manipulating what tags your agent belongs to we keep up with change.   We provide secure agent registration, de-registration and presence detection. Simply run dataloop-agent --deregister to remove your agent from the system.  We also support agent finger printing so that you can recreate nodes and reattach to old metrics. This is especially useful for phoenix deployments.  By default agents will deregister on graceful shutdown which works well on AWS with auto scaling groups. You can spin up and down instances and your dashboards and alert rules will dynamically update.   Q. Where is your product roadmap?   A. We are very customer focused and are constantly iterating the product based on feedback. You can see and vote on which features you would like to see in Dataloop here: https://dataloop.uservoice.com/forums/289987-general/filters/top    Q. What are shells?   A. Something we added mostly for Windows.   Linux and OSX run scripts with a shebang at the top of the file. Windows treats everything as if it\u2019s a .cmd or a .exe. For those wanting to run Powershell or WSF / vbs / js files you can setup shell paths under the settings page. You could for instance setup a WSF shell pointing to c:\\windows\\system32\\cscript.exe and apply it to your WSF script using the shell dropdown in the editor.  Shells can also be used on Linux / OSX and we\u2019ve found them very helpful for running PhantomJS scripts.   Q. How do I remove the agent?   A. On Linux remove the dataloop-agent package   On Windows remove the agent via the add/remove programs interface.   Q. What\u2019s the quickest way to write Nagios plugins?   A. If you\u2019re using Linux then you\u2019re spoilt for choice as you have Bash, Ruby and Python installed by default. Pick your favourite scripting language and start writing.   On Windows you can use batch, or setup a shell for Powershell. Or, alternately use our built in Python interpreter by selecting 'Built-In Python' from the drop down at the top of the plugin editor page.  You can find the Nagios Plugin Development Guide here: link  If you need any help with writing scripts let us know.   Q. Will you support other operating systems and architectures?   A. We\u2019ll probably only ever create installers for Linux and Windows. However, for those that want to install our agent on more exotic platforms (like Raspberry Pi or AIX / HP-UX / Solaris etc) we may be open to providing special builds. The agent is written in Python so in theory should run everywhere. If you\u2019d like another operating system package please contact us.    Q. How can I secure the Agent?   A. We have a support page for security.    Q. The share dashboard button has gone missing. How can I get it back?   A. This can happen when you have browser extensions like Adblock installed",
            "title": "FAQ"
        },
        {
            "location": "/getting_started/getting_help/",
            "text": "Getting Help\n\n\nLogging a Ticket\n\n\nFor long running issues we recommend that you \nlog a support ticket\n via Zendesk web interface so that you can keep track of the status. We try hard to be extremely responsive to tickets.\n\n\n\n\nChat Support\n\n\nFor quick questions or help troubleshooting issues in real time please visit our Slack channel at \nhttps://slack.dataloop.io\n. There's usually somebody online from Dataloop to help out. Feel free to ask us any kind of questions related to monitoring.\n\n\n\n\nE-Mail Support\n\n\nIf you're more comfortable firing off a quick email with a question then send them to \nsupport@dataloop.io\n. This is a group mailbox that our support team looks at and you'll get a reply back as quick as humanly possible. \n\n\nFeel free to send us any type of enquiry to these support channels. From product, bugs, billing, security or any range of question. We're always happy to help.",
            "title": "Getting help"
        },
        {
            "location": "/getting_started/getting_help/#getting-help",
            "text": "",
            "title": "Getting Help"
        },
        {
            "location": "/getting_started/getting_help/#logging-a-ticket",
            "text": "For long running issues we recommend that you  log a support ticket  via Zendesk web interface so that you can keep track of the status. We try hard to be extremely responsive to tickets.",
            "title": "Logging a Ticket"
        },
        {
            "location": "/getting_started/getting_help/#chat-support",
            "text": "For quick questions or help troubleshooting issues in real time please visit our Slack channel at  https://slack.dataloop.io . There's usually somebody online from Dataloop to help out. Feel free to ask us any kind of questions related to monitoring.",
            "title": "Chat Support"
        },
        {
            "location": "/getting_started/getting_help/#e-mail-support",
            "text": "If you're more comfortable firing off a quick email with a question then send them to  support@dataloop.io . This is a group mailbox that our support team looks at and you'll get a reply back as quick as humanly possible.   Feel free to send us any type of enquiry to these support channels. From product, bugs, billing, security or any range of question. We're always happy to help.",
            "title": "E-Mail Support"
        },
        {
            "location": "/getting_started/",
            "text": "Getting Started\n\n\nUse Cases\n\n\nGetting Help\n\n\nOverview of Dataloop\n\n\nAPI\n\n\nCommand Line Utility\n\n\nSecurity\n\n\nData Retention Policy\n\n\nWhere We Fit in the Monitoring Landscape\n\n\nFAQ\n\n\nRoadmap",
            "title": "Home"
        },
        {
            "location": "/getting_started/#getting-started",
            "text": "Use Cases  Getting Help  Overview of Dataloop  API  Command Line Utility  Security  Data Retention Policy  Where We Fit in the Monitoring Landscape  FAQ  Roadmap",
            "title": "Getting Started"
        },
        {
            "location": "/getting_started/overview/",
            "text": "Overview\n\n\nAgent Installation and Tagging\n\n\nYou should plan to install the dataloop-agent on every server. On Linux the agent is packaged up as a deb and rpm to make this simple. We have configuration management repositories for:\n\n\nPuppet\n\n\nChef\n\n\nAnsible\n\n\nSalt\n  \n\n\nOn Windows it's a manual install via the installer. However, this does support silent install if you wish to automate with Powershell.\n\n\nWe advise that you use configuration management to tag your servers so that plugin deployment, dashboards and alerts can be setup in an aggregated way. Each configuration management repository has a README.md that describes how to do the tagging.\n\n\nOur recommended best practices for tags are:\n\n\nproduct_name,environment,role\n\n\n\n\nFor example, production web servers for the acme todo list app would get tagged:\n\n\ntodo,prod,web\n\n\n\n\nThen when a product manager comes along and says.. we should do some kind of funky calendar app next. You can spin up new product environments with:\n\n\ncalendar,prod,web\n\n\n\n\nor,\n\n\ncalendar,qa,database\n\n\n\n\nTagging can also be done in the UI although this is only recommended for static environments. You want your servers to spin up with the agent installed and with a set of tags that can be used in Dataloop to setup the rest of the monitoring.\n\n\nYou can of course add any number of tags to your agents. The more you add the better actually. The whole premise of our future auto discovery is that we'll detect what is running on your server and automatically tag it. But feel free to apply tags for versions, colours (blue, green deploy?) or any thing you'd like to use in future to refine how plugins are deployed, or dashboards and alerts are created.\n\n\nIt is important that you have an agent installed on every server, that they are in the right tags and that you keep vigilant for any discrepancies between what's in Dataloop and what is reality. If you get the building blocks correct then the plugins, dashboards and alerts you layer on top will almost guarantee you'll get told when something breaks. If you accidentally don't install agents or put agents into the correct tags then the monitoring is invalid.\n\n\nMetrics Collection\n\n\nOnce you have the agents installed and the tags setup as per section 1. you can start to layer on monitoring coverage.\n\n\nBy default every agent gets put into a tag called \nall\n. This is useful because you can drag plugins onto this tag and they will run on every single server. We use this tag to deploy a script called 'base' which sends back all of the basic operating system metrics you would expect. Things like CPU, Memory, Disk, Network etc. We'd advise you leave 'base' running on the all tag otherwise you won't get alerted when disks fill up or servers max out on CPU.\n\n\nEvery server role that you have probably has something installed on it, like apache, or nginx or mysql or elasticsearch etc. For every role you should ensure that each of the components you depend on has at least one script deployed to monitor it. We provide scripts for most commonly used services. If your service is not listed then you may need to create a plugin to cover it, or request one via email to info@dataloop.io\n\n\nIn our example before we had todo,prod and web tags. These boxes only run Nginx. In the Dataloop UI we would therefore drag the Nginx plugin on top of the 'web' tag so that it will automatically deploy and start sending back metrics.\n\n\nIt is worth noting at this stage that some plugins require some configuration. You should always click \nedit\n on a plugin, select a server to test it on and hit run. The console will show either an error, which needs fixing by either configuration or setting some variables in the script. In the case of variables we tend to put good comments into the script so it's easy to see what variables to update. Commonly altered variables will always be set at the top of the script in uppercase.\n\n\nIn a ideal world you'd have base running on every system via the all tag. Then a plugin deployed for every service you have. Ultimately this will all be auto discovered, but for now you need to assign the correct plugins.\n\n\nYou will also undoubtedly have some things you want to check that we don't have out-of-the-box. For these things write some quick check scripts in the browser, test them via the run button, and drag them onto the tag you wish to run them on to deploy.\n\n\nMonitoring is an ongoing effort. You should aim for good coverage out of the gate, however, you will need to continually add check scripts as services are added, or as gaps in your monitoring are found (usually as a result of incident reviews and postmortems). The more boolean checks and meaningful metrics you have, the better the dashboards and alerts get when you create them.\n\n\nOnce you have good Nagios check script coverage it might be time to think about some other ways to improve your coverage.\n\n\nReal-Time metrics: Not everything needs to be measured in real-time. You'll end up with a lot of coverage from the Nagios check scripts and you probably won't use all of those. We know this because we've got a great deal of experience collecting metrics in realtime that have never been used, ever, and likely never will. The argument for 'collect everything as you can never go back in time and enable it is valid' and for every case I've wanted to look at trends back in time, 30 second resolution check scripts have been good enough.\n\n\nYou can install 3rd party agents like CollectD or BrightCover Diamond, configured with the agent fingerprint as the metrics prefix to send metrics back to Dataloop in realtime. Doing this provides real-time metrics at the expense of additional complexity that needs to be managed outside of Dataloop. Use these tools if you want to spend the time setting them up and would like to create dashboards that update faster and alert on highly time sensitive items. We would however caution that this adds complexity back into your configuration management and typically puts the setup work back onto a select number of people.\n\n\nYou can fire metrics at graphite.dataloop.io at one second resolution. These metrics 'append' the agent metrics, in so far as they bind to an agent fingerprint so they look like they come from the same source as your nagios plugins from that server. We have a support page dedicated to how graphite metrics work.\n\n\nExamples of things you may want realtime metrics on are api response times or message queue statistics. These power the graphs that people stare at when there is an issue, and they expect to see immediate changes when something is fixed. We recommend you run scripts that send these real-time metrics to us from the boxes themselves, hitting localhost. This way they append the Nagios metrics and you can use the same tagging system in Dataloop to aggregate all response times across multiple hosts in a tag in graphs and alerts. You'll then see instantly which box has the problem.\n\n\nYou may want to check the entire service response time from outside. In this case it is valid to run the scripts on external agents, hitting an endpoint like a load balancer. If you do this be careful to identify the agent these metrics are bound to as something easily recognisable as an external check box. Otherwise, you may end up with metrics bound to agents that are not in any way associated with the environment you are monitoring. It is not uncommon for people to setup amazon micro instances in various regions to run these. Why would you do this in preference to something like Pingdom? Well, you get to run scripts and have an entire programming language at your disposal. You can get these agents to constantly run smoke tests against production. Obviously, it's a bit overkill if you do just want to do simple curls.\n\n\nThe agent and plugins are our primary source of monitoring, and the graphite metrics sit on top. The deployment architecture is extremely flexible as you can install an agent anywhere, either inside your network, or outside and monitor either localhost or other servers.\n\n\nApplication Metrics: Nagios metrics poll the operating system and services from the outside. Some services expose a lot of useful metrics that we write scripts to collect. But what about the application that your team wrote? For that there are a couple of options.\n\n\n\n\n\n\nThe developers can expose metrics via a rest endpoint which could be scraped via a plugin. You could either make up your own spec, or use one like this: \n\n\nhttps://github.com/beamly/SE4\n  \n\n\nIf you're in a Java shop the Coda Hale metrics library is awesome. A lot of other languages have similar libraries for exposing internal metrics over an api so that monitoring tools like Dataloop can extract them.  \n\n\n\n\n\n\nStatsD - This is a piece of software that you can install on a central server somewhere (or we can host one for you). You then add a StatsD client library in your code and start to push realtime application level metrics into Dataloop. We use StatsD internally a lot to debug backend processes. Every time you send us a metric we measure how that flows through our system by monitoring various worker stats like inbound, outbound, held metrics etc. On top of counters like that you can also use StatsD to collect business and performance metrics. Timing the duration of functions and metrics is quite fun, especially when you can see what's happening in realtime in production.\n\n\n\n\n\n\nFor large installations we recommend setting up a StatsD server per environment and tagging the agent on the StatsD server with the environment tag it is part of. This then makes metrics browsing easier inside Dataloop.\n\n\nDashboards\n\n\nAlways create dashboards widgets based on tags, or combinations of tags. We let you create them for individual hosts but that's mostly for new people playing. If you create a dashboard of widgets based off a host and you delete that host, you will need to edit all of your widgets. Whereas if you created the dashboard from a tag, the hosts are expendable. You just need to put an agent back into the tag again to resume showing metrics.\n\n\nWe recommend creating a dashboard per environment / role combo. So in our example above we had production web servers for the todo list product. For this I'd create a dashboard called \nTodo Prod Webservers\n and start to add a variety of tag level widgets.\n\n\nThe most important widgets to get onto a dashboard are the status widgets. Add an agent.status so you can visibly see all servers are alive. Beyond this, each plugin has a .status metric you should add. For our example this would be nginx.status. Then for each service there may be various metrics you want to see. For Nginx these might be:\n\n\n\n\n\n\nRequests per second\n\n\n\n\n\n\nConnections\n\n\n\n\n\n\n4xx and 5xx errors\n\n\n\n\n\n\nResponse times\n\n\n\n\n\n\nYou may want to add a few OS metrics too. Then you'll be able to tell if your disks are getting filled, or if you need to buy bigger boxes with more cpu or memory.\n\n\nIt's also a good idea to create some business dashboards. Write scripts that poll systems that have those metrics in them, or use StatsD as mentioned above. As with scripts, the dashboards are an ongoing process. Update them every time you want to keep an eye on something new.\n\n\nRules and Alerts\n\n\nBy default you get an \nall systems\n ruleset with criterial in it that will alert you on the \nbase\n plugin on the all tag. Things like servers going down, cpu, disk, load etc. You should probably leave these as they are and possibly extend them if there are things you want to check across every box.\n\n\nFor each of your services it's probably worth having a catch all ruleset. For instance you might create a ruleset for \nNginx\n and put some criteria in there for the service status being alive and perhaps requests per second being below a certain threshold.\n\n\nYou should create different rulesets depending on who gets alerted. For instance you might create a ruleset for todo,stag,web that simply emails a group email address when alert criteria are triggered. Whereas your ruleset for todo,prod,web might send emails to pagerduty so that people get an SMS and woken up.",
            "title": "Overview"
        },
        {
            "location": "/getting_started/overview/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/getting_started/overview/#agent-installation-and-tagging",
            "text": "You should plan to install the dataloop-agent on every server. On Linux the agent is packaged up as a deb and rpm to make this simple. We have configuration management repositories for:  Puppet  Chef  Ansible  Salt     On Windows it's a manual install via the installer. However, this does support silent install if you wish to automate with Powershell.  We advise that you use configuration management to tag your servers so that plugin deployment, dashboards and alerts can be setup in an aggregated way. Each configuration management repository has a README.md that describes how to do the tagging.  Our recommended best practices for tags are:  product_name,environment,role  For example, production web servers for the acme todo list app would get tagged:  todo,prod,web  Then when a product manager comes along and says.. we should do some kind of funky calendar app next. You can spin up new product environments with:  calendar,prod,web  or,  calendar,qa,database  Tagging can also be done in the UI although this is only recommended for static environments. You want your servers to spin up with the agent installed and with a set of tags that can be used in Dataloop to setup the rest of the monitoring.  You can of course add any number of tags to your agents. The more you add the better actually. The whole premise of our future auto discovery is that we'll detect what is running on your server and automatically tag it. But feel free to apply tags for versions, colours (blue, green deploy?) or any thing you'd like to use in future to refine how plugins are deployed, or dashboards and alerts are created.  It is important that you have an agent installed on every server, that they are in the right tags and that you keep vigilant for any discrepancies between what's in Dataloop and what is reality. If you get the building blocks correct then the plugins, dashboards and alerts you layer on top will almost guarantee you'll get told when something breaks. If you accidentally don't install agents or put agents into the correct tags then the monitoring is invalid.",
            "title": "Agent Installation and Tagging"
        },
        {
            "location": "/getting_started/overview/#metrics-collection",
            "text": "Once you have the agents installed and the tags setup as per section 1. you can start to layer on monitoring coverage.  By default every agent gets put into a tag called  all . This is useful because you can drag plugins onto this tag and they will run on every single server. We use this tag to deploy a script called 'base' which sends back all of the basic operating system metrics you would expect. Things like CPU, Memory, Disk, Network etc. We'd advise you leave 'base' running on the all tag otherwise you won't get alerted when disks fill up or servers max out on CPU.  Every server role that you have probably has something installed on it, like apache, or nginx or mysql or elasticsearch etc. For every role you should ensure that each of the components you depend on has at least one script deployed to monitor it. We provide scripts for most commonly used services. If your service is not listed then you may need to create a plugin to cover it, or request one via email to info@dataloop.io  In our example before we had todo,prod and web tags. These boxes only run Nginx. In the Dataloop UI we would therefore drag the Nginx plugin on top of the 'web' tag so that it will automatically deploy and start sending back metrics.  It is worth noting at this stage that some plugins require some configuration. You should always click  edit  on a plugin, select a server to test it on and hit run. The console will show either an error, which needs fixing by either configuration or setting some variables in the script. In the case of variables we tend to put good comments into the script so it's easy to see what variables to update. Commonly altered variables will always be set at the top of the script in uppercase.  In a ideal world you'd have base running on every system via the all tag. Then a plugin deployed for every service you have. Ultimately this will all be auto discovered, but for now you need to assign the correct plugins.  You will also undoubtedly have some things you want to check that we don't have out-of-the-box. For these things write some quick check scripts in the browser, test them via the run button, and drag them onto the tag you wish to run them on to deploy.  Monitoring is an ongoing effort. You should aim for good coverage out of the gate, however, you will need to continually add check scripts as services are added, or as gaps in your monitoring are found (usually as a result of incident reviews and postmortems). The more boolean checks and meaningful metrics you have, the better the dashboards and alerts get when you create them.  Once you have good Nagios check script coverage it might be time to think about some other ways to improve your coverage.  Real-Time metrics: Not everything needs to be measured in real-time. You'll end up with a lot of coverage from the Nagios check scripts and you probably won't use all of those. We know this because we've got a great deal of experience collecting metrics in realtime that have never been used, ever, and likely never will. The argument for 'collect everything as you can never go back in time and enable it is valid' and for every case I've wanted to look at trends back in time, 30 second resolution check scripts have been good enough.  You can install 3rd party agents like CollectD or BrightCover Diamond, configured with the agent fingerprint as the metrics prefix to send metrics back to Dataloop in realtime. Doing this provides real-time metrics at the expense of additional complexity that needs to be managed outside of Dataloop. Use these tools if you want to spend the time setting them up and would like to create dashboards that update faster and alert on highly time sensitive items. We would however caution that this adds complexity back into your configuration management and typically puts the setup work back onto a select number of people.  You can fire metrics at graphite.dataloop.io at one second resolution. These metrics 'append' the agent metrics, in so far as they bind to an agent fingerprint so they look like they come from the same source as your nagios plugins from that server. We have a support page dedicated to how graphite metrics work.  Examples of things you may want realtime metrics on are api response times or message queue statistics. These power the graphs that people stare at when there is an issue, and they expect to see immediate changes when something is fixed. We recommend you run scripts that send these real-time metrics to us from the boxes themselves, hitting localhost. This way they append the Nagios metrics and you can use the same tagging system in Dataloop to aggregate all response times across multiple hosts in a tag in graphs and alerts. You'll then see instantly which box has the problem.  You may want to check the entire service response time from outside. In this case it is valid to run the scripts on external agents, hitting an endpoint like a load balancer. If you do this be careful to identify the agent these metrics are bound to as something easily recognisable as an external check box. Otherwise, you may end up with metrics bound to agents that are not in any way associated with the environment you are monitoring. It is not uncommon for people to setup amazon micro instances in various regions to run these. Why would you do this in preference to something like Pingdom? Well, you get to run scripts and have an entire programming language at your disposal. You can get these agents to constantly run smoke tests against production. Obviously, it's a bit overkill if you do just want to do simple curls.  The agent and plugins are our primary source of monitoring, and the graphite metrics sit on top. The deployment architecture is extremely flexible as you can install an agent anywhere, either inside your network, or outside and monitor either localhost or other servers.  Application Metrics: Nagios metrics poll the operating system and services from the outside. Some services expose a lot of useful metrics that we write scripts to collect. But what about the application that your team wrote? For that there are a couple of options.    The developers can expose metrics via a rest endpoint which could be scraped via a plugin. You could either make up your own spec, or use one like this:   https://github.com/beamly/SE4     If you're in a Java shop the Coda Hale metrics library is awesome. A lot of other languages have similar libraries for exposing internal metrics over an api so that monitoring tools like Dataloop can extract them.      StatsD - This is a piece of software that you can install on a central server somewhere (or we can host one for you). You then add a StatsD client library in your code and start to push realtime application level metrics into Dataloop. We use StatsD internally a lot to debug backend processes. Every time you send us a metric we measure how that flows through our system by monitoring various worker stats like inbound, outbound, held metrics etc. On top of counters like that you can also use StatsD to collect business and performance metrics. Timing the duration of functions and metrics is quite fun, especially when you can see what's happening in realtime in production.    For large installations we recommend setting up a StatsD server per environment and tagging the agent on the StatsD server with the environment tag it is part of. This then makes metrics browsing easier inside Dataloop.",
            "title": "Metrics Collection"
        },
        {
            "location": "/getting_started/overview/#dashboards",
            "text": "Always create dashboards widgets based on tags, or combinations of tags. We let you create them for individual hosts but that's mostly for new people playing. If you create a dashboard of widgets based off a host and you delete that host, you will need to edit all of your widgets. Whereas if you created the dashboard from a tag, the hosts are expendable. You just need to put an agent back into the tag again to resume showing metrics.  We recommend creating a dashboard per environment / role combo. So in our example above we had production web servers for the todo list product. For this I'd create a dashboard called  Todo Prod Webservers  and start to add a variety of tag level widgets.  The most important widgets to get onto a dashboard are the status widgets. Add an agent.status so you can visibly see all servers are alive. Beyond this, each plugin has a .status metric you should add. For our example this would be nginx.status. Then for each service there may be various metrics you want to see. For Nginx these might be:    Requests per second    Connections    4xx and 5xx errors    Response times    You may want to add a few OS metrics too. Then you'll be able to tell if your disks are getting filled, or if you need to buy bigger boxes with more cpu or memory.  It's also a good idea to create some business dashboards. Write scripts that poll systems that have those metrics in them, or use StatsD as mentioned above. As with scripts, the dashboards are an ongoing process. Update them every time you want to keep an eye on something new.",
            "title": "Dashboards"
        },
        {
            "location": "/getting_started/overview/#rules-and-alerts",
            "text": "By default you get an  all systems  ruleset with criterial in it that will alert you on the  base  plugin on the all tag. Things like servers going down, cpu, disk, load etc. You should probably leave these as they are and possibly extend them if there are things you want to check across every box.  For each of your services it's probably worth having a catch all ruleset. For instance you might create a ruleset for  Nginx  and put some criteria in there for the service status being alive and perhaps requests per second being below a certain threshold.  You should create different rulesets depending on who gets alerted. For instance you might create a ruleset for todo,stag,web that simply emails a group email address when alert criteria are triggered. Whereas your ruleset for todo,prod,web might send emails to pagerduty so that people get an SMS and woken up.",
            "title": "Rules and Alerts"
        },
        {
            "location": "/getting_started/roadmap/",
            "text": "Roadmap\n\n\nOur product roadmap is largely driven by customer feedback and our long term dedication to improving the lives of everyone involved with building and operating SaaS products.\n\n\nIf you have any suggestions please search and vote or raise a new feature idea on our user voice forum.\n\n\nhttps://dataloop.uservoice.com/forums/289987-general/filters/top\n\n\nAlternatively, come and chat with us on \nSlack\n.\n\n\nBelow is high level overview of what we will be working on next. Ordering may change depending on customer priorities.\n\n\nFront End\n\n\n\n\n\n\nGuided setup auto discovery (packs) for server monitoring\n\n\n\n\n\n\nPrivate packs\n\n\n\n\n\n\nWildcard scope selection on dashboard metrics\n\n\n\n\n\n\nAlert UI updates for advanced features\n\n\n\n\n\n\nMetrics browser for instant searching and graphing of dimensional metrics\n\n\n\n\n\n\nGuided setup for developers who don't want to install agents\n\n\n\n\n\n\nGuided setup for container monitoring\n\n\n\n\n\n\nEvent feeds that show user activity across different accounts in an organisation\n\n\n\n\n\n\nDashboard rescoping to allow quick switching between environments\n\n\n\n\n\n\nDashboard rewrite based on metrics browser\n\n\n\n\n\n\nOrg level dashboards (accessing data in multiple accounts)\n\n\n\n\n\n\nNotifications centre with integrations to common incident management and chat tools\n\n\n\n\n\n\nCustom events on dashboards, alerts and details pages\n\n\n\n\n\n\nBack End\n\n\n\n\n\n\nMetric dimensions\n\n\n\n\n\n\nAdvanced queries in alerts\n\n\n\n\n\n\nAlerting on no-data\n\n\n\n\n\n\nPause rules\n\n\n\n\n\n\nView Only member role\n\n\n\n\n\n\nMore query functions (see dalmatinerdb issues list)\n\n\n\n\n\n\nEvents service for storing plugin output, audit log, custom events and annotations\n\n\n\n\n\n\nAlert Handlers that trigger script execution across one or more agents\n\n\n\n\n\n\nHistograms\n\n\n\n\n\n\nLink variables for use in plugins (like Nagios macros)\n\n\n\n\n\n\nCloud integrations: AWS, GCE, Azure\n\n\n\n\n\n\nApp integrations: New Relic, AppDynamics, Splunk, ELK, Pingdom\n\n\n\n\n\n\nSAML single sign on\n\n\n\n\n\n\nOther\n\n\n\n\n\n\nImproved API docs and more features in dlcli\n\n\n\n\n\n\nGrafana 3 plugin\n\n\n\n\n\n\nDocker auto-discovery to collect host metrics and run plugins across containers\n\n\n\n\n\n\nSupport for more platforms (SmartOS, BSD, OSX etc)\n\n\n\n\n\n\nMany more packs and plugins\n\n\n\n\n\n\nGrafana dashboard conversion (both ways)",
            "title": "Roadmap"
        },
        {
            "location": "/getting_started/roadmap/#roadmap",
            "text": "Our product roadmap is largely driven by customer feedback and our long term dedication to improving the lives of everyone involved with building and operating SaaS products.  If you have any suggestions please search and vote or raise a new feature idea on our user voice forum.  https://dataloop.uservoice.com/forums/289987-general/filters/top  Alternatively, come and chat with us on  Slack .  Below is high level overview of what we will be working on next. Ordering may change depending on customer priorities.",
            "title": "Roadmap"
        },
        {
            "location": "/getting_started/roadmap/#front-end",
            "text": "Guided setup auto discovery (packs) for server monitoring    Private packs    Wildcard scope selection on dashboard metrics    Alert UI updates for advanced features    Metrics browser for instant searching and graphing of dimensional metrics    Guided setup for developers who don't want to install agents    Guided setup for container monitoring    Event feeds that show user activity across different accounts in an organisation    Dashboard rescoping to allow quick switching between environments    Dashboard rewrite based on metrics browser    Org level dashboards (accessing data in multiple accounts)    Notifications centre with integrations to common incident management and chat tools    Custom events on dashboards, alerts and details pages",
            "title": "Front End"
        },
        {
            "location": "/getting_started/roadmap/#back-end",
            "text": "Metric dimensions    Advanced queries in alerts    Alerting on no-data    Pause rules    View Only member role    More query functions (see dalmatinerdb issues list)    Events service for storing plugin output, audit log, custom events and annotations    Alert Handlers that trigger script execution across one or more agents    Histograms    Link variables for use in plugins (like Nagios macros)    Cloud integrations: AWS, GCE, Azure    App integrations: New Relic, AppDynamics, Splunk, ELK, Pingdom    SAML single sign on",
            "title": "Back End"
        },
        {
            "location": "/getting_started/roadmap/#other",
            "text": "Improved API docs and more features in dlcli    Grafana 3 plugin    Docker auto-discovery to collect host metrics and run plugins across containers    Support for more platforms (SmartOS, BSD, OSX etc)    Many more packs and plugins    Grafana dashboard conversion (both ways)",
            "title": "Other"
        },
        {
            "location": "/getting_started/security/",
            "text": "Security\n\n\nDataloop Agent\n\n\nThe Dataloop Agent connects outbound on port \n443\n only.\n\n\nInstallation of the agent can be done in a number of ways. Most of our customers use configuration management tools and the repos provided in the public Dataloop Github account. These are extremely simple; they add a repo file, install the dataloop-agent package, modify a config file and start a service.\n\n\nWe provide a curl installer for people who want to give Dataloop a quick try on a test machine. Curl installers by their very nature are not secure, however they are extremely quick (just a simple copy and paste). We strongly advise people stop using the curl installer after they have moved past the test phase.\n\n\nBy default the agent runs as a non privileged user \ndataloop\n which can be locked down further by the operating system if required.\n\n\nThe agent uses an non privileged account key to join Dataloop. On initial start up a key exchange happens and a unique agent identifier is stored in a fingerprint file in the agent config directory. This securely registers your server to Dataloop.\n\n\nData Privacy\n\n\nBy default we only send back basic operating system metrics like cpu, disk, memory, network, and a process list and some metadata about your servers. This metadata includes things like network addresses, environment variables, and metadata from services like AWS and Facter / Ohai. We send all of this data back for the sole purpose of helping you to troubleshoot issues and for auto discovery of services, so that we can automate the setup of your monitoring. We will never share this data with any 3rd party.\n\n\nSolo Mode\n\n\nDataloop has some unique technology that allows teams outside of operations to quickly write plugins and deploy them to groups of servers. We recommend that this feature is enabled and used in development and test so that you get the full value out of our self service capability.\n\n\nIn some cases, usually on production or in more tightly controlled environments you will need to turn off these capabilities. For this scenario we provide \nSolo\n mode which completely disables remote script execution and deployment. This is as simple as updating the agent.yaml with solo: yes and restarting the service.\n\n\nIn Solo Mode the agent no longer polls the Dataloop.IO API for plugin downloads and instead polls the local disk. Drop all of your plugins into the plugins directory and the agent will still only load those configured in the app. This means you keep the benefit of rapid script creation and deployment, while retaining control of what runs on your servers via your normal config management workflow.",
            "title": "Security"
        },
        {
            "location": "/getting_started/security/#security",
            "text": "",
            "title": "Security"
        },
        {
            "location": "/getting_started/security/#dataloop-agent",
            "text": "The Dataloop Agent connects outbound on port  443  only.  Installation of the agent can be done in a number of ways. Most of our customers use configuration management tools and the repos provided in the public Dataloop Github account. These are extremely simple; they add a repo file, install the dataloop-agent package, modify a config file and start a service.  We provide a curl installer for people who want to give Dataloop a quick try on a test machine. Curl installers by their very nature are not secure, however they are extremely quick (just a simple copy and paste). We strongly advise people stop using the curl installer after they have moved past the test phase.  By default the agent runs as a non privileged user  dataloop  which can be locked down further by the operating system if required.  The agent uses an non privileged account key to join Dataloop. On initial start up a key exchange happens and a unique agent identifier is stored in a fingerprint file in the agent config directory. This securely registers your server to Dataloop.",
            "title": "Dataloop Agent"
        },
        {
            "location": "/getting_started/security/#data-privacy",
            "text": "By default we only send back basic operating system metrics like cpu, disk, memory, network, and a process list and some metadata about your servers. This metadata includes things like network addresses, environment variables, and metadata from services like AWS and Facter / Ohai. We send all of this data back for the sole purpose of helping you to troubleshoot issues and for auto discovery of services, so that we can automate the setup of your monitoring. We will never share this data with any 3rd party.",
            "title": "Data Privacy"
        },
        {
            "location": "/getting_started/security/#solo-mode",
            "text": "Dataloop has some unique technology that allows teams outside of operations to quickly write plugins and deploy them to groups of servers. We recommend that this feature is enabled and used in development and test so that you get the full value out of our self service capability.  In some cases, usually on production or in more tightly controlled environments you will need to turn off these capabilities. For this scenario we provide  Solo  mode which completely disables remote script execution and deployment. This is as simple as updating the agent.yaml with solo: yes and restarting the service.  In Solo Mode the agent no longer polls the Dataloop.IO API for plugin downloads and instead polls the local disk. Drop all of your plugins into the plugins directory and the agent will still only load those configured in the app. This means you keep the benefit of rapid script creation and deployment, while retaining control of what runs on your servers via your normal config management workflow.",
            "title": "Solo Mode"
        },
        {
            "location": "/getting_started/use_cases/",
            "text": "Use Cases\n\n\nDataloop supports several ways to collect data. We are typically used as a central aggregation point for consolidating many tools into a single platform. This document describes best practices for which collection method we recommend depending upon use case.\n\n\nServer Monitoring\n\n\nInstall a Dataloop Agent onto each server. This lets Dataloop know if your server is up or down and collects operating system metrics like CPU, Disk, Memory and Network.\n\n\nService Monitoring\n\n\nFor common services Dataloop provides packs which include a plugin, dashboard and alert rules. \n\n\nCustom Service Monitoring\n\n\nCreate custom Nagios check scripts and apply them to agents using tags. Build dashboards and alert rules based on tags. To automate sharing configuration between teams create private packs.\n\n\nDocker Monitoring\n\n\nInstall a Dataloop Agent onto each Docker host to collect host level metrics and run Nagios plugins for service monitoring. Also run a dataloop/dataloop-docker container on each Docker host to collect container resource metrics.\n\n\nApplication Metrics\n\n\nInstrument your application with a Prometheus client library. Install a Dataloop Agent onto each server and scrape the Prometheus /metrics endpoint using a Prometheus format plugin.\n\n\nhttps://prometheus.io/docs/instrumenting/clientlibs/\n\n\nApplications Instrumented with StatsD\n\n\nRun the Prometheus StatsD exporter on each StatsD server.\n\n\nhttps://github.com/prometheus/statsd_exporter\n\n\nConfigure StatsD to send metrics into the StatsD exporter. Scrape the Prometheus metrics endpoint on the StatsD exporter using a Dataloop Agent and Prometheus format plugin.\n\n\nhttps://blog.dataloop.io/statsd-tags-and-dataloop\n\n\nLong Running Jobs\n\n\nFor tasks that take a long time to complete write the output to a local file and monitor it with a Dataloop Agent and custom Nagios check script.\n\n\nEvent Driven Jobs\n\n\nFor tasks that are initiated as a result of an external event like CI builds or cron jobs push Graphite line protocol metrics to graphite.dataloop.io on TCP port 2003 using netcat.\n\n\nhttps://support.dataloop.io/endpoints/graphite\n\n\nRemote Monitoring\n\n\nUse the Dataloop Agent and custom Nagios plugins to remotely monitor over the network. For remote website monitoring designate a single agent to run the plugin. For monitoring external services like AWS RDS run the plugin from the application servers that access it. Where configuration varies between agents deploy a YAML file with settings from configuration management and read the file on each plugin run.\n\n\nAggregation\n\n\nMany open source collection tools like CollectD support Graphite output. These collectors can be configured to send metrics into Dataloop via our Graphite port.",
            "title": "Use cases"
        },
        {
            "location": "/getting_started/use_cases/#use-cases",
            "text": "Dataloop supports several ways to collect data. We are typically used as a central aggregation point for consolidating many tools into a single platform. This document describes best practices for which collection method we recommend depending upon use case.",
            "title": "Use Cases"
        },
        {
            "location": "/getting_started/use_cases/#server-monitoring",
            "text": "Install a Dataloop Agent onto each server. This lets Dataloop know if your server is up or down and collects operating system metrics like CPU, Disk, Memory and Network.",
            "title": "Server Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#service-monitoring",
            "text": "For common services Dataloop provides packs which include a plugin, dashboard and alert rules.",
            "title": "Service Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#custom-service-monitoring",
            "text": "Create custom Nagios check scripts and apply them to agents using tags. Build dashboards and alert rules based on tags. To automate sharing configuration between teams create private packs.",
            "title": "Custom Service Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#docker-monitoring",
            "text": "Install a Dataloop Agent onto each Docker host to collect host level metrics and run Nagios plugins for service monitoring. Also run a dataloop/dataloop-docker container on each Docker host to collect container resource metrics.",
            "title": "Docker Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#application-metrics",
            "text": "Instrument your application with a Prometheus client library. Install a Dataloop Agent onto each server and scrape the Prometheus /metrics endpoint using a Prometheus format plugin.  https://prometheus.io/docs/instrumenting/clientlibs/",
            "title": "Application Metrics"
        },
        {
            "location": "/getting_started/use_cases/#applications-instrumented-with-statsd",
            "text": "Run the Prometheus StatsD exporter on each StatsD server.  https://github.com/prometheus/statsd_exporter  Configure StatsD to send metrics into the StatsD exporter. Scrape the Prometheus metrics endpoint on the StatsD exporter using a Dataloop Agent and Prometheus format plugin.  https://blog.dataloop.io/statsd-tags-and-dataloop",
            "title": "Applications Instrumented with StatsD"
        },
        {
            "location": "/getting_started/use_cases/#long-running-jobs",
            "text": "For tasks that take a long time to complete write the output to a local file and monitor it with a Dataloop Agent and custom Nagios check script.",
            "title": "Long Running Jobs"
        },
        {
            "location": "/getting_started/use_cases/#event-driven-jobs",
            "text": "For tasks that are initiated as a result of an external event like CI builds or cron jobs push Graphite line protocol metrics to graphite.dataloop.io on TCP port 2003 using netcat.  https://support.dataloop.io/endpoints/graphite",
            "title": "Event Driven Jobs"
        },
        {
            "location": "/getting_started/use_cases/#remote-monitoring",
            "text": "Use the Dataloop Agent and custom Nagios plugins to remotely monitor over the network. For remote website monitoring designate a single agent to run the plugin. For monitoring external services like AWS RDS run the plugin from the application servers that access it. Where configuration varies between agents deploy a YAML file with settings from configuration management and read the file on each plugin run.",
            "title": "Remote Monitoring"
        },
        {
            "location": "/getting_started/use_cases/#aggregation",
            "text": "Many open source collection tools like CollectD support Graphite output. These collectors can be configured to send metrics into Dataloop via our Graphite port.",
            "title": "Aggregation"
        },
        {
            "location": "/getting_started/where_we_fit/",
            "text": "Where We Fit in the Monitoring Landscape\n\n\nWe are a platform for time-series data visualisation and alerting. You can send us any volume of data and we will aim to process and display it in real time with seamless historical querying. Our platform is agnostic in terms of what time-series data you send to us. However, we spend most of our time helping SaaS companies scale their online services. This generally includes operating system, application and business level metrics; hence a focus on plugins, interfaces and integrations that make sense in that context.\n\n\nWe believe in using a few best of breed tools rather than a single mediocre unified tool. With open standards protocols and APIs, you can easily join tools together to prevent vendor lock-in and future-proof your monitoring stack.\n\n\nLogs\n\n\nWe don't accept log data. Other tools such as \nELK\n, \nSplunk\n and \nGraylog\n do a great job at processing, indexing and querying logs. However, we do integrate with these systems. In the case of the ELK stack, you can use the StatsD forwarder to send time-series metrics from your logging pipeline to us via our Graphite port. For Splunk we have plugins that can scrape the API.\n\n\nAPM\n\n\nWe don't provide tools that profile or introspect your production applications. Tools like \nNew Relic\n and \nAppDynamics\n do a good job here. We integrate with these tools via scripts that poll their API.\n\n\nStatsD\n\n\nWhere it gets slightly confusing is with \nStatsD\n. We do support StatsD metrics which a lot of companies use to send back data from their live production apps. These are custom metrics that have to be specifically configured within the application code.\n\n\nAPM and StatsD are best used in combination. Use an APM tool to profile your application and provide top 10s and drill-downs into slow transactions. Use StatsD custom metrics when you want to track a particular thing, like the performance of a section of code, business metrics like signups or even feature usage.\n\n\nOn-Call Management\n\n\nManaging on-call scheduling, nagging, escalation and policies isn't our specialty at this time. We integrate with other tools like PagerDuty and OpsGenie for those features.\n\n\nException Tracking\n\n\nTools like Bugsnag will email your developers directly with stack traces caused by users in production. We're not that type of tool.\n\n\nIncident Management Tools\n\n\nWe focus on gathering data, visualizing it and alerting. We don't help manage incidents in the way that tools like \nBigPanda\n do. If you have a wide variety of monitoring tools then our alerts could be configured to send to incident management tools for correlation.",
            "title": "Where we fit"
        },
        {
            "location": "/getting_started/where_we_fit/#where-we-fit-in-the-monitoring-landscape",
            "text": "We are a platform for time-series data visualisation and alerting. You can send us any volume of data and we will aim to process and display it in real time with seamless historical querying. Our platform is agnostic in terms of what time-series data you send to us. However, we spend most of our time helping SaaS companies scale their online services. This generally includes operating system, application and business level metrics; hence a focus on plugins, interfaces and integrations that make sense in that context.  We believe in using a few best of breed tools rather than a single mediocre unified tool. With open standards protocols and APIs, you can easily join tools together to prevent vendor lock-in and future-proof your monitoring stack.",
            "title": "Where We Fit in the Monitoring Landscape"
        },
        {
            "location": "/getting_started/where_we_fit/#logs",
            "text": "We don't accept log data. Other tools such as  ELK ,  Splunk  and  Graylog  do a great job at processing, indexing and querying logs. However, we do integrate with these systems. In the case of the ELK stack, you can use the StatsD forwarder to send time-series metrics from your logging pipeline to us via our Graphite port. For Splunk we have plugins that can scrape the API.",
            "title": "Logs"
        },
        {
            "location": "/getting_started/where_we_fit/#apm",
            "text": "We don't provide tools that profile or introspect your production applications. Tools like  New Relic  and  AppDynamics  do a good job here. We integrate with these tools via scripts that poll their API.",
            "title": "APM"
        },
        {
            "location": "/getting_started/where_we_fit/#statsd",
            "text": "Where it gets slightly confusing is with  StatsD . We do support StatsD metrics which a lot of companies use to send back data from their live production apps. These are custom metrics that have to be specifically configured within the application code.  APM and StatsD are best used in combination. Use an APM tool to profile your application and provide top 10s and drill-downs into slow transactions. Use StatsD custom metrics when you want to track a particular thing, like the performance of a section of code, business metrics like signups or even feature usage.",
            "title": "StatsD"
        },
        {
            "location": "/getting_started/where_we_fit/#on-call-management",
            "text": "Managing on-call scheduling, nagging, escalation and policies isn't our specialty at this time. We integrate with other tools like PagerDuty and OpsGenie for those features.",
            "title": "On-Call Management"
        },
        {
            "location": "/getting_started/where_we_fit/#exception-tracking",
            "text": "Tools like Bugsnag will email your developers directly with stack traces caused by users in production. We're not that type of tool.",
            "title": "Exception Tracking"
        },
        {
            "location": "/getting_started/where_we_fit/#incident-management-tools",
            "text": "We focus on gathering data, visualizing it and alerting. We don't help manage incidents in the way that tools like  BigPanda  do. If you have a wide variety of monitoring tools then our alerts could be configured to send to incident management tools for correlation.",
            "title": "Incident Management Tools"
        },
        {
            "location": "/integrations/thirdparty/aws/",
            "text": "Amazon Web Services\n\n\nSome of the Dataloop plugins need an \nAWS Access Key ID\n and \nSecret Access Key\n in order to extract metrics from the CloudWatch API.\n\n\nTo generate them use the following instructions:\n\n\n\n\nTo get started, open the \nAWS Management Console\n\n\nClick the IAM tab.\n\n\nClick the Create a New Group of Users button.\n\n\nEnter a Group Name called Dataloop.\n\n\nSelect the Read Only Access Policy Template then click Continue.\n\n\nClick the Create New Users tab.\n\n\nEnter a new User Name called Dataloop and click Continue and then Finish.\n\n\nClick Show User Security Credentials.\n\n\nCopy and paste your \nAccess Key Id\n and the \nSecret Access Key\n somewhere safe",
            "title": "Aws"
        },
        {
            "location": "/integrations/thirdparty/aws/#amazon-web-services",
            "text": "Some of the Dataloop plugins need an  AWS Access Key ID  and  Secret Access Key  in order to extract metrics from the CloudWatch API.  To generate them use the following instructions:   To get started, open the  AWS Management Console  Click the IAM tab.  Click the Create a New Group of Users button.  Enter a Group Name called Dataloop.  Select the Read Only Access Policy Template then click Continue.  Click the Create New Users tab.  Enter a new User Name called Dataloop and click Continue and then Finish.  Click Show User Security Credentials.  Copy and paste your  Access Key Id  and the  Secret Access Key  somewhere safe",
            "title": "Amazon Web Services"
        },
        {
            "location": "/integrations/webhook/bigpanda/",
            "text": "BigPanda\n\n\nSetting up BigPanda\n\n\nSet the webhook URL in Dataloop to:\n\n\nhttps://api.bigpanda.io/data/v2/alerts?access_token=\n\n\nThen in the Extra Payload box add the following:\n\n\n{\n\"app_key\": \"your_bigpanda_app_key\"\n}\n\n\n\n\nHit the test button and you should get an incident triggered in BigPanda.\n\n\nOptional Settings\n\n\nYou can pass in some additional details to BigPanda via the optional fields as specified in the Developer Docs\n\n\nhttps://www.bigpanda.io/docs/display/BD/POST+alerts",
            "title": "Bigpanda"
        },
        {
            "location": "/integrations/webhook/bigpanda/#bigpanda",
            "text": "",
            "title": "BigPanda"
        },
        {
            "location": "/integrations/webhook/bigpanda/#setting-up-bigpanda",
            "text": "Set the webhook URL in Dataloop to:  https://api.bigpanda.io/data/v2/alerts?access_token=  Then in the Extra Payload box add the following:  {\n\"app_key\": \"your_bigpanda_app_key\"\n}  Hit the test button and you should get an incident triggered in BigPanda.",
            "title": "Setting up BigPanda"
        },
        {
            "location": "/integrations/webhook/bigpanda/#optional-settings",
            "text": "You can pass in some additional details to BigPanda via the optional fields as specified in the Developer Docs  https://www.bigpanda.io/docs/display/BD/POST+alerts",
            "title": "Optional Settings"
        },
        {
            "location": "/integrations/webhook/hipchat/",
            "text": "Hipchat\n\n\nSetting up HipChat\n\n\nSet the webhook URL in Dataloop to:\n\n\nhttps://api.hipchat.com/v2/room/{room_id_or_name}/notification?auth_token=YOUR_TOKEN\n\n\nThen in the Extra Payload box add the following:\n\n\n{\n\"from\": \"Dataloop\",\n\"message_format\": \"text\"\n}\n\n\n\n\nHit the test button and you should get a message in your HipChat room.\n\n\nOptional Settings\n\n\nYou can pass in some additional details to HipChat via the optional fields as specified in the Developer Docs\n\n\nhttps://www.hipchat.com/docs/apiv2/method/send_room_notification",
            "title": "Hipchat"
        },
        {
            "location": "/integrations/webhook/hipchat/#hipchat",
            "text": "",
            "title": "Hipchat"
        },
        {
            "location": "/integrations/webhook/hipchat/#setting-up-hipchat",
            "text": "Set the webhook URL in Dataloop to:  https://api.hipchat.com/v2/room/{room_id_or_name}/notification?auth_token=YOUR_TOKEN  Then in the Extra Payload box add the following:  {\n\"from\": \"Dataloop\",\n\"message_format\": \"text\"\n}  Hit the test button and you should get a message in your HipChat room.",
            "title": "Setting up HipChat"
        },
        {
            "location": "/integrations/webhook/hipchat/#optional-settings",
            "text": "You can pass in some additional details to HipChat via the optional fields as specified in the Developer Docs  https://www.hipchat.com/docs/apiv2/method/send_room_notification",
            "title": "Optional Settings"
        },
        {
            "location": "/integrations/webhook/opsgenie/",
            "text": "OpsGenie\n\n\nSetting up OpsGenie\n\n\nWe have an official integration with OpsGenie. Instructions for how to set this up can be found here:\n\n\nhttps://www.opsgenie.com/docs/integrations/dataloop-io-integration\n\n\nOptional Settings\n\n\nYou can pass in some additional details to OpsGenie via the optional fields as specified in the Developer Docs\n\n\nhttps://www.opsgenie.com/docs/web-api/alert-api\n\n\nOne of the more common options may be to specify which teams to alert.\n\n\n{\n\"apiKey: \"your_opsgenie_api_key\",\n\"teams\" : [\"operations\", \"developers\"]\n}",
            "title": "Opsgenie"
        },
        {
            "location": "/integrations/webhook/opsgenie/#opsgenie",
            "text": "",
            "title": "OpsGenie"
        },
        {
            "location": "/integrations/webhook/opsgenie/#setting-up-opsgenie",
            "text": "We have an official integration with OpsGenie. Instructions for how to set this up can be found here:  https://www.opsgenie.com/docs/integrations/dataloop-io-integration",
            "title": "Setting up OpsGenie"
        },
        {
            "location": "/integrations/webhook/opsgenie/#optional-settings",
            "text": "You can pass in some additional details to OpsGenie via the optional fields as specified in the Developer Docs  https://www.opsgenie.com/docs/web-api/alert-api  One of the more common options may be to specify which teams to alert.  {\n\"apiKey: \"your_opsgenie_api_key\",\n\"teams\" : [\"operations\", \"developers\"]\n}",
            "title": "Optional Settings"
        },
        {
            "location": "/integrations/webhook/pagerduty/",
            "text": "Pagerduty\n\n\nSetting up Pagerduty\n\n\nSetup a new Generic API Service in Pagerduty and take a copy of the Integration Key.\n\n\nSet the webhook URL in Dataloop to:\n\n\nhttps://events.pagerduty.com/generic/2010-04-15/create_event.json\n\n\nThen in the Extra Payload box add the following:\n\n\n{\n\"service_key\": \"your_pagerduty_integration_key\"\n}\n\n\n\n\nHit the test button and you should get an incident triggered in Pagerduty.\n\n\nWhen the rule is resolved in Dataloop the webhook will fire again to resolve the incident in Pagerduty automatically.\n\n\nOptional Settings\n\n\nYou can pass in some additional details to PagerDuty via the optional fields as specified in the Developer Docs\n\n\nhttps://developer.pagerduty.com/documentation/integration/events/trigger",
            "title": "Pagerduty"
        },
        {
            "location": "/integrations/webhook/pagerduty/#pagerduty",
            "text": "",
            "title": "Pagerduty"
        },
        {
            "location": "/integrations/webhook/pagerduty/#setting-up-pagerduty",
            "text": "Setup a new Generic API Service in Pagerduty and take a copy of the Integration Key.  Set the webhook URL in Dataloop to:  https://events.pagerduty.com/generic/2010-04-15/create_event.json  Then in the Extra Payload box add the following:  {\n\"service_key\": \"your_pagerduty_integration_key\"\n}  Hit the test button and you should get an incident triggered in Pagerduty.  When the rule is resolved in Dataloop the webhook will fire again to resolve the incident in Pagerduty automatically.",
            "title": "Setting up Pagerduty"
        },
        {
            "location": "/integrations/webhook/pagerduty/#optional-settings",
            "text": "You can pass in some additional details to PagerDuty via the optional fields as specified in the Developer Docs  https://developer.pagerduty.com/documentation/integration/events/trigger",
            "title": "Optional Settings"
        },
        {
            "location": "/integrations/webhook/slack/",
            "text": "Slack\n\n\nSetting up Slack\n\n\nCreate a new incoming webhook in Slack. For more details see their documentation:\n\n\nhttps://api.slack.com/incoming-webhooks\n\n\nThis will provide a URL that you can paste into the webhook.\n\n\nClick the test button and you should receive a message in your slack channel.",
            "title": "Slack"
        },
        {
            "location": "/integrations/webhook/slack/#slack",
            "text": "",
            "title": "Slack"
        },
        {
            "location": "/integrations/webhook/slack/#setting-up-slack",
            "text": "Create a new incoming webhook in Slack. For more details see their documentation:  https://api.slack.com/incoming-webhooks  This will provide a URL that you can paste into the webhook.  Click the test button and you should receive a message in your slack channel.",
            "title": "Setting up Slack"
        },
        {
            "location": "/integrations/webhook/victorops/",
            "text": "VictorOps\n\n\nSetting up VictorOps\n\n\nSet the webhook URL in Dataloop to:\n\n\nhttps://alert.victorops.com/integrations/generic/20131114/alert/[YOUR_API_KEY_HERE]/[ROUTING_KEY_HERE]\n\n\nHit the test button and you should get an incident triggered in VictorOps.\n\n\nWhen the rule is resolved in Dataloop the webhook will fire again to resolve the incident in VictorOps automatically.\n\n\nOptional Settings\n\n\nYou can pass in some additional details to VictorOps via the optional fields as specified in the Developer Docs\n\n\nhttp://victorops.force.com/knowledgebase/articles/Integration/Alert-Ingestion-API-Documentation",
            "title": "Victorops"
        },
        {
            "location": "/integrations/webhook/victorops/#victorops",
            "text": "",
            "title": "VictorOps"
        },
        {
            "location": "/integrations/webhook/victorops/#setting-up-victorops",
            "text": "Set the webhook URL in Dataloop to:  https://alert.victorops.com/integrations/generic/20131114/alert/[YOUR_API_KEY_HERE]/[ROUTING_KEY_HERE]  Hit the test button and you should get an incident triggered in VictorOps.  When the rule is resolved in Dataloop the webhook will fire again to resolve the incident in VictorOps automatically.",
            "title": "Setting up VictorOps"
        },
        {
            "location": "/integrations/webhook/victorops/#optional-settings",
            "text": "You can pass in some additional details to VictorOps via the optional fields as specified in the Developer Docs  http://victorops.force.com/knowledgebase/articles/Integration/Alert-Ingestion-API-Documentation",
            "title": "Optional Settings"
        },
        {
            "location": "/nagios/built_in_python/",
            "text": "Built-In Python Interpreter\n\n\nWhen creating or editing a Nagios plugin you can select the shell to use when executing the script from the drop down. The default is 'system default' which means scripts get executed as if you ran them from the command line on the server. On Linux this means whatever you have set the shebang to.\n\n\n'Built-In Python' is a shell we ship with the agent. This is a statically compiled Python 2.7 interpreter with a whole bunch of libraries baked in. When you select this we will always run the contents of your script using this interpreter.\n\n\nHere are a list of libraries currently enabled.\n\n\n###\n# Core Agent dependencies. These are require for the Agent to run\n###\nAPScheduler==3.0.3\nnetifaces==0.10.4\nnose==1.3.7\npsutil==4.3.0\npytz==2016.10\nPyYAML==3.11\nrequests==2.12.3\nwebsocket-client==0.37.0\n\n\n\n\n##\n##\n# Plugin dependencies. These are required to run the out of the box plugins\n##\nbackports.ssl-match-hostname==3.4.0.2\nBeautifulSoup==3.2.1\nboto==2.23.0\nboto3==1.4.0\ncffi==1.7.0\ncryptography==1.4\ndlcli==0.3.6\ndocker-py==1.9.0\ngevent==1.0.2\ngrequests==0.3.0\ngoogle-api-python-client==1.4.0\njenkinsapi==0.2.22\nmeld3==1.0.0\nmock==1.0.1\nMySQL-python==1.2.5\nnagioscheck==0.1.6\nnagiosplugin==1.2.2\noauth2client==1.5.2\npexpect==4.0.1\npg8000==1.10.1\npika==0.9.14\npsycopg2==2.6\npycparser==2.14\npyjolokia==0.3.1\npymongo==2.6.3\npynag==0.9.1\npynagios==0.1.1\npyOpenSSL==0.14\npysnmp==4.2.5\npyvmomi==5.5.0.2014.1.1\nredis==2.10.3\nrethinkdb==2.2.0.post6\nrobobrowser==0.5.3\nsix==1.10.0\nsplunk-sdk==1.3.1\ntestinfra==1.0.2\nxmltodict==0.9.0\n\n\n\n\nYou can add additional libraries on the server by running pip from the embedded interpreter bin folder.\n\n\nOn Linux:\n\n\n/opt/dataloop/embedded/bin/pip install <module>\n\n\n\n\nOn Windows:\n\n\nc:\\dataloop\\embedded\\bin\\pip.exe install <module>\n\n\n\n\nIf you contact us at support@dataloop.io you can request we bundle other libraries.",
            "title": "Built in python"
        },
        {
            "location": "/nagios/built_in_python/#built-in-python-interpreter",
            "text": "When creating or editing a Nagios plugin you can select the shell to use when executing the script from the drop down. The default is 'system default' which means scripts get executed as if you ran them from the command line on the server. On Linux this means whatever you have set the shebang to.  'Built-In Python' is a shell we ship with the agent. This is a statically compiled Python 2.7 interpreter with a whole bunch of libraries baked in. When you select this we will always run the contents of your script using this interpreter.  Here are a list of libraries currently enabled.  ###\n# Core Agent dependencies. These are require for the Agent to run\n###\nAPScheduler==3.0.3\nnetifaces==0.10.4\nnose==1.3.7\npsutil==4.3.0\npytz==2016.10\nPyYAML==3.11\nrequests==2.12.3\nwebsocket-client==0.37.0  ##\n##\n# Plugin dependencies. These are required to run the out of the box plugins\n##\nbackports.ssl-match-hostname==3.4.0.2\nBeautifulSoup==3.2.1\nboto==2.23.0\nboto3==1.4.0\ncffi==1.7.0\ncryptography==1.4\ndlcli==0.3.6\ndocker-py==1.9.0\ngevent==1.0.2\ngrequests==0.3.0\ngoogle-api-python-client==1.4.0\njenkinsapi==0.2.22\nmeld3==1.0.0\nmock==1.0.1\nMySQL-python==1.2.5\nnagioscheck==0.1.6\nnagiosplugin==1.2.2\noauth2client==1.5.2\npexpect==4.0.1\npg8000==1.10.1\npika==0.9.14\npsycopg2==2.6\npycparser==2.14\npyjolokia==0.3.1\npymongo==2.6.3\npynag==0.9.1\npynagios==0.1.1\npyOpenSSL==0.14\npysnmp==4.2.5\npyvmomi==5.5.0.2014.1.1\nredis==2.10.3\nrethinkdb==2.2.0.post6\nrobobrowser==0.5.3\nsix==1.10.0\nsplunk-sdk==1.3.1\ntestinfra==1.0.2\nxmltodict==0.9.0  You can add additional libraries on the server by running pip from the embedded interpreter bin folder.  On Linux:  /opt/dataloop/embedded/bin/pip install <module>  On Windows:  c:\\dataloop\\embedded\\bin\\pip.exe install <module>  If you contact us at support@dataloop.io you can request we bundle other libraries.",
            "title": "Built-In Python Interpreter"
        },
        {
            "location": "/nagios/nagios_performance_data/",
            "text": "Nagios Performance Data\n\n\nYou can create, edit, run / test and deploy Nagios format check scripts in any language from within the Dataloop web interface. This is unique to Dataloop and we believe this is the secret sauce that enables adoption.\n\n\nFor performance data we adhere to the full spec listed \nhere\n (the relevant bit copied below)\n\n\nWe support the absolute minimum in terms of what you can get away with and still have us graph the output. The following is a perfectly legitimate bash script that would graph properly if you used it in Dataloop:\n\n\n#!/bin/bash\n\necho \"OK | something=$RANDOM\"\n\n\n\n\nWe graph everything after the pipe | symbol. You could even add a space after something=$RANDOM and add another whatever=value.\n\n\nIf you deployed this via drag and drop to some agents and wait 30 seconds (our default script interval) then you'll see 'something' and 'whatever' appear as metrics in the dashboard side panel and in the rules criteria metrics drop down.\n\n\nObviously in your scripts you will want to build up the performance data string so that it includes metrics worth graphing. If you have metrics that can be grouped our advice is to use dots to separate. For instance:\n\n\nOK | load.load1min=1234;;;; load.load5min=1234;;;; load.load15min=1234;;;;\n\n\nWe do some cool stuff in the Dataloop UI to group on dots.\n\n\nThe full specification\n\n\nNagios 3 and newer will concatenate the parts following a \"|\" in a) the first line output by the plugin, and b) in the second to last line, into a string it passes to whatever performance data processing it has configured. (Note that it currently does not insert additional whitespace between both, so the plugin needs to provide some to prevent the last pair of a) and the first of b) getting run together.) Please refer to the Nagios documentation for information on how to configure such processing. However, it is the responsibility of the plugin writer to ensure the performance data is in a \"Nagios Plugins\" format.\n\n\nThis is the expected format: \n\n\n'label'=value[UOM];[warn];[crit];[min];[max]\n\n\n\n\nNotes:\n\n\n\n\nspace separated list of label/value pairs \n\n\nlabel can contain any characters except the equals sign or single quote (') \n\n\nthe single quotes for the label are optional. Required if spaces are in the label \n\n\nlabel length is arbitrary, but ideally the first 19 characters are unique (due to a limitation in RRD). Be aware of a limitation in the amount of data that NRPE returns to Nagios \n\n\nto specify a quote character, use two single quotes \n\n\nwarn, crit, min or max may be null (for example, if the threshold is not defined or min and max do not apply). Trailing unfilled semicolons can be dropped \n\n\nmin and max are not required if UOM=% \n\n\nvalue, min and max in class [-0-9.]. Must all be the same UOM. value may be a literal \"U\" instead, this would indicate that the actual value couldn't be determined \n\n\nwarn and crit are in the range format (see the Section called Threshold and ranges). Must be the same UOM \n\n\nUOM (unit of measurement) is one of: \n\n\nno unit specified - assume a number (int or float) of things (eg, users, processes, load averages) \n\n\ns - seconds (also us, ms) \n\n\n% - percentage \n\n\nB - bytes (also KB, MB, TB) \n\n\nc - a continous counter (such as bytes transmitted on an interface)",
            "title": "Nagios performance data"
        },
        {
            "location": "/nagios/nagios_performance_data/#nagios-performance-data",
            "text": "You can create, edit, run / test and deploy Nagios format check scripts in any language from within the Dataloop web interface. This is unique to Dataloop and we believe this is the secret sauce that enables adoption.  For performance data we adhere to the full spec listed  here  (the relevant bit copied below)  We support the absolute minimum in terms of what you can get away with and still have us graph the output. The following is a perfectly legitimate bash script that would graph properly if you used it in Dataloop:  #!/bin/bash\n\necho \"OK | something=$RANDOM\"  We graph everything after the pipe | symbol. You could even add a space after something=$RANDOM and add another whatever=value.  If you deployed this via drag and drop to some agents and wait 30 seconds (our default script interval) then you'll see 'something' and 'whatever' appear as metrics in the dashboard side panel and in the rules criteria metrics drop down.  Obviously in your scripts you will want to build up the performance data string so that it includes metrics worth graphing. If you have metrics that can be grouped our advice is to use dots to separate. For instance:  OK | load.load1min=1234;;;; load.load5min=1234;;;; load.load15min=1234;;;;  We do some cool stuff in the Dataloop UI to group on dots.",
            "title": "Nagios Performance Data"
        },
        {
            "location": "/nagios/nagios_performance_data/#the-full-specification",
            "text": "Nagios 3 and newer will concatenate the parts following a \"|\" in a) the first line output by the plugin, and b) in the second to last line, into a string it passes to whatever performance data processing it has configured. (Note that it currently does not insert additional whitespace between both, so the plugin needs to provide some to prevent the last pair of a) and the first of b) getting run together.) Please refer to the Nagios documentation for information on how to configure such processing. However, it is the responsibility of the plugin writer to ensure the performance data is in a \"Nagios Plugins\" format.  This is the expected format:   'label'=value[UOM];[warn];[crit];[min];[max]",
            "title": "The full specification"
        },
        {
            "location": "/nagios/nagios_performance_data/#notes",
            "text": "space separated list of label/value pairs   label can contain any characters except the equals sign or single quote (')   the single quotes for the label are optional. Required if spaces are in the label   label length is arbitrary, but ideally the first 19 characters are unique (due to a limitation in RRD). Be aware of a limitation in the amount of data that NRPE returns to Nagios   to specify a quote character, use two single quotes   warn, crit, min or max may be null (for example, if the threshold is not defined or min and max do not apply). Trailing unfilled semicolons can be dropped   min and max are not required if UOM=%   value, min and max in class [-0-9.]. Must all be the same UOM. value may be a literal \"U\" instead, this would indicate that the actual value couldn't be determined   warn and crit are in the range format (see the Section called Threshold and ranges). Must be the same UOM   UOM (unit of measurement) is one of:   no unit specified - assume a number (int or float) of things (eg, users, processes, load averages)   s - seconds (also us, ms)   % - percentage   B - bytes (also KB, MB, TB)   c - a continous counter (such as bytes transmitted on an interface)",
            "title": "Notes:"
        },
        {
            "location": "/nagios/nagios_plugins/",
            "text": "Nagios Plugins\n\n\nDataloop supports Nagios standard plugins.\n\n\nThe simplest example of a Nagios plugin written in bash is:\n\n\n#!/usr/bin/env bash\n\necho \"OK | metric=0\"\n\nexit 0 \n\n\n\n\nNagios plugins write a string to stdout containing a message, a pipe symbol and then key value pairs separated by an equals sign as per above. You can put multiple key value pairs on the same line separated by spaces. Anything after the pipe symbol can be used as performance data in Dataloop. Tip: ensure there is a pipe symbol in your plugins.\n\n\nYou should also place exit codes in logic throughout your plugins. 0 means OK, 1 means warning, 2 means critical and 3 means unknown. These map to plugin_name.status metrics in Dataloop and can be used in Dashboards and alerts to express a state change which will change widget colours and fire off email alerts if you set them up.\n\n\nYou can also add units of measure and other options to the stdout string of Nagios plugins. The full spec can be found here:\n\n\nhttps://nagios-plugins.org/doc/guidelines.html\n\n\nIn Dataloop something is either considered working, or it is broken. So for alerting off exit codes we only alert when the status is critical (2). Which means the majority of your plugins should emit exit code 0 on success and 2 on failure.\n\n\nIf you need to monitor something generic, and we don't have it as an out of the box plugin, then usually the first place to look is the Nagios Exchange:\n\n\nhttp://exchange.nagios.org/directory/Plugins\n\n\nAlternatively, email us at info@dataloop.io or \ncome and find us in Slack\n and we'll create a script for you and place it into our plugin library on Github:\n\n\nhttps://github.com/dataloop/plugins",
            "title": "Nagios plugins"
        },
        {
            "location": "/nagios/nagios_plugins/#nagios-plugins",
            "text": "Dataloop supports Nagios standard plugins.  The simplest example of a Nagios plugin written in bash is:  #!/usr/bin/env bash\n\necho \"OK | metric=0\"\n\nexit 0   Nagios plugins write a string to stdout containing a message, a pipe symbol and then key value pairs separated by an equals sign as per above. You can put multiple key value pairs on the same line separated by spaces. Anything after the pipe symbol can be used as performance data in Dataloop. Tip: ensure there is a pipe symbol in your plugins.  You should also place exit codes in logic throughout your plugins. 0 means OK, 1 means warning, 2 means critical and 3 means unknown. These map to plugin_name.status metrics in Dataloop and can be used in Dashboards and alerts to express a state change which will change widget colours and fire off email alerts if you set them up.  You can also add units of measure and other options to the stdout string of Nagios plugins. The full spec can be found here:  https://nagios-plugins.org/doc/guidelines.html  In Dataloop something is either considered working, or it is broken. So for alerting off exit codes we only alert when the status is critical (2). Which means the majority of your plugins should emit exit code 0 on success and 2 on failure.  If you need to monitor something generic, and we don't have it as an out of the box plugin, then usually the first place to look is the Nagios Exchange:  http://exchange.nagios.org/directory/Plugins  Alternatively, email us at info@dataloop.io or  come and find us in Slack  and we'll create a script for you and place it into our plugin library on Github:  https://github.com/dataloop/plugins",
            "title": "Nagios Plugins"
        },
        {
            "location": "/nagios/plugin_deplotmen_models/",
            "text": "Plugin Deployment Models\n\n\n1. Default  Mode (centralised configuration)\n\n\nIn this model you create your plugins in the Dataloop web UI. They get stored centrally within your Dataloop account and can be edited and tested using our built-in plugin editor. You can add these plugins to either agents directly, or tags of agents, so that they deploy instantly and start returning metrics that can be used on dashboards and in alert rules.\n\n\n\n\nThere are two problems this mode solves;  firstly getting adoption so that more people can collaborate on increasing your monitoring coverage, and secondly reducing the round trip time between knowing what to measure and having those metrics available to use (improving the ooda loop).\n\n\n2. Hybrid  Mode (centralised + decentralised configuration)\n\n\nThe default mode of operation for the agent also supports loading plugins from disk. You can still deploy your plugins using the centralised model shown above, and additionally you can also put plugins into /opt/dataloop/plugins (or c:\\dataloop\\plugins on windows) so that they get loaded automatically by the agent.\n\n\n\n\nIn this mode you get to keep the benefits of the default mode for ad-hoc monitoring but can can use config management as your primary mechanism for determining what agents run what plugins, as well as using your version controlled change management process for deploying them.\n\n\n\n\nIn this mode we will never upload the plugins deployed from disk into Dataloop, so they won\u2019t appear in your plugins list. However, the plugins will show as running in the agent details page. Also, the run button will only ever run plugins written in the browser.\n\n\n\n\n3. Solo Mode  (decentralised configuration)\n\n\nSolo mode disables the ability to create, test and deploy plugins in the browser. For this mode you will need to use configuration management to deploy your plugins directly into the local plugin directory. These plugins then automatically register inside Dataloop and appear in the agent details page when running (but never in the plugins list, since they don\u2019t get uploaded to us).\n\n\n\n\nSome benefits to this are that you get to work the way you may always have worked with monitoring tools like Nagios and Sensu. Infact, with Dataloop it\u2019s even simpler than those tools, you simply need to drop plugins into the correct directory and the agent will automatically run them. You\u2019ll still need to tag your agents as before so that metrics appear in dashboards and alert rules based on those tags work.\n\n\nThe advantage of using agents in solo mode over hybrid mode are mostly around security. You can run a mix of solo mode agents alongside default mode agents depending on how you want to manage risk.",
            "title": "Plugin deplotmen models"
        },
        {
            "location": "/nagios/plugin_deplotmen_models/#plugin-deployment-models",
            "text": "",
            "title": "Plugin Deployment Models"
        },
        {
            "location": "/nagios/plugin_deplotmen_models/#1-default-mode-centralised-configuration",
            "text": "In this model you create your plugins in the Dataloop web UI. They get stored centrally within your Dataloop account and can be edited and tested using our built-in plugin editor. You can add these plugins to either agents directly, or tags of agents, so that they deploy instantly and start returning metrics that can be used on dashboards and in alert rules.   There are two problems this mode solves;  firstly getting adoption so that more people can collaborate on increasing your monitoring coverage, and secondly reducing the round trip time between knowing what to measure and having those metrics available to use (improving the ooda loop).",
            "title": "1. Default  Mode (centralised configuration)"
        },
        {
            "location": "/nagios/plugin_deplotmen_models/#2-hybrid-mode-centralised-decentralised-configuration",
            "text": "The default mode of operation for the agent also supports loading plugins from disk. You can still deploy your plugins using the centralised model shown above, and additionally you can also put plugins into /opt/dataloop/plugins (or c:\\dataloop\\plugins on windows) so that they get loaded automatically by the agent.   In this mode you get to keep the benefits of the default mode for ad-hoc monitoring but can can use config management as your primary mechanism for determining what agents run what plugins, as well as using your version controlled change management process for deploying them.   In this mode we will never upload the plugins deployed from disk into Dataloop, so they won\u2019t appear in your plugins list. However, the plugins will show as running in the agent details page. Also, the run button will only ever run plugins written in the browser.",
            "title": "2. Hybrid  Mode (centralised + decentralised configuration)"
        },
        {
            "location": "/nagios/plugin_deplotmen_models/#3-solo-mode-decentralised-configuration",
            "text": "Solo mode disables the ability to create, test and deploy plugins in the browser. For this mode you will need to use configuration management to deploy your plugins directly into the local plugin directory. These plugins then automatically register inside Dataloop and appear in the agent details page when running (but never in the plugins list, since they don\u2019t get uploaded to us).   Some benefits to this are that you get to work the way you may always have worked with monitoring tools like Nagios and Sensu. Infact, with Dataloop it\u2019s even simpler than those tools, you simply need to drop plugins into the correct directory and the agent will automatically run them. You\u2019ll still need to tag your agents as before so that metrics appear in dashboards and alert rules based on those tags work.  The advantage of using agents in solo mode over hybrid mode are mostly around security. You can run a mix of solo mode agents alongside default mode agents depending on how you want to manage risk.",
            "title": "3. Solo Mode  (decentralised configuration)"
        },
        {
            "location": "/nagios/powershell/",
            "text": "Powershell\n\n\nYou can create native Powershell plugins in Dataloop by creating a custom shell and ensuring your Powershell code exits correctly. \n\n\nOpen the Settings page in your account and create a new shell called 'Powershell' with the path set to:\n\n\nC:\\Windows\\System32\\WindowsPowershell\\v1.0\\powershell.exe -executionpolicy bypass -File\n\n\n\n\nWhen creating plugins ensure they have the .ps1 extension set and change the shell on the details page to 'Powershell'.\n\n\nWhen writing Powershell plugins please use the following function to set the exit code correctly so that Dataloop gets the correct code returned.\n\n\nfunction ExitWithCode\n{\n    param\n    (\n        $exitcode\n    )  \n    $host.SetShouldExit($exitcode)\n    exit\n}\n\n\n\n\nThen wherever you would usually exit use ExitWithCode instead. This will enable passing of the standard nagios exit codes 0,1,2 and 3 back to Dataloop for use in dashboard status widgets and alerts.\n\n\nYou can also use arguments by updating the shell arguments on the plugins details page. For example if you set:\n\n\n-arg1 123 -arg2 456\n\n\n\n\nAs the shell arguments on a plugin. You can then reference them in your Powershell script by first definining them as params \n\n\n[CmdletBinding()]\nParam\n(\n    [string]$arg1,\n    [string]$arg2\n)\n\n\n\n\nThen referencing the arguments in your scripts with \n$arg1\n and \n$arg2\n or whatever you name your arguments.",
            "title": "Powershell"
        },
        {
            "location": "/nagios/powershell/#powershell",
            "text": "You can create native Powershell plugins in Dataloop by creating a custom shell and ensuring your Powershell code exits correctly.   Open the Settings page in your account and create a new shell called 'Powershell' with the path set to:  C:\\Windows\\System32\\WindowsPowershell\\v1.0\\powershell.exe -executionpolicy bypass -File  When creating plugins ensure they have the .ps1 extension set and change the shell on the details page to 'Powershell'.  When writing Powershell plugins please use the following function to set the exit code correctly so that Dataloop gets the correct code returned.  function ExitWithCode\n{\n    param\n    (\n        $exitcode\n    )  \n    $host.SetShouldExit($exitcode)\n    exit\n}  Then wherever you would usually exit use ExitWithCode instead. This will enable passing of the standard nagios exit codes 0,1,2 and 3 back to Dataloop for use in dashboard status widgets and alerts.  You can also use arguments by updating the shell arguments on the plugins details page. For example if you set:  -arg1 123 -arg2 456  As the shell arguments on a plugin. You can then reference them in your Powershell script by first definining them as params   [CmdletBinding()]\nParam\n(\n    [string]$arg1,\n    [string]$arg2\n)  Then referencing the arguments in your scripts with  $arg1  and  $arg2  or whatever you name your arguments.",
            "title": "Powershell"
        },
        {
            "location": "/nagios/prometheus_plugins/",
            "text": "Prometheus Plugins\n\n\nThe Dataloop agent can run plugins that scrape Prometheus http endpoints. We recommend that you install a Dataloop agent on each server and then create a separate plugin to monitor each endpoint on localhost.\n\n\nExample for Node Exporter\n\n\nStart the Prometheus Node Exporter on a server following the instructions here:\n\n\nhttps://github.com/prometheus/node_exporter\n\n\nThen create a new plugin in Dataloop called node_exporter.py.\n\n\nPaste in the following code to scrape the endpoint:\n\n\n#!/usr/bin/env python\nimport requests\nrequests.get('http://localhost:9100/metrics').text\n\n\n\n\nWhere port 9100 is the port the Node Exporter is running. Select the agent and press run to confirm that metrics are being received.\n\n\nNow \nset the output format to Prometheus\n and specify the scrape interval (the default is to scrape every 30 seconds) on the plugin details page. Save and apply to either a single agent or a tag for deployment.\n\n\nA list of other exporters can be found here:\n\n\nhttps://prometheus.io/docs/instrumenting/exporters/\n\n\nExample for Application Instrumentation\n\n\nSelect a Prometheus client library for your language.\n\n\nhttps://prometheus.io/docs/instrumenting/clientlibs/\n\n\nEach library has a set of documentation for how to instrument your code. In our example we'll pick a Python app for payments processing. The example shows how to import the client library and expose metrics on http://localhost:8000.\n\n\nCreate a plugin called payments.py with the following content:\n\n\n#!/usr/bin/env python\nimport requests\nrequests.get('http://localhost:8000/metrics').text\n\n\n\n\nTest this by pressing the run button against a node running the payments application. Remember to switch the plugin format to Prometheus on the plugin details page. Save and apply to either a single agent or a tag for deployment.",
            "title": "Prometheus plugins"
        },
        {
            "location": "/nagios/prometheus_plugins/#prometheus-plugins",
            "text": "The Dataloop agent can run plugins that scrape Prometheus http endpoints. We recommend that you install a Dataloop agent on each server and then create a separate plugin to monitor each endpoint on localhost.",
            "title": "Prometheus Plugins"
        },
        {
            "location": "/nagios/prometheus_plugins/#example-for-node-exporter",
            "text": "Start the Prometheus Node Exporter on a server following the instructions here:  https://github.com/prometheus/node_exporter  Then create a new plugin in Dataloop called node_exporter.py.  Paste in the following code to scrape the endpoint:  #!/usr/bin/env python\nimport requests\nrequests.get('http://localhost:9100/metrics').text  Where port 9100 is the port the Node Exporter is running. Select the agent and press run to confirm that metrics are being received.  Now  set the output format to Prometheus  and specify the scrape interval (the default is to scrape every 30 seconds) on the plugin details page. Save and apply to either a single agent or a tag for deployment.  A list of other exporters can be found here:  https://prometheus.io/docs/instrumenting/exporters/",
            "title": "Example for Node Exporter"
        },
        {
            "location": "/nagios/prometheus_plugins/#example-for-application-instrumentation",
            "text": "Select a Prometheus client library for your language.  https://prometheus.io/docs/instrumenting/clientlibs/  Each library has a set of documentation for how to instrument your code. In our example we'll pick a Python app for payments processing. The example shows how to import the client library and expose metrics on http://localhost:8000.  Create a plugin called payments.py with the following content:  #!/usr/bin/env python\nimport requests\nrequests.get('http://localhost:8000/metrics').text  Test this by pressing the run button against a node running the payments application. Remember to switch the plugin format to Prometheus on the plugin details page. Save and apply to either a single agent or a tag for deployment.",
            "title": "Example for Application Instrumentation"
        }
    ]
}